{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a96a795-ec4d-45d3-b29d-10fb67f792d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import year, date_format,col\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f139a43-89b3-4005-a51e-1bf246f19c2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = '/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/etl_script/data/2023_data/yellow_tripdata_2023-01.parquet'\n",
    "import logging \n",
    "# import logging\n",
    "\n",
    "# Set the log level to WARN\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "# .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\")\n",
    "# sc = pyspark.SparkContext('local[*]')\n",
    "# sc.setLogLevel(\"ERROR\")\n",
    "# spark = SparkSession.builder.config(\"spark.jars\",\"~/Desktop/tech_project/nyc_taxi_project/postgresql-42.7.1.jar\").master(\"local[*]\").appName(\"test\").getOrCreate()\n",
    "# spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af8dabe-95a9-453d-9d6d-30b0f3293eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/19 00:27:13 WARN Utils: Your hostname, idowu-pc resolves to a loopback address: 127.0.1.1; using 192.168.1.127 instead (on interface wlo1)\n",
      "24/02/19 00:27:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/idowuileks/.ivy2/cache\n",
      "The jars for the packages stored in: /home/idowuileks/.ivy2/jars\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-46a053c2-78ff-47ea-93db-581e01b0d49d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.postgresql#postgresql;42.6.0 in central\n",
      "\tfound org.checkerframework#checker-qual;3.31.0 in central\n",
      ":: resolution report :: resolve 134ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\torg.checkerframework#checker-qual;3.31.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.6.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-46a053c2-78ff-47ea-93db-581e01b0d49d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/7ms)\n",
      "24/02/19 00:27:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"hello\").config(\"spark.master\", \"local[*]\").config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.6.0\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648b551f-f7bc-4bcb-8147-d82507ad9d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config(\"spark.jars\",\"/home/idowuileks/Desktop/tech_project/nyc_taxi_project/postgresql-42.7.1.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2eba255-b4f8-4345-a63c-9c98395f424f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\",\"true\").format(\"parquet\").load(data_path,inferSchema=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e4778d0-20d2-4601-9b6c-d16b265d5d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_info_pyspark(df, column, new_column_prefix):\n",
    "  \"\"\"\n",
    "  Extracts various date information from a PySpark DataFrame column.\n",
    "\n",
    "  Args:\n",
    "    df: The PySpark DataFrame.\n",
    "    column: The name of the date column.\n",
    "    new_column_prefix: The prefix for the new columns containing date information.\n",
    "\n",
    "  Returns:\n",
    "    The modified DataFrame with new columns containing year, month, day, hour, minute,\n",
    "    day of week, day name, month-end flag, and year-end flag.\n",
    "  \"\"\"\n",
    "\n",
    "  df = df.withColumn(f\"{new_column_prefix}_year\", F.year(F.col(column)))\n",
    "  df = df.withColumn(f\"{new_column_prefix}_month\", F.month(F.col(column)))\n",
    "  df = df.withColumn(f\"{new_column_prefix}_day\", F.dayofmonth(F.col(column)))\n",
    "  df = df.withColumn(f\"{new_column_prefix}_hour\", F.hour(F.col(column)))\n",
    "  df = df.withColumn(f\"{new_column_prefix}_minute\", F.minute(F.col(column)))\n",
    "  df = df.withColumn(f\"{new_column_prefix}_day_of_week\", F.dayofweek(column))\n",
    "  df = df.withColumn(f\"{new_column_prefix}_day_name\", F.date_format(column, \"EEEE\"))\n",
    "  df = df.withColumn(f\"{new_column_prefix}_month_name\", F.date_format(column, \"MMMM\"))\n",
    "  df = df.withColumn(f\"{new_column_prefix}_is_month_end\", F.last_day(F.col(column)) == F.col(column))\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7485587a-bd9e-4d9c-ad18-976c04128c76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_time_table_pickup = get_date_info_pyspark(df,'tpep_dropoff_datetime','pickup')\n",
    "\n",
    "# df_time_table_pickup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b103e802-20cb-464d-990d-b6273e079cd3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------+------------+----------+-----------+-------------+------------------+---------------+-----------------+-------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|pickup_year|pickup_month|pickup_day|pickup_hour|pickup_minute|pickup_day_of_week|pickup_day_name|pickup_month_name|pickup_is_month_end|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------+------------+----------+-----------+-------------+------------------+---------------+-----------------+-------------------+\n",
      "|       2| 2023-01-01 00:32:10|  2023-01-01 00:40:36|            1.0|         0.97|       1.0|                 N|         161|         141|           2|        9.3|  1.0|    0.5|       0.0|         0.0|                  1.0|        14.3|                 2.5|        0.0|       2023|           1|         1|          0|           40|                 1|         Sunday|          January|              false|\n",
      "|       2| 2023-01-01 00:55:08|  2023-01-01 01:01:27|            1.0|          1.1|       1.0|                 N|          43|         237|           1|        7.9|  1.0|    0.5|       4.0|         0.0|                  1.0|        16.9|                 2.5|        0.0|       2023|           1|         1|          1|            1|                 1|         Sunday|          January|              false|\n",
      "|       2| 2023-01-01 00:25:04|  2023-01-01 00:37:49|            1.0|         2.51|       1.0|                 N|          48|         238|           1|       14.9|  1.0|    0.5|      15.0|         0.0|                  1.0|        34.9|                 2.5|        0.0|       2023|           1|         1|          0|           37|                 1|         Sunday|          January|              false|\n",
      "|       1| 2023-01-01 00:03:48|  2023-01-01 00:13:25|            0.0|          1.9|       1.0|                 N|         138|           7|           1|       12.1| 7.25|    0.5|       0.0|         0.0|                  1.0|       20.85|                 0.0|       1.25|       2023|           1|         1|          0|           13|                 1|         Sunday|          January|              false|\n",
      "|       2| 2023-01-01 00:10:29|  2023-01-01 00:21:19|            1.0|         1.43|       1.0|                 N|         107|          79|           1|       11.4|  1.0|    0.5|      3.28|         0.0|                  1.0|       19.68|                 2.5|        0.0|       2023|           1|         1|          0|           21|                 1|         Sunday|          January|              false|\n",
      "|       2| 2023-01-01 00:50:34|  2023-01-01 01:02:52|            1.0|         1.84|       1.0|                 N|         161|         137|           1|       12.8|  1.0|    0.5|      10.0|         0.0|                  1.0|        27.8|                 2.5|        0.0|       2023|           1|         1|          1|            2|                 1|         Sunday|          January|              false|\n",
      "|       2| 2023-01-01 00:09:22|  2023-01-01 00:19:49|            1.0|         1.66|       1.0|                 N|         239|         143|           1|       12.1|  1.0|    0.5|      3.42|         0.0|                  1.0|       20.52|                 2.5|        0.0|       2023|           1|         1|          0|           19|                 1|         Sunday|          January|              false|\n",
      "|       2| 2023-01-01 00:27:12|  2023-01-01 00:49:56|            1.0|         11.7|       1.0|                 N|         142|         200|           1|       45.7|  1.0|    0.5|     10.74|         3.0|                  1.0|       64.44|                 2.5|        0.0|       2023|           1|         1|          0|           49|                 1|         Sunday|          January|              false|\n",
      "|       2| 2023-01-01 00:21:44|  2023-01-01 00:36:40|            1.0|         2.95|       1.0|                 N|         164|         236|           1|       17.7|  1.0|    0.5|      5.68|         0.0|                  1.0|       28.38|                 2.5|        0.0|       2023|           1|         1|          0|           36|                 1|         Sunday|          January|              false|\n",
      "|       2| 2023-01-01 00:39:42|  2023-01-01 00:50:36|            1.0|         3.01|       1.0|                 N|         141|         107|           2|       14.9|  1.0|    0.5|       0.0|         0.0|                  1.0|        19.9|                 2.5|        0.0|       2023|           1|         1|          0|           50|                 1|         Sunday|          January|              false|\n",
      "|       2| 2023-01-01 00:53:01|  2023-01-01 01:01:45|            1.0|          1.8|       1.0|                 N|         234|          68|           1|       11.4|  1.0|    0.5|      3.28|         0.0|                  1.0|       19.68|                 2.5|        0.0|       2023|           1|         1|          1|            1|                 1|         Sunday|          January|              false|\n",
      "|       1| 2023-01-01 00:43:37|  2023-01-01 01:17:18|            4.0|          7.3|       1.0|                 N|          79|         264|           1|       33.8|  3.5|    0.5|      7.75|         0.0|                  1.0|       46.55|                 2.5|        0.0|       2023|           1|         1|          1|           17|                 1|         Sunday|          January|              false|\n",
      "|       2| 2023-01-01 00:34:44|  2023-01-01 01:04:25|            1.0|         3.23|       1.0|                 N|         164|         143|           1|       26.1|  1.0|    0.5|      6.22|         0.0|                  1.0|       37.32|                 2.5|        0.0|       2023|           1|         1|          1|            4|                 1|         Sunday|          January|              false|\n",
      "|       2| 2023-01-01 00:09:29|  2023-01-01 00:29:23|            2.0|        11.43|       1.0|                 N|         138|          33|           1|       44.3|  6.0|    0.5|     13.26|         0.0|                  1.0|       66.31|                 0.0|       1.25|       2023|           1|         1|          0|           29|                 1|         Sunday|          January|              false|\n",
      "|       2| 2023-01-01 00:33:53|  2023-01-01 00:49:15|            1.0|         2.95|       1.0|                 N|          33|          61|           1|       17.7|  1.0|    0.5|      4.04|         0.0|                  1.0|       24.24|                 0.0|        0.0|       2023|           1|         1|          0|           49|                 1|         Sunday|          January|              false|\n",
      "|       2| 2023-01-01 00:13:04|  2023-01-01 00:22:10|            1.0|         1.52|       1.0|                 N|          79|         186|           1|       10.0|  1.0|    0.5|      1.25|         0.0|                  1.0|       16.25|                 2.5|        0.0|       2023|           1|         1|          0|           22|                 1|         Sunday|          January|              false|\n",
      "|       2| 2023-01-01 00:45:11|  2023-01-01 01:07:39|            1.0|         2.23|       1.0|                 N|          90|          48|           1|       19.8|  1.0|    0.5|      4.96|         0.0|                  1.0|       29.76|                 2.5|        0.0|       2023|           1|         1|          1|            7|                 1|         Sunday|          January|              false|\n",
      "|       1| 2023-01-01 00:04:33|  2023-01-01 00:19:22|            1.0|          4.5|       1.0|                 N|         113|         255|           1|       20.5|  3.5|    0.5|       4.0|         0.0|                  1.0|        29.5|                 2.5|        0.0|       2023|           1|         1|          0|           19|                 1|         Sunday|          January|              false|\n",
      "|       1| 2023-01-01 00:03:36|  2023-01-01 00:09:36|            3.0|          1.2|       1.0|                 N|         237|         239|           2|        8.6|  3.5|    0.5|       0.0|         0.0|                  1.0|        13.6|                 2.5|        0.0|       2023|           1|         1|          0|            9|                 1|         Sunday|          January|              false|\n",
      "|       1| 2023-01-01 00:15:23|  2023-01-01 00:29:41|            2.0|          2.5|       1.0|                 N|         143|         229|           2|       15.6|  3.5|    0.5|       0.0|         0.0|                  1.0|        20.6|                 2.5|        0.0|       2023|           1|         1|          0|           29|                 1|         Sunday|          January|              false|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-----------+------------+----------+-----------+-------------+------------------+---------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time_table_pickup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a346f78a-fef0-4c9c-9a43-abddbcfb8de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------------------+----------------------------+----------------------+--------------------+-----------------+-------------------------+-------------------+-------------------+-------------------+------------------+------------+--------------+-----------------+-------------------+----------------------------+-------------------+---------------------------+------------------+\n",
      "|VendorID_isNull|tpep_pickup_datetime_isNull|tpep_dropoff_datetime_isNull|passenger_count_isNull|trip_distance_isNull|RatecodeID_isNull|store_and_fwd_flag_isNull|PULocationID_isNull|DOLocationID_isNull|payment_type_isNull|fare_amount_isNull|extra_isNull|mta_tax_isNull|tip_amount_isNull|tolls_amount_isNull|improvement_surcharge_isNull|total_amount_isNull|congestion_surcharge_isNull|airport_fee_isNull|\n",
      "+---------------+---------------------------+----------------------------+----------------------+--------------------+-----------------+-------------------------+-------------------+-------------------+-------------------+------------------+------------+--------------+-----------------+-------------------+----------------------------+-------------------+---------------------------+------------------+\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "|          false|                      false|                       false|                 false|               false|            false|                    false|              false|              false|              false|             false|       false|         false|            false|              false|                       false|              false|                      false|             false|\n",
      "+---------------+---------------------------+----------------------------+----------------------+--------------------+-----------------+-------------------------+-------------------+-------------------+-------------------+------------------+------------+--------------+-----------------+-------------------+----------------------------+-------------------+---------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_check_df = df_time_table_pickup.select([col(c).isNull().alias(c+\"_isNull\") for c in df.columns])\n",
    "\n",
    "# Show the result\n",
    "null_check_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e552aba7-dca2-454b-85be-135cfbce58c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2023, 1, 1, 0, 32, 10), tpep_dropoff_datetime=datetime.datetime(2023, 1, 1, 0, 40, 36), passenger_count=1.0, trip_distance=0.97, RatecodeID=1.0, store_and_fwd_flag='N', PULocationID=161, DOLocationID=141, payment_type=2, fare_amount=9.3, extra=1.0, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=14.3, congestion_surcharge=2.5, airport_fee=0.0, pickup_year=2023, pickup_month=1, pickup_day=1, pickup_hour=0, pickup_minute=40, pickup_day_of_week=1, pickup_day_name='Sunday', pickup_month_name='January', pickup_is_month_end=False)\n"
     ]
    }
   ],
   "source": [
    "print(df_time_table_pickup.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f122b41a-bb22-4f5e-a476-bb15510be523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df = df_time_table_pickup.limit(10).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea4918b3-294e-4e63-959d-e06ab37490a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_clipboard('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "974ac960-b8a8-4d50-a05d-6d097679c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bee4b0e7-19b0-427c-a3c4-0e2dbe996cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    'datetime_id',\n",
    "    concat_ws('_', col('tpep_pickup_datetime').cast(StringType()), col('tpep_dropoff_datetime').cast(StringType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a97d97d-ab52-469d-b1e7-9f4e7702c755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2023, 1, 1, 0, 32, 10), tpep_dropoff_datetime=datetime.datetime(2023, 1, 1, 0, 40, 36), passenger_count=1.0, trip_distance=0.97, RatecodeID=1.0, store_and_fwd_flag='N', PULocationID=161, DOLocationID=141, payment_type=2, fare_amount=9.3, extra=1.0, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=14.3, congestion_surcharge=2.5, airport_fee=0.0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35c8da75-9fe8-4bc1-bb83-3c52348e4b43",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/pyspark/serializers.py\", line 437, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 563, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 653, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 526, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 507, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in _extract_code_globals\n",
      "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/spark/python/pyspark/cloudpickle/cloudpickle.py\", line 236, in <setcomp>\n",
      "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/spark/python/pyspark/serializers.py:437\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cloudpickle\u001b[38;5;241m.\u001b[39mdumps(obj, pickle_protocol)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m cp\u001b[38;5;241m.\u001b[39mdump(obj)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:563\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mself\u001b[39m, obj)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:653\u001b[0m, in \u001b[0;36mCloudPickler.reducer_override\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[0;32m--> 653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_reduce(obj)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;66;03m# distpatch_table\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:526\u001b[0m, in \u001b[0;36mCloudPickler._function_reduce\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynamic_function_reduce(obj)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:507\u001b[0m, in \u001b[0;36mCloudPickler._dynamic_function_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    506\u001b[0m newargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_getnewargs(func)\n\u001b[0;32m--> 507\u001b[0m state \u001b[38;5;241m=\u001b[39m _function_getstate(func)\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (types\u001b[38;5;241m.\u001b[39mFunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m         _function_setstate)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/cloudpickle/cloudpickle_fast.py:157\u001b[0m, in \u001b[0;36m_function_getstate\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    146\u001b[0m slotstate \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__qualname__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__closure__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__closure__\u001b[39m,\n\u001b[1;32m    155\u001b[0m }\n\u001b[0;32m--> 157\u001b[0m f_globals_ref \u001b[38;5;241m=\u001b[39m _extract_code_globals(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m)\n\u001b[1;32m    158\u001b[0m f_globals \u001b[38;5;241m=\u001b[39m {k: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    159\u001b[0m              func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m}\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/cloudpickle/cloudpickle.py:236\u001b[0m, in \u001b[0;36m_extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    235\u001b[0m names \u001b[38;5;241m=\u001b[39m co\u001b[38;5;241m.\u001b[39mco_names\n\u001b[0;32m--> 236\u001b[0m out_names \u001b[38;5;241m=\u001b[39m {names[oparg] \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresonding to the one\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/cloudpickle/cloudpickle.py:236\u001b[0m, in \u001b[0;36m<setcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    235\u001b[0m names \u001b[38;5;241m=\u001b[39m co\u001b[38;5;241m.\u001b[39mco_names\n\u001b[0;32m--> 236\u001b[0m out_names \u001b[38;5;241m=\u001b[39m {names[oparg] \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresonding to the one\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1997-02-10\u001b[39m\u001b[38;5;124m'\u001b[39m,)], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:675\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(\n\u001b[1;32m    674\u001b[0m         data, schema, samplingRatio, verifySchema)\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:701\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    700\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[0;32m--> 701\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n\u001b[1;32m    702\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), schema\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m    703\u001b[0m df \u001b[38;5;241m=\u001b[39m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:2620\u001b[0m, in \u001b[0;36mRDD._to_java_object_rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2614\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Return a JavaRDD of Object by unpickling\u001b[39;00m\n\u001b[1;32m   2615\u001b[0m \n\u001b[1;32m   2616\u001b[0m \u001b[38;5;124;03mIt will convert each Python object into Java object by Pyrolite, whenever the\u001b[39;00m\n\u001b[1;32m   2617\u001b[0m \u001b[38;5;124;03mRDD is serialized in batch or not.\u001b[39;00m\n\u001b[1;32m   2618\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2619\u001b[0m rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickled()\n\u001b[0;32m-> 2620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mpythonToJava(rdd\u001b[38;5;241m.\u001b[39m_jrdd, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:2951\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2948\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2949\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2951\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m _wrap_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd_deserializer,\n\u001b[1;32m   2952\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer, profiler)\n\u001b[1;32m   2953\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func,\n\u001b[1;32m   2954\u001b[0m                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier)\n\u001b[1;32m   2955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_val \u001b[38;5;241m=\u001b[39m python_rdd\u001b[38;5;241m.\u001b[39masJavaRDD()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:2830\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   2828\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2829\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 2830\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[1;32m   2831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\u001b[38;5;28mbytearray\u001b[39m(pickled_command), env, includes, sc\u001b[38;5;241m.\u001b[39mpythonExec,\n\u001b[1;32m   2832\u001b[0m                               sc\u001b[38;5;241m.\u001b[39mpythonVer, broadcast_vars, sc\u001b[38;5;241m.\u001b[39m_javaAccumulator)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/rdd.py:2816\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc, command):\n\u001b[1;32m   2814\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   2815\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 2816\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m ser\u001b[38;5;241m.\u001b[39mdumps(command)\n\u001b[1;32m   2817\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   2818\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n\u001b[1;32m   2819\u001b[0m         broadcast \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mbroadcast(pickled_command)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/serializers.py:447\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    445\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    446\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('1997-02-10',)], ['d'])\n",
    "\n",
    "# df.select(last_day(df.d).alias('date')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6ed571b-0403-490d-951e-7b2482de4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_pick_drop(column_a, column_b):\n",
    "    # return col(column_a).cast(StringType()) + \"_\" + col(column_b).cast(StringType())\n",
    "    return concat_ws(\"_\", col(column_a).cast(StringType()), col(column_b).cast(StringType()))\n",
    "    # return concat_ws(\"_\", col(column_a).cast(StringType()), col(column_b).cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04db867d-fc50-4b81-9de0-46a4ad68c53a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "92bbe187-5419-4051-9531-e7916a5b6c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('test_concat', join_pick_drop(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "799624fe-0976-4df1-9fd8-69384d58ce85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2023, 1, 1, 0, 32, 10), tpep_dropoff_datetime=datetime.datetime(2023, 1, 1, 0, 40, 36), passenger_count=1.0, trip_distance=0.97, RatecodeID=1.0, store_and_fwd_flag='N', PULocationID=161, DOLocationID=141, payment_type=2, fare_amount=9.3, extra=1.0, mta_tax=0.5, tip_amount=0.0, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=14.3, congestion_surcharge=2.5, airport_fee=0.0, test_concat='2023-01-01 00:32:10_2023-01-01 00:40:36')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "12c6c0c4-01d5-4d0e-9355-2d03bec0036e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2023-01-01 00:32:10|  2023-01-01 00:40:36|            1.0|         0.97|       1.0|                 N|         161|         141|           2|        9.3|  1.0|    0.5|       0.0|         0.0|                  1.0|        14.3|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:55:08|  2023-01-01 01:01:27|            1.0|          1.1|       1.0|                 N|          43|         237|           1|        7.9|  1.0|    0.5|       4.0|         0.0|                  1.0|        16.9|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:25:04|  2023-01-01 00:37:49|            1.0|         2.51|       1.0|                 N|          48|         238|           1|       14.9|  1.0|    0.5|      15.0|         0.0|                  1.0|        34.9|                 2.5|        0.0|\n",
      "|       1| 2023-01-01 00:03:48|  2023-01-01 00:13:25|            0.0|          1.9|       1.0|                 N|         138|           7|           1|       12.1| 7.25|    0.5|       0.0|         0.0|                  1.0|       20.85|                 0.0|       1.25|\n",
      "|       2| 2023-01-01 00:10:29|  2023-01-01 00:21:19|            1.0|         1.43|       1.0|                 N|         107|          79|           1|       11.4|  1.0|    0.5|      3.28|         0.0|                  1.0|       19.68|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:50:34|  2023-01-01 01:02:52|            1.0|         1.84|       1.0|                 N|         161|         137|           1|       12.8|  1.0|    0.5|      10.0|         0.0|                  1.0|        27.8|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:09:22|  2023-01-01 00:19:49|            1.0|         1.66|       1.0|                 N|         239|         143|           1|       12.1|  1.0|    0.5|      3.42|         0.0|                  1.0|       20.52|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:27:12|  2023-01-01 00:49:56|            1.0|         11.7|       1.0|                 N|         142|         200|           1|       45.7|  1.0|    0.5|     10.74|         3.0|                  1.0|       64.44|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:21:44|  2023-01-01 00:36:40|            1.0|         2.95|       1.0|                 N|         164|         236|           1|       17.7|  1.0|    0.5|      5.68|         0.0|                  1.0|       28.38|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:39:42|  2023-01-01 00:50:36|            1.0|         3.01|       1.0|                 N|         141|         107|           2|       14.9|  1.0|    0.5|       0.0|         0.0|                  1.0|        19.9|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:53:01|  2023-01-01 01:01:45|            1.0|          1.8|       1.0|                 N|         234|          68|           1|       11.4|  1.0|    0.5|      3.28|         0.0|                  1.0|       19.68|                 2.5|        0.0|\n",
      "|       1| 2023-01-01 00:43:37|  2023-01-01 01:17:18|            4.0|          7.3|       1.0|                 N|          79|         264|           1|       33.8|  3.5|    0.5|      7.75|         0.0|                  1.0|       46.55|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:34:44|  2023-01-01 01:04:25|            1.0|         3.23|       1.0|                 N|         164|         143|           1|       26.1|  1.0|    0.5|      6.22|         0.0|                  1.0|       37.32|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:09:29|  2023-01-01 00:29:23|            2.0|        11.43|       1.0|                 N|         138|          33|           1|       44.3|  6.0|    0.5|     13.26|         0.0|                  1.0|       66.31|                 0.0|       1.25|\n",
      "|       2| 2023-01-01 00:33:53|  2023-01-01 00:49:15|            1.0|         2.95|       1.0|                 N|          33|          61|           1|       17.7|  1.0|    0.5|      4.04|         0.0|                  1.0|       24.24|                 0.0|        0.0|\n",
      "|       2| 2023-01-01 00:13:04|  2023-01-01 00:22:10|            1.0|         1.52|       1.0|                 N|          79|         186|           1|       10.0|  1.0|    0.5|      1.25|         0.0|                  1.0|       16.25|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:45:11|  2023-01-01 01:07:39|            1.0|         2.23|       1.0|                 N|          90|          48|           1|       19.8|  1.0|    0.5|      4.96|         0.0|                  1.0|       29.76|                 2.5|        0.0|\n",
      "|       1| 2023-01-01 00:04:33|  2023-01-01 00:19:22|            1.0|          4.5|       1.0|                 N|         113|         255|           1|       20.5|  3.5|    0.5|       4.0|         0.0|                  1.0|        29.5|                 2.5|        0.0|\n",
      "|       1| 2023-01-01 00:03:36|  2023-01-01 00:09:36|            3.0|          1.2|       1.0|                 N|         237|         239|           2|        8.6|  3.5|    0.5|       0.0|         0.0|                  1.0|        13.6|                 2.5|        0.0|\n",
      "|       1| 2023-01-01 00:15:23|  2023-01-01 00:29:41|            2.0|          2.5|       1.0|                 N|         143|         229|           2|       15.6|  3.5|    0.5|       0.0|         0.0|                  1.0|        20.6|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90819458-c9bd-4492-80a4-db44db415948",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def transform_data(data):\n",
    "    lower_column_name = [col(col_name).alias(col_name.lower()) for col_name in data.columns]\n",
    "    data = data.select(*lower_column_name)\n",
    "\n",
    "    fact_columns = ['tpep_dropoff_datetime','tpep_pickup_datetime','passenger_count', 'trip_distance','fare_amount', 'extra',\n",
    "        'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
    "        'total_amount', 'congestion_surcharge', 'airport_fee']\n",
    "\n",
    "    dimension_table_data = ['tpep_dropoff_datetime','tpep_pickup_datetime','ratecodeid','store_and_fwd_flag','pulocationid','dolocationid','payment_type']\n",
    "\n",
    "    dimension_table_data = data.select(*dimension_table_data)\n",
    "\n",
    "    time_table_data = data.select('tpep_dropoff_datetime','tpep_pickup_datetime')\n",
    "    df_time_table_pickup = get_date_info_pyspark(time_table_data,'tpep_dropoff_datetime','pickup')\n",
    "    df_time_table_dropoff = get_date_info_pyspark(df_time_table_pickup,'tpep_dropoff_datetime','dropoff')\n",
    "\n",
    "    df_time_table_dropoff = df_time_table_dropoff.withColumn('datetime_id', join_pick_drop('tpep_pickup_datetime', 'tpep_dropoff_datetime'))\n",
    "\n",
    "    fact_data_table = data.select(*fact_columns)\n",
    "    fact_data_table = fact_data_table.drop('tpep_dropoff_datetime', 'tpep_pickup_datetime')\n",
    "    dimension_table_data = dimension_table_data.withColumn('datetime_id', join_pick_drop('tpep_pickup_datetime', 'tpep_dropoff_datetime'))\n",
    "    dimension_table_data = dimension_table_data.select('datetime_id', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type')\n",
    "    \n",
    "    ordered_column_time = ['datetime_id','tpep_dropoff_datetime',\n",
    "    'tpep_pickup_datetime',\n",
    "    'pickup_year',\n",
    "    'pickup_month',\n",
    "    'pickup_day',\n",
    "    'pickup_hour',\n",
    "    'pickup_minute',\n",
    "    'pickup_day_of_week',\n",
    "    'pickup_day_name',\n",
    "    'pickup_is_month_end',\n",
    "    'dropoff_year',\n",
    "    'dropoff_month',\n",
    "    'dropoff_day',\n",
    "    'dropoff_hour',\n",
    "    'dropoff_minute',\n",
    "    'dropoff_day_of_week',\n",
    "    'dropoff_day_name',\n",
    "    'dropoff_is_month_end']\n",
    "\n",
    "    df_time_table_dropoff = df_time_table_dropoff.selectExpr(*ordered_column_time)\n",
    "\n",
    "    df_time_table_dropoff = df_time_table_dropoff.withColumn('tpep_dropoff_datetime', df_time_table_dropoff.tpep_dropoff_datetime.cast('timestamp'))\n",
    "    df_time_table_dropoff = df_time_table_dropoff.withColumn('tpep_pickup_datetime', df_time_table_dropoff.tpep_pickup_datetime.cast('timestamp'))\n",
    "\n",
    "    df_time_table_dropoff = df_time_table_dropoff.dropDuplicates(subset=['datetime_id'])\n",
    "    fact_data_table = fact_data_table.dropDuplicates(subset=['datetime_id'])\n",
    "    dimension_table_data = dimension_table_data.dropDuplicates(subset=['datetime_id'])\n",
    "\n",
    "    return dimension_table_data, df_time_table_dropoff, fact_data_table\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06b4eb03-8b4d-4285-9b28-0c148dd23aa6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2023-01-01 00:32:10|  2023-01-01 00:40:36|            1.0|         0.97|       1.0|                 N|         161|         141|           2|        9.3|  1.0|    0.5|       0.0|         0.0|                  1.0|        14.3|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:55:08|  2023-01-01 01:01:27|            1.0|          1.1|       1.0|                 N|          43|         237|           1|        7.9|  1.0|    0.5|       4.0|         0.0|                  1.0|        16.9|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:25:04|  2023-01-01 00:37:49|            1.0|         2.51|       1.0|                 N|          48|         238|           1|       14.9|  1.0|    0.5|      15.0|         0.0|                  1.0|        34.9|                 2.5|        0.0|\n",
      "|       1| 2023-01-01 00:03:48|  2023-01-01 00:13:25|            0.0|          1.9|       1.0|                 N|         138|           7|           1|       12.1| 7.25|    0.5|       0.0|         0.0|                  1.0|       20.85|                 0.0|       1.25|\n",
      "|       2| 2023-01-01 00:10:29|  2023-01-01 00:21:19|            1.0|         1.43|       1.0|                 N|         107|          79|           1|       11.4|  1.0|    0.5|      3.28|         0.0|                  1.0|       19.68|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:50:34|  2023-01-01 01:02:52|            1.0|         1.84|       1.0|                 N|         161|         137|           1|       12.8|  1.0|    0.5|      10.0|         0.0|                  1.0|        27.8|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:09:22|  2023-01-01 00:19:49|            1.0|         1.66|       1.0|                 N|         239|         143|           1|       12.1|  1.0|    0.5|      3.42|         0.0|                  1.0|       20.52|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:27:12|  2023-01-01 00:49:56|            1.0|         11.7|       1.0|                 N|         142|         200|           1|       45.7|  1.0|    0.5|     10.74|         3.0|                  1.0|       64.44|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:21:44|  2023-01-01 00:36:40|            1.0|         2.95|       1.0|                 N|         164|         236|           1|       17.7|  1.0|    0.5|      5.68|         0.0|                  1.0|       28.38|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:39:42|  2023-01-01 00:50:36|            1.0|         3.01|       1.0|                 N|         141|         107|           2|       14.9|  1.0|    0.5|       0.0|         0.0|                  1.0|        19.9|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:53:01|  2023-01-01 01:01:45|            1.0|          1.8|       1.0|                 N|         234|          68|           1|       11.4|  1.0|    0.5|      3.28|         0.0|                  1.0|       19.68|                 2.5|        0.0|\n",
      "|       1| 2023-01-01 00:43:37|  2023-01-01 01:17:18|            4.0|          7.3|       1.0|                 N|          79|         264|           1|       33.8|  3.5|    0.5|      7.75|         0.0|                  1.0|       46.55|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:34:44|  2023-01-01 01:04:25|            1.0|         3.23|       1.0|                 N|         164|         143|           1|       26.1|  1.0|    0.5|      6.22|         0.0|                  1.0|       37.32|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:09:29|  2023-01-01 00:29:23|            2.0|        11.43|       1.0|                 N|         138|          33|           1|       44.3|  6.0|    0.5|     13.26|         0.0|                  1.0|       66.31|                 0.0|       1.25|\n",
      "|       2| 2023-01-01 00:33:53|  2023-01-01 00:49:15|            1.0|         2.95|       1.0|                 N|          33|          61|           1|       17.7|  1.0|    0.5|      4.04|         0.0|                  1.0|       24.24|                 0.0|        0.0|\n",
      "|       2| 2023-01-01 00:13:04|  2023-01-01 00:22:10|            1.0|         1.52|       1.0|                 N|          79|         186|           1|       10.0|  1.0|    0.5|      1.25|         0.0|                  1.0|       16.25|                 2.5|        0.0|\n",
      "|       2| 2023-01-01 00:45:11|  2023-01-01 01:07:39|            1.0|         2.23|       1.0|                 N|          90|          48|           1|       19.8|  1.0|    0.5|      4.96|         0.0|                  1.0|       29.76|                 2.5|        0.0|\n",
      "|       1| 2023-01-01 00:04:33|  2023-01-01 00:19:22|            1.0|          4.5|       1.0|                 N|         113|         255|           1|       20.5|  3.5|    0.5|       4.0|         0.0|                  1.0|        29.5|                 2.5|        0.0|\n",
      "|       1| 2023-01-01 00:03:36|  2023-01-01 00:09:36|            3.0|          1.2|       1.0|                 N|         237|         239|           2|        8.6|  3.5|    0.5|       0.0|         0.0|                  1.0|        13.6|                 2.5|        0.0|\n",
      "|       1| 2023-01-01 00:15:23|  2023-01-01 00:29:41|            2.0|          2.5|       1.0|                 N|         143|         229|           2|       15.6|  3.5|    0.5|       0.0|         0.0|                  1.0|        20.6|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4f22fa7-40a2-45bc-ac35-b44c4fe256cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_date_info_pyspark(df, column, new_column_prefix):\n",
    "#   \"\"\"\n",
    "#   Extracts various date information from a PySpark DataFrame column.\n",
    "\n",
    "#   Args:\n",
    "#     df: The PySpark DataFrame.\n",
    "#     column: The name of the date column.\n",
    "#     new_column_prefix: The prefix for the new columns containing date information.\n",
    "\n",
    "#   Returns:\n",
    "#     The modified DataFrame with new columns containing year, month, day, hour, minute,\n",
    "#     day of week, day name, month-end flag, and year-end flag.\n",
    "#   \"\"\"\n",
    "\n",
    "#   df = df.withColumn(f\"{new_column_prefix}_year\", F.year(F.col(column)))\n",
    "#   df = df.withColumn(f\"{new_column_prefix}_month\", F.month(F.col(column)))\n",
    "#   df = df.withColumn(f\"{new_column_prefix}_day\", F.dayofmonth(F.col(column)))\n",
    "#   df = df.withColumn(f\"{new_column_prefix}_hour\", F.hour(F.col(column)))\n",
    "#   df = df.withColumn(f\"{new_column_prefix}_minute\", F.minute(F.col(column)))\n",
    "#   df = df.withColumn(f\"{new_column_prefix}_day_of_week\", F.dayofweek(column))\n",
    "#   df = df.withColumn(f\"{new_column_prefix}_day_name\", F.date_format(column, \"EEEE\"))\n",
    "#   df = df.withColumn(f\"{new_column_prefix}_is_month_end\", F.last_day(column) == column)\n",
    "#   df = df.withColumn(f\"{new_column_prefix}_is_year_end\", F.last_day(column) == F.date_add(\n",
    "#       F.last_day(column), F.months(1) - F.dayofmonth(column)\n",
    "#   ))\n",
    "  # return df\n",
    "\n",
    "# Example usage\n",
    "# df_pyspark = spark.createDataFrame([(\"2024-01-31 23:59:59\",)], [\"date\"])\n",
    "# df_with_date_info = get_date_info_pyspark(df_pyspark, \"date\", \"date_info\")\n",
    "# df_with_date_info.show()\n",
    "df_time_table_pickup = get_date_info_pyspark(df,'tpep_dropoff_datetime','pickup')\n",
    "\n",
    "# df_time_table_pickup.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth, hour, minute, dayofweek, date_format, last_day\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# def join_pick_drop(column_a, column_b):\n",
    "#     return col(column_a).cast(StringType()) + \"_\" + col(column_b).cast(StringType())\n",
    "\n",
    "# def get_date_info(df, column, new_column_prefix):\n",
    "#     df = df.withColumn(f'{new_column_prefix}_year', year(col(column)))\n",
    "#     df = df.withColumn(f'{new_column_prefix}_month', month(col(column)))\n",
    "#     df = df.withColumn(f'{new_column_prefix}_day', dayofmonth(col(column)))\n",
    "#     df = df.withColumn(f'{new_column_prefix}_hour', hour(col(column)))\n",
    "#     df = df.withColumn(f'{new_column_prefix}_minute', minute(col(column)))\n",
    "#     df = df.withColumn(f'{new_column_prefix}_day_of_week', dayofweek(col(column)))\n",
    "#     df = df.withColumn(f'{new_column_prefix}_day_name', date_format(col(column), 'EEEE'))\n",
    "#     df = df.withColumn(f'{new_column_prefix}_is_month_end', date_format(last_day(col(column)), 'yyyy-MM-dd') == col(column))\n",
    "#     df = df.withColumn(f'{new_column_prefix}_is_year_end', year(last_day(col(column))) == year(col(column)))\n",
    "\n",
    "#     return df\n",
    "\n",
    "def transform_data(data):\n",
    "    # spark = SparkSession.builder.appName(\"data_transform\").getOrCreate()\n",
    "\n",
    "    lower_column_name = [col(col_name).alias(col_name.lower()) for col_name in data.columns]\n",
    "    data = data.select(*lower_column_name)\n",
    "\n",
    "    fact_columns = ['tpep_dropoff_datetime','tpep_pickup_datetime','passenger_count', 'trip_distance','fare_amount', 'extra',\n",
    "        'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
    "        'total_amount', 'congestion_surcharge', 'airport_fee']\n",
    "\n",
    "    dimension_table_data = ['tpep_dropoff_datetime','tpep_pickup_datetime','ratecodeid','store_and_fwd_flag','pulocationid','dolocationid','payment_type']\n",
    "\n",
    "    dimension_table_data = data.select(*dimension_table_data)\n",
    "\n",
    "    time_table_data = data.select('tpep_dropoff_datetime','tpep_pickup_datetime')\n",
    "\n",
    "    df_time_table_pickup = get_date_info(time_table_data, 'tpep_dropoff_datetime', 'pickup')\n",
    "\n",
    "    df_time_table_dropoff = get_date_info(df_time_table_pickup, 'tpep_dropoff_datetime', 'dropoff')\n",
    "\n",
    "    # df_time_table_dropoff.show()\n",
    "\n",
    "    df_time_table_dropoff = df_time_table_dropoff.withColumn('datetime_id', join_pick_drop(\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"))\n",
    "\n",
    "    fact_data_table = data.select(fact_columns)\n",
    "\n",
    "    fact_data_table = fact_data_table.withColumn('datetime_id', join_pick_drop('tpep_pickup_datetime', 'tpep_dropoff_datetime'))\n",
    "    fact_data_table = fact_data_table.select(fact_data_table.columns[::-1])\n",
    "    fact_data_table = fact_data_table.drop('tpep_dropoff_datetime', 'tpep_pickup_datetime')\n",
    "    print(dimension_table_data.schema)\n",
    "\n",
    "    dimension_table_data = dimension_table_data.withColumn('datetime_id', join_pick_drop('tpep_pickup_datetime', 'tpep_dropoff_datetime'))\n",
    "    print(dimension_table_data.schema)\n",
    "    dimension_table_data = dimension_table_data.select('datetime_id', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type')\n",
    "\n",
    "    ordered_column_time = ['datetime_id','tpep_dropoff_datetime',\n",
    "    'tpep_pickup_datetime',\n",
    "    'pickup_year',\n",
    "    'pickup_month',\n",
    "    'pickup_day',\n",
    "    'pickup_hour',\n",
    "    'pickup_minute',\n",
    "    'pickup_day_of_week',\n",
    "    'pickup_day_name',\n",
    "    'pickup_is_month_end',\n",
    "    'dropoff_year',\n",
    "    'dropoff_month',\n",
    "    'dropoff_day',\n",
    "    'dropoff_hour',\n",
    "    'dropoff_minute',\n",
    "    'dropoff_day_of_week',\n",
    "    'dropoff_day_name',\n",
    "    'dropoff_is_month_end']\n",
    "\n",
    "    df_time_table_dropoff = df_time_table_dropoff.select(ordered_column_time)\n",
    "\n",
    "    df_time_table_dropoff = df_time_table_dropoff.withColumn('tpep_dropoff_datetime', df_time_table_dropoff.tpep_dropoff_datetime.cast('timestamp'))\n",
    "    df_time_table_dropoff = df_time_table_dropoff.withColumn('tpep_pickup_datetime', df_time_table_dropoff.tpep_pickup_datetime.cast('timestamp'))\n",
    "\n",
    "    df_time_table_dropoff = df_time_table_dropoff.dropDuplicates(subset=['datetime_id'])\n",
    "    fact_data_table = fact_data_table.dropDuplicates(subset=['datetime_id'])\n",
    "    dimension_table_data = dimension_table_data.dropDuplicates(subset=['datetime_id'])\n",
    "\n",
    "    return dimension_table_data, df_time_table_dropoff, fact_data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "390a6736-61a5-4490-899f-db2f0a67b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transform_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "59043b6e-d5db-4a3b-b785-28d7e5487053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: bigint, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, passenger_count: double, trip_distance: double, RatecodeID: double, store_and_fwd_flag: string, PULocationID: bigint, DOLocationID: bigint, payment_type: bigint, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double, congestion_surcharge: double, airport_fee: double]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c40ef96c-3671-4d46-ac9b-b8777e3df749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(tpep_dropoff_datetime,TimestampType,true),StructField(tpep_pickup_datetime,TimestampType,true),StructField(ratecodeid,DoubleType,true),StructField(store_and_fwd_flag,StringType,true),StructField(pulocationid,LongType,true),StructField(dolocationid,LongType,true),StructField(payment_type,LongType,true)))\n",
      "StructType(List(StructField(tpep_dropoff_datetime,TimestampType,true),StructField(tpep_pickup_datetime,TimestampType,true),StructField(ratecodeid,DoubleType,true),StructField(store_and_fwd_flag,StringType,true),StructField(pulocationid,LongType,true),StructField(dolocationid,LongType,true),StructField(payment_type,LongType,true),StructField(datetime_id,StringType,false)))\n"
     ]
    }
   ],
   "source": [
    "dimension_table_data, time_table_data, fact_data_table = transform_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3b4c354-e11d-46a3-b1db-0aaee7c509d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+--------------------+-----------+------------+----------+-----------+-------------+------------------+---------------+-------------------+------------+-------------+-----------+------------+--------------+-------------------+----------------+--------------------+\n",
      "|         datetime_id|tpep_dropoff_datetime|tpep_pickup_datetime|pickup_year|pickup_month|pickup_day|pickup_hour|pickup_minute|pickup_day_of_week|pickup_day_name|pickup_is_month_end|dropoff_year|dropoff_month|dropoff_day|dropoff_hour|dropoff_minute|dropoff_day_of_week|dropoff_day_name|dropoff_is_month_end|\n",
      "+--------------------+---------------------+--------------------+-----------+------------+----------+-----------+-------------+------------------+---------------+-------------------+------------+-------------+-----------+------------+--------------+-------------------+----------------+--------------------+\n",
      "|2008-12-31 23:04:...|  2009-01-01 19:55:36| 2008-12-31 23:04:41|       2009|           1|         1|         19|           55|                 5|       Thursday|              false|        2009|            1|          1|          19|            55|                  5|        Thursday|               false|\n",
      "|2022-10-25 01:59:...|  2022-10-25 02:09:02| 2022-10-25 01:59:02|       2022|          10|        25|          2|            9|                 3|        Tuesday|              false|        2022|           10|         25|           2|             9|                  3|         Tuesday|               false|\n",
      "|2023-01-01 00:01:...|  2023-01-01 00:22:01| 2023-01-01 00:01:07|       2023|           1|         1|          0|           22|                 1|         Sunday|              false|        2023|            1|          1|           0|            22|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:01:...|  2023-01-01 00:17:01| 2023-01-01 00:01:18|       2023|           1|         1|          0|           17|                 1|         Sunday|              false|        2023|            1|          1|           0|            17|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:01:...|  2023-01-01 00:21:30| 2023-01-01 00:01:24|       2023|           1|         1|          0|           21|                 1|         Sunday|              false|        2023|            1|          1|           0|            21|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:02:...|  2023-01-01 00:44:02| 2023-01-01 00:02:19|       2023|           1|         1|          0|           44|                 1|         Sunday|              false|        2023|            1|          1|           0|            44|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:02:...|  2023-01-01 00:29:57| 2023-01-01 00:02:29|       2023|           1|         1|          0|           29|                 1|         Sunday|              false|        2023|            1|          1|           0|            29|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:02:...|  2023-01-01 00:12:15| 2023-01-01 00:02:33|       2023|           1|         1|          0|           12|                 1|         Sunday|              false|        2023|            1|          1|           0|            12|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:02:...|  2023-01-01 00:08:01| 2023-01-01 00:02:59|       2023|           1|         1|          0|            8|                 1|         Sunday|              false|        2023|            1|          1|           0|             8|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:03:...|  2023-01-01 00:17:51| 2023-01-01 00:03:48|       2023|           1|         1|          0|           17|                 1|         Sunday|              false|        2023|            1|          1|           0|            17|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:03:...|  2023-01-01 00:18:27| 2023-01-01 00:03:52|       2023|           1|         1|          0|           18|                 1|         Sunday|              false|        2023|            1|          1|           0|            18|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:03:...|  2023-01-01 00:37:00| 2023-01-01 00:03:53|       2023|           1|         1|          0|           37|                 1|         Sunday|              false|        2023|            1|          1|           0|            37|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:04:...|  2023-01-01 00:09:49| 2023-01-01 00:04:28|       2023|           1|         1|          0|            9|                 1|         Sunday|              false|        2023|            1|          1|           0|             9|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:04:...|  2023-01-01 00:09:33| 2023-01-01 00:04:32|       2023|           1|         1|          0|            9|                 1|         Sunday|              false|        2023|            1|          1|           0|             9|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:04:...|  2023-01-01 00:29:21| 2023-01-01 00:04:43|       2023|           1|         1|          0|           29|                 1|         Sunday|              false|        2023|            1|          1|           0|            29|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:04:...|  2023-01-01 00:45:38| 2023-01-01 00:04:45|       2023|           1|         1|          0|           45|                 1|         Sunday|              false|        2023|            1|          1|           0|            45|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:05:...|  2023-01-01 00:08:25| 2023-01-01 00:05:05|       2023|           1|         1|          0|            8|                 1|         Sunday|              false|        2023|            1|          1|           0|             8|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:05:...|  2023-01-01 00:11:13| 2023-01-01 00:05:09|       2023|           1|         1|          0|           11|                 1|         Sunday|              false|        2023|            1|          1|           0|            11|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:05:...|  2023-01-01 00:33:15| 2023-01-01 00:05:11|       2023|           1|         1|          0|           33|                 1|         Sunday|              false|        2023|            1|          1|           0|            33|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:05:...|  2023-01-01 00:10:37| 2023-01-01 00:05:12|       2023|           1|         1|          0|           10|                 1|         Sunday|              false|        2023|            1|          1|           0|            10|                  1|          Sunday|               false|\n",
      "+--------------------+---------------------+--------------------+-----------+------------+----------+-----------+-------------+------------------+---------------+-------------------+------------+-------------+-----------+------------+--------------+-------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "time_table_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5af65243-f358-4913-9232-e6c78dc53901",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(datetime_id='2023-01-01 00:32:10_2023-01-01 00:40:36', tpep_dropoff_datetime=datetime.datetime(2023, 1, 1, 0, 40, 36), tpep_pickup_datetime=datetime.datetime(2023, 1, 1, 0, 32, 10), pickup_year=2023, pickup_month=1, pickup_day=1, pickup_hour=0, pickup_minute=40, pickup_day_of_week=1, pickup_day_name='Sunday', pickup_is_month_end=False, dropoff_year=2023, dropoff_month=1, dropoff_day=1, dropoff_hour=0, dropoff_minute=40, dropoff_day_of_week=1, dropoff_day_name='Sunday', dropoff_is_month_end=False)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_table_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "69ea55fa-0374-4489-be08-cc6984eaa742",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+------------+------------+------------+\n",
      "|         datetime_id|ratecodeid|store_and_fwd_flag|pulocationid|dolocationid|payment_type|\n",
      "+--------------------+----------+------------------+------------+------------+------------+\n",
      "|2023-01-01 00:32:...|       1.0|                 N|         161|         141|           2|\n",
      "|2023-01-01 00:55:...|       1.0|                 N|          43|         237|           1|\n",
      "|2023-01-01 00:25:...|       1.0|                 N|          48|         238|           1|\n",
      "|2023-01-01 00:03:...|       1.0|                 N|         138|           7|           1|\n",
      "|2023-01-01 00:10:...|       1.0|                 N|         107|          79|           1|\n",
      "|2023-01-01 00:50:...|       1.0|                 N|         161|         137|           1|\n",
      "|2023-01-01 00:09:...|       1.0|                 N|         239|         143|           1|\n",
      "|2023-01-01 00:27:...|       1.0|                 N|         142|         200|           1|\n",
      "|2023-01-01 00:21:...|       1.0|                 N|         164|         236|           1|\n",
      "|2023-01-01 00:39:...|       1.0|                 N|         141|         107|           2|\n",
      "|2023-01-01 00:53:...|       1.0|                 N|         234|          68|           1|\n",
      "|2023-01-01 00:43:...|       1.0|                 N|          79|         264|           1|\n",
      "|2023-01-01 00:34:...|       1.0|                 N|         164|         143|           1|\n",
      "|2023-01-01 00:09:...|       1.0|                 N|         138|          33|           1|\n",
      "|2023-01-01 00:33:...|       1.0|                 N|          33|          61|           1|\n",
      "|2023-01-01 00:13:...|       1.0|                 N|          79|         186|           1|\n",
      "|2023-01-01 00:45:...|       1.0|                 N|          90|          48|           1|\n",
      "|2023-01-01 00:04:...|       1.0|                 N|         113|         255|           1|\n",
      "|2023-01-01 00:03:...|       1.0|                 N|         237|         239|           2|\n",
      "|2023-01-01 00:15:...|       1.0|                 N|         143|         229|           2|\n",
      "+--------------------+----------+------------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dimension_table_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd24c819-82f3-4f30-a698-e9fab9c8c8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[datetime_id: string, ratecodeid: double, store_and_fwd_flag: string, pulocationid: bigint, dolocationid: bigint, payment_type: bigint]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension_table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cb8d8b2-ee82-4440-a750-79d9d124ed9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[datetime_id: string, tpep_dropoff_datetime: timestamp, tpep_pickup_datetime: timestamp, pickup_year: int, pickup_month: int, pickup_day: int, pickup_hour: int, pickup_minute: int, pickup_day_of_week: int, pickup_day_name: string, pickup_is_month_end: boolean, dropoff_year: int, dropoff_month: int, dropoff_day: int, dropoff_hour: int, dropoff_minute: int, dropoff_day_of_week: int, dropoff_day_name: string, dropoff_is_month_end: boolean]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f978eea7-5d0e-4a0e-9030-732c95cb191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+--------------------+-----------+------------+----------+-----------+-------------+------------------+---------------+-------------------+------------+-------------+-----------+------------+--------------+-------------------+----------------+--------------------+\n",
      "|         datetime_id|tpep_dropoff_datetime|tpep_pickup_datetime|pickup_year|pickup_month|pickup_day|pickup_hour|pickup_minute|pickup_day_of_week|pickup_day_name|pickup_is_month_end|dropoff_year|dropoff_month|dropoff_day|dropoff_hour|dropoff_minute|dropoff_day_of_week|dropoff_day_name|dropoff_is_month_end|\n",
      "+--------------------+---------------------+--------------------+-----------+------------+----------+-----------+-------------+------------------+---------------+-------------------+------------+-------------+-----------+------------+--------------+-------------------+----------------+--------------------+\n",
      "|2008-12-31 23:04:...|  2009-01-01 19:55:36| 2008-12-31 23:04:41|       2009|           1|         1|         19|           55|                 5|       Thursday|              false|        2009|            1|          1|          19|            55|                  5|        Thursday|               false|\n",
      "|2022-10-25 01:59:...|  2022-10-25 02:09:02| 2022-10-25 01:59:02|       2022|          10|        25|          2|            9|                 3|        Tuesday|              false|        2022|           10|         25|           2|             9|                  3|         Tuesday|               false|\n",
      "|2023-01-01 00:01:...|  2023-01-01 00:22:01| 2023-01-01 00:01:07|       2023|           1|         1|          0|           22|                 1|         Sunday|              false|        2023|            1|          1|           0|            22|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:01:...|  2023-01-01 00:17:01| 2023-01-01 00:01:18|       2023|           1|         1|          0|           17|                 1|         Sunday|              false|        2023|            1|          1|           0|            17|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:01:...|  2023-01-01 00:21:30| 2023-01-01 00:01:24|       2023|           1|         1|          0|           21|                 1|         Sunday|              false|        2023|            1|          1|           0|            21|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:02:...|  2023-01-01 00:44:02| 2023-01-01 00:02:19|       2023|           1|         1|          0|           44|                 1|         Sunday|              false|        2023|            1|          1|           0|            44|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:02:...|  2023-01-01 00:29:57| 2023-01-01 00:02:29|       2023|           1|         1|          0|           29|                 1|         Sunday|              false|        2023|            1|          1|           0|            29|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:02:...|  2023-01-01 00:12:15| 2023-01-01 00:02:33|       2023|           1|         1|          0|           12|                 1|         Sunday|              false|        2023|            1|          1|           0|            12|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:02:...|  2023-01-01 00:08:01| 2023-01-01 00:02:59|       2023|           1|         1|          0|            8|                 1|         Sunday|              false|        2023|            1|          1|           0|             8|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:03:...|  2023-01-01 00:17:51| 2023-01-01 00:03:48|       2023|           1|         1|          0|           17|                 1|         Sunday|              false|        2023|            1|          1|           0|            17|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:03:...|  2023-01-01 00:18:27| 2023-01-01 00:03:52|       2023|           1|         1|          0|           18|                 1|         Sunday|              false|        2023|            1|          1|           0|            18|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:03:...|  2023-01-01 00:37:00| 2023-01-01 00:03:53|       2023|           1|         1|          0|           37|                 1|         Sunday|              false|        2023|            1|          1|           0|            37|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:04:...|  2023-01-01 00:09:49| 2023-01-01 00:04:28|       2023|           1|         1|          0|            9|                 1|         Sunday|              false|        2023|            1|          1|           0|             9|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:04:...|  2023-01-01 00:09:33| 2023-01-01 00:04:32|       2023|           1|         1|          0|            9|                 1|         Sunday|              false|        2023|            1|          1|           0|             9|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:04:...|  2023-01-01 00:29:21| 2023-01-01 00:04:43|       2023|           1|         1|          0|           29|                 1|         Sunday|              false|        2023|            1|          1|           0|            29|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:04:...|  2023-01-01 00:45:38| 2023-01-01 00:04:45|       2023|           1|         1|          0|           45|                 1|         Sunday|              false|        2023|            1|          1|           0|            45|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:05:...|  2023-01-01 00:08:25| 2023-01-01 00:05:05|       2023|           1|         1|          0|            8|                 1|         Sunday|              false|        2023|            1|          1|           0|             8|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:05:...|  2023-01-01 00:11:13| 2023-01-01 00:05:09|       2023|           1|         1|          0|           11|                 1|         Sunday|              false|        2023|            1|          1|           0|            11|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:05:...|  2023-01-01 00:33:15| 2023-01-01 00:05:11|       2023|           1|         1|          0|           33|                 1|         Sunday|              false|        2023|            1|          1|           0|            33|                  1|          Sunday|               false|\n",
      "|2023-01-01 00:05:...|  2023-01-01 00:10:37| 2023-01-01 00:05:12|       2023|           1|         1|          0|           10|                 1|         Sunday|              false|        2023|            1|          1|           0|            10|                  1|          Sunday|               false|\n",
      "+--------------------+---------------------+--------------------+-----------+------------+----------+-----------+-------------+------------------+---------------+-------------------+------------+-------------+-----------+------------+--------------+-------------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "time_table_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bb5f529-b5ea-41a3-ba28-39227de575b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/19 00:49:21 ERROR Executor: Exception in task 5.0 in stage 26.0 (TID 99)8]\n",
      "java.sql.BatchUpdateException: Batch entry 0 INSERT INTO raw.datetime_trip_table (\"datetime_id\",\"tpep_dropoff_datetime\",\"tpep_pickup_datetime\",\"pickup_year\",\"pickup_month\",\"pickup_day\",\"pickup_hour\",\"pickup_minute\",\"pickup_day_of_week\",\"pickup_day_name\",\"pickup_is_month_end\",\"dropoff_year\",\"dropoff_month\",\"dropoff_day\",\"dropoff_hour\",\"dropoff_minute\",\"dropoff_day_of_week\",\"dropoff_day_name\",\"dropoff_is_month_end\") VALUES ('2008-12-31 23:01:42_2009-01-01 14:29:11','2009-01-01 14:29:11+00'::timestamp,'2008-12-31 23:01:42+00'::timestamp,2009,1,1,14,29,5,'Thursday','FALSE',2009,1,1,14,29,5,'Thursday','FALSE') was aborted: ERROR: null value in column \"pickup_is_year_end\" of relation \"datetime_trip_table\" violates not-null constraint\n",
      "  Detail: Failing row contains (2008-12-31 23:01:42_2009-01-01 14:29:11, 2009-01-01 14:29:11, 2008-12-31 23:01:42, 2009, 1, 1, 14, 29, 5, Thursday, f, null, 2009, 1, 1, 14, 29, 5, Thursday, f, null).  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2133)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1490)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1515)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:559)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1677)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:723)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:890)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: null value in column \"pickup_is_year_end\" of relation \"datetime_trip_table\" violates not-null constraint\n",
      "  Detail: Failing row contains (2008-12-31 23:01:42_2009-01-01 14:29:11, 2009-01-01 14:29:11, 2008-12-31 23:01:42, 2009, 1, 1, 14, 29, 5, Thursday, f, null, 2009, 1, 1, 14, 29, 5, Thursday, f, null).\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2712)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2400)\n",
      "\t... 21 more\n",
      "24/02/19 00:49:21 ERROR TaskSetManager: Task 5 in stage 26.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1000.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 26.0 failed 1 times, most recent failure: Lost task 5.0 in stage 26.0 (TID 99) (idowu-pc.home executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO raw.datetime_trip_table (\"datetime_id\",\"tpep_dropoff_datetime\",\"tpep_pickup_datetime\",\"pickup_year\",\"pickup_month\",\"pickup_day\",\"pickup_hour\",\"pickup_minute\",\"pickup_day_of_week\",\"pickup_day_name\",\"pickup_is_month_end\",\"dropoff_year\",\"dropoff_month\",\"dropoff_day\",\"dropoff_hour\",\"dropoff_minute\",\"dropoff_day_of_week\",\"dropoff_day_name\",\"dropoff_is_month_end\") VALUES ('2008-12-31 23:01:42_2009-01-01 14:29:11','2009-01-01 14:29:11+00'::timestamp,'2008-12-31 23:01:42+00'::timestamp,2009,1,1,14,29,5,'Thursday','FALSE',2009,1,1,14,29,5,'Thursday','FALSE') was aborted: ERROR: null value in column \"pickup_is_year_end\" of relation \"datetime_trip_table\" violates not-null constraint\n  Detail: Failing row contains (2008-12-31 23:01:42_2009-01-01 14:29:11, 2009-01-01 14:29:11, 2008-12-31 23:01:42, 2009, 1, 1, 14, 29, 5, Thursday, f, null, 2009, 1, 1, 14, 29, 5, Thursday, f, null).  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2133)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1490)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1515)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:559)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1677)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:723)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:890)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.postgresql.util.PSQLException: ERROR: null value in column \"pickup_is_year_end\" of relation \"datetime_trip_table\" violates not-null constraint\n  Detail: Failing row contains (2008-12-31 23:01:42_2009-01-01 14:29:11, 2009-01-01 14:29:11, 2008-12-31 23:01:42, 2009, 1, 1, 14, 29, 5, Thursday, f, null, 2009, 1, 1, 14, 29, 5, Thursday, f, null).\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2712)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2400)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:888)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:69)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO raw.datetime_trip_table (\"datetime_id\",\"tpep_dropoff_datetime\",\"tpep_pickup_datetime\",\"pickup_year\",\"pickup_month\",\"pickup_day\",\"pickup_hour\",\"pickup_minute\",\"pickup_day_of_week\",\"pickup_day_name\",\"pickup_is_month_end\",\"dropoff_year\",\"dropoff_month\",\"dropoff_day\",\"dropoff_hour\",\"dropoff_minute\",\"dropoff_day_of_week\",\"dropoff_day_name\",\"dropoff_is_month_end\") VALUES ('2008-12-31 23:01:42_2009-01-01 14:29:11','2009-01-01 14:29:11+00'::timestamp,'2008-12-31 23:01:42+00'::timestamp,2009,1,1,14,29,5,'Thursday','FALSE',2009,1,1,14,29,5,'Thursday','FALSE') was aborted: ERROR: null value in column \"pickup_is_year_end\" of relation \"datetime_trip_table\" violates not-null constraint\n  Detail: Failing row contains (2008-12-31 23:01:42_2009-01-01 14:29:11, 2009-01-01 14:29:11, 2008-12-31 23:01:42, 2009, 1, 1, 14, 29, 5, Thursday, f, null, 2009, 1, 1, 14, 29, 5, Thursday, f, null).  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2133)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1490)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1515)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:559)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1677)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:723)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:890)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: null value in column \"pickup_is_year_end\" of relation \"datetime_trip_table\" violates not-null constraint\n  Detail: Failing row contains (2008-12-31 23:01:42_2009-01-01 14:29:11, 2009-01-01 14:29:11, 2008-12-31 23:01:42, 2009, 1, 1, 14, 29, 5, Thursday, f, null, 2009, 1, 1, 14, 29, 5, Thursday, f, null).\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2712)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2400)\n\t... 21 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtime_table_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjdbc:postgresql://localhost:5433/ny_taxi_database\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdriver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.postgresql.Driver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdbtable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw.datetime_trip_table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43midowuuser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpassword\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpasswordguddy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:738\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1305\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1306\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1308\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1309\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1313\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1000.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 26.0 failed 1 times, most recent failure: Lost task 5.0 in stage 26.0 (TID 99) (idowu-pc.home executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO raw.datetime_trip_table (\"datetime_id\",\"tpep_dropoff_datetime\",\"tpep_pickup_datetime\",\"pickup_year\",\"pickup_month\",\"pickup_day\",\"pickup_hour\",\"pickup_minute\",\"pickup_day_of_week\",\"pickup_day_name\",\"pickup_is_month_end\",\"dropoff_year\",\"dropoff_month\",\"dropoff_day\",\"dropoff_hour\",\"dropoff_minute\",\"dropoff_day_of_week\",\"dropoff_day_name\",\"dropoff_is_month_end\") VALUES ('2008-12-31 23:01:42_2009-01-01 14:29:11','2009-01-01 14:29:11+00'::timestamp,'2008-12-31 23:01:42+00'::timestamp,2009,1,1,14,29,5,'Thursday','FALSE',2009,1,1,14,29,5,'Thursday','FALSE') was aborted: ERROR: null value in column \"pickup_is_year_end\" of relation \"datetime_trip_table\" violates not-null constraint\n  Detail: Failing row contains (2008-12-31 23:01:42_2009-01-01 14:29:11, 2009-01-01 14:29:11, 2008-12-31 23:01:42, 2009, 1, 1, 14, 29, 5, Thursday, f, null, 2009, 1, 1, 14, 29, 5, Thursday, f, null).  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2133)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1490)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1515)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:559)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1677)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:723)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:890)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.postgresql.util.PSQLException: ERROR: null value in column \"pickup_is_year_end\" of relation \"datetime_trip_table\" violates not-null constraint\n  Detail: Failing row contains (2008-12-31 23:01:42_2009-01-01 14:29:11, 2009-01-01 14:29:11, 2008-12-31 23:01:42, 2009, 1, 1, 14, 29, 5, Thursday, f, null, 2009, 1, 1, 14, 29, 5, Thursday, f, null).\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2712)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2400)\n\t... 21 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:888)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:69)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO raw.datetime_trip_table (\"datetime_id\",\"tpep_dropoff_datetime\",\"tpep_pickup_datetime\",\"pickup_year\",\"pickup_month\",\"pickup_day\",\"pickup_hour\",\"pickup_minute\",\"pickup_day_of_week\",\"pickup_day_name\",\"pickup_is_month_end\",\"dropoff_year\",\"dropoff_month\",\"dropoff_day\",\"dropoff_hour\",\"dropoff_minute\",\"dropoff_day_of_week\",\"dropoff_day_name\",\"dropoff_is_month_end\") VALUES ('2008-12-31 23:01:42_2009-01-01 14:29:11','2009-01-01 14:29:11+00'::timestamp,'2008-12-31 23:01:42+00'::timestamp,2009,1,1,14,29,5,'Thursday','FALSE',2009,1,1,14,29,5,'Thursday','FALSE') was aborted: ERROR: null value in column \"pickup_is_year_end\" of relation \"datetime_trip_table\" violates not-null constraint\n  Detail: Failing row contains (2008-12-31 23:01:42_2009-01-01 14:29:11, 2009-01-01 14:29:11, 2008-12-31 23:01:42, 2009, 1, 1, 14, 29, 5, Thursday, f, null, 2009, 1, 1, 14, 29, 5, Thursday, f, null).  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2401)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2133)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1490)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1515)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:559)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:896)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:919)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1677)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:723)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:890)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: null value in column \"pickup_is_year_end\" of relation \"datetime_trip_table\" violates not-null constraint\n  Detail: Failing row contains (2008-12-31 23:01:42_2009-01-01 14:29:11, 2009-01-01 14:29:11, 2008-12-31 23:01:42, 2009, 1, 1, 14, 29, 5, Thursday, f, null, 2009, 1, 1, 14, 29, 5, Thursday, f, null).\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2712)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2400)\n\t... 21 more\n"
     ]
    }
   ],
   "source": [
    "time_table_data.write.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://localhost:5433/ny_taxi_database\").option(\"driver\", \"org.postgresql.Driver\").mode(\"append\").option(\"dbtable\", \"raw.datetime_trip_table\").option(\"user\",\"idowuuser\").option(\"password\", \"passwordguddy\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a14a8a6-b6d0-47f1-a5ee-828f79ba8e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.9.tar.gz (384 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: psycopg2\n",
      "  Building wheel for psycopg2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for psycopg2: filename=psycopg2-2.9.9-cp311-cp311-linux_x86_64.whl size=168870 sha256=fbbe6c7d4898c723143686b24e2edfd65306451396b690ce9c82db9a320cbfa9\n",
      "  Stored in directory: /home/idowuileks/.cache/pip/wheels/ab/34/b9/78ebef1b3220b4840ee482461e738566c3c9165d2b5c914f51\n",
      "Successfully built psycopg2\n",
      "Installing collected packages: psycopg2\n",
      "Successfully installed psycopg2-2.9.9\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff10f922-3551-4a6d-adf6-3dfbbc99b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine,sql\n",
    "import pandas as pd\n",
    "engine = create_engine('postgresql+psycopg2://idowuuser:passwordguddy@localhost:5433/ny_taxi_database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b975680-1871-424d-b8dd-cfe99c88dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f7870c9-acbe-4df6-a53f-6c731d7025c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"SELECT * FROM raw.test_new_new LIMIT 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e250bfec-3f85-4031-813b-b96e38ac04be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 19)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_sql(sql_query, con=engine).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31464e-876f-4c90-b408-01050935cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension_table_data, time_table_data, fact_data_table = transform_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d0d82a72-5665-4e42-9482-6aa34f790dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_load_data_db(df, table_name, schema_name):\n",
    "    sql_query_check_data = f\"\"\"SELECT * FROM {schema_name}.{table_name}\"\"\"\n",
    "    if pd.read_sql(sql_query, con=engine).shape[0] > 3:\n",
    "        df.repartition(100).write.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://localhost:5433/ny_taxi_database\").option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", f\"{schema_name}.{table_name}\").option(\"user\",\"idowuuser\").option(\"password\", \"passwordguddy\").mode('append').save()\n",
    "    else:\n",
    "        df.repartition(100).write.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://localhost:5433/ny_taxi_database\").option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", f\"{schema_name}.{table_name}\").option(\"user\",\"idowuuser\").option(\"password\", \"passwordguddy\").mode('overwrite').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99528b02-6961-4035-aff0-e10d08e7c7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "read_load_data_db(dimension_table_data, 'ny_taxi_dimension_table', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed4f5a67-75c5-4a8b-9674-14297ad4b68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "read_load_data_db(time_table_data, 'ny_taxi_time_table', 'raw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9965f81e-dd68-47e9-a756-697c31941944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "read_load_data_db(fact_data_table, 'ny_taxi_fact_table', 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5150f6a-d3b4-47a4-bd41-0177567ca7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:====================================================>  (96 + 4) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.378268480300903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start_time = time()\n",
    "df.repartition(100).write.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://localhost:5433/ny_taxi_database\").option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", \"raw.test_new_new\").option(\"user\",\"idowuuser\").option(\"password\", \"passwordguddy\").mode('overwrite').save()\n",
    "end_time = time()\n",
    "\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c97c61d-f930-488e-91f8-766506237e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d1325b-39b4-45a6-9ffc-b68f63ea2e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, lower, to_timestamp, date_format, udf, monotonically_increasing_id\n",
    "\n",
    "def transform_data_pyspark(data_df):\n",
    "    \"\"\"\n",
    "    Transforms a PySpark DataFrame containing taxi trip data into dimension and fact tables.\n",
    "\n",
    "    Args:\n",
    "        data_df (pyspark.sql.DataFrame): The input DataFrame containing taxi trip data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple(pyspark.sql.DataFrame, pyspark.sql.DataFrame, pyspark.sql.DataFrame):\n",
    "            - dimension_table: The dimension table containing date-related and static attributes.\n",
    "            - time_table: The time table containing extracted date and time components from timestamps.\n",
    "            - fact_table: The fact table containing aggregated trip metrics associated with datetimes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lowercase column names for consistency\n",
    "    data_df = data_df.withColumnRenamed('*', lower(col('*')))\n",
    "\n",
    "    # Define fact and dimension table columns\n",
    "    fact_columns = [\n",
    "        'tpep_dropoff_datetime', 'tpep_pickup_datetime', 'passenger_count', 'trip_distance',\n",
    "        'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge',\n",
    "        'total_amount', 'congestion_surcharge', 'airport_fee'\n",
    "    ]\n",
    "    dimension_columns = [\n",
    "        'tpep_dropoff_datetime', 'tpep_pickup_datetime', 'ratecodeid', 'store_and_fwd_flag',\n",
    "        'pulocationid', 'dolocationid', 'payment_type'\n",
    "    ]\n",
    "\n",
    "    # Extract date and time components using to_timestamp and date_format\n",
    "    time_table_df = data_df.select(\n",
    "        'tpep_dropoff_datetime', 'tpep_pickup_datetime'\n",
    "    ).withColumn('tpep_dropoff_datetime', to_timestamp(col('tpep_dropoff_datetime'))) \\\n",
    "       .withColumn('tpep_pickup_datetime', to_timestamp(col('tpep_pickup_datetime'))) \\\n",
    "       .withColumn('pickup_year', date_format(col('tpep_pickup_datetime'), 'yyyy')) \\\n",
    "       .withColumn('pickup_month', date_format(col('tpep_pickup_datetime'), 'MM')) \\\n",
    "       .withColumn('pickup_day', date_format(col('tpep_pickup_datetime'), 'dd')) \\\n",
    "       .withColumn('pickup_hour', date_format(col('tpep_pickup_datetime'), 'HH')) \\\n",
    "       .withColumn('pickup_minute', date_format(col('tpep_pickup_datetime'), 'mm')) \\\n",
    "       .withColumn('pickup_day_of_week', date_format(col('tpep_pickup_datetime'), 'E')) \\\n",
    "       .withColumn('pickup_day_name', date_format(col('tpep_pickup_datetime'), 'EEEE')) \\\n",
    "       .withColumn('pickup_is_month_end', date_format(col('tpep_pickup_datetime'), 'd').cast('int') == 31) \\\n",
    "       .withColumn('pickup_is_year_end', date_format(col('tpep_pickup_datetime'), 'MM').cast('int') == 12) \\\n",
    "       .withColumn('dropoff_year', date_format(col('tpep_dropoff_datetime'), 'yyyy')) \\\n",
    "       .withColumn('dropoff_month', date_format(col('tpep_dropoff_datetime'), 'MM')) \\\n",
    "       .withColumn('dropoff_day', date_format(col('tpep_dropoff_datetime'), 'dd')) \\\n",
    "       .withColumn('dropoff_hour', date_format(col('tpep_dropoff_datetime'), 'HH')) \\\n",
    "       .withColumn('dropoff_minute', date_format(col('tpep_dropoff_datetime'), 'mm')) \\\n",
    "       .withColumn('dropoff_day_of_week', date_format(col('tpep_dropoff_datetime'), 'E')) \\\n",
    "       .withColumn('dropoff_day_name', date_format(col('tpep_dropoff_datetime'), 'EEEE')) \\\n",
    "       .withColumn('dropoff_is_month_end', date_format(col('tpep_dropoff_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
