2024-02-29 04:06:35,963 INFO - Task context logging is enabled
2024-02-29 04:06:35,964 INFO - Loaded executor: LocalExecutor
2024-02-29 04:06:35,998 INFO - Starting the scheduler
2024-02-29 04:06:35,999 INFO - Processing each file at most -1 times
2024-02-29 04:06:36,118 INFO - Launched DagFileProcessorManager with pid: 1188951
2024-02-29 04:06:36,119 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 04:06:36,123 INFO - Configured default timezone UTC
2024-02-29 04:11:36,190 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 04:16:36,225 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 04:17:32,826 INFO - Setting next_dagrun for run_pipeline_new to 2022-02-01 00:00:00+00:00, run_after=2022-03-01 00:00:00+00:00
2024-02-29 04:17:32,861 INFO - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:17:31.854773+00:00: manual__2024-02-29T04:17:31.854773+00:00, state:running, queued_at: 2024-02-29 04:17:31.876787+00:00. externally triggered: True> successful
2024-02-29 04:17:32,862 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:17:31.854773+00:00, run_id=manual__2024-02-29T04:17:31.854773+00:00, run_start_date=2024-02-29 04:17:32.834738+00:00, run_end_date=2024-02-29 04:17:32.861974+00:00, run_duration=0.027236, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=b0bb47566357eee3e1d41de1b5d0b6c5
2024-02-29 04:17:32,874 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:17:32,874 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 04:17:32,874 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:17:32,877 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:17:32,877 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:17:32,877 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:17:32,880 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:17:32,902 INFO - Setting next_dagrun for run_pipeline_new to 2022-03-01 00:00:00+00:00, run_after=2022-04-01 00:00:00+00:00
2024-02-29 04:17:32,911 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:17:32,940 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:17:32,940 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 04:17:32,941 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:17:32,943 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:17:32,943 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:17:32,944 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:17:32,946 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:17:32,969 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 04:17:32,987 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:17:33,077 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:17:33,078 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 04:17:33,078 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:17:33,081 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:17:33,083 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:17:33,084 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:17:33,087 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:17:33,176 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:17:40,788 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2022-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:17:40,803 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2022-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:17:40,885 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2022-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:17:41,453 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:17:41,462 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:17:41,527 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:17:42,025 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:17:42,026 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-03-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:17:42,026 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:17:42,034 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2022-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:17:40.934807+00:00, run_end_date=2024-02-29 04:17:41.334419+00:00, run_duration=0.399612, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=2, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:17:32.875237+00:00, queued_by_job_id=1, pid=1202769
2024-02-29 04:17:42,034 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2022-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:17:41.036897+00:00, run_end_date=2024-02-29 04:17:41.420939+00:00, run_duration=0.384042, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=4, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:17:32.941868+00:00, queued_by_job_id=1, pid=1202779
2024-02-29 04:17:42,035 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2022-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:17:40.945304+00:00, run_end_date=2024-02-29 04:17:41.359622+00:00, run_duration=0.414318, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=3, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:17:33.079339+00:00, queued_by_job_id=1, pid=1202771
2024-02-29 04:21:36,280 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 04:23:10,905 INFO - Setting next_dagrun for run_pipeline_new to 2022-02-01 00:00:00+00:00, run_after=2022-03-01 00:00:00+00:00
2024-02-29 04:23:10,929 INFO - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:23:10.457423+00:00: manual__2024-02-29T04:23:10.457423+00:00, state:running, queued_at: 2024-02-29 04:23:10.475513+00:00. externally triggered: True> successful
2024-02-29 04:23:10,929 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:23:10.457423+00:00, run_id=manual__2024-02-29T04:23:10.457423+00:00, run_start_date=2024-02-29 04:23:10.911576+00:00, run_end_date=2024-02-29 04:23:10.929477+00:00, run_duration=0.017901, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=63912eae4476e2279fc992c8d23e2126
2024-02-29 04:23:10,939 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:23:10,939 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 04:23:10,939 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:23:10,941 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:23:10,941 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:23:10,941 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:23:10,943 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:23:10,964 INFO - Setting next_dagrun for run_pipeline_new to 2022-03-01 00:00:00+00:00, run_after=2022-04-01 00:00:00+00:00
2024-02-29 04:23:10,977 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:23:11,004 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:23:11,004 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 04:23:11,004 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:23:11,005 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:23:11,006 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:23:11,006 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:23:11,008 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:23:11,030 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 04:23:11,044 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:23:11,139 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:23:11,140 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 04:23:11,140 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:23:11,150 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:23:11,151 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:23:11,152 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:23:11,156 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:23:11,267 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:23:18,473 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2022-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:23:18,582 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2022-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:23:18,673 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2022-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:23:19,106 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:23:19,220 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:23:19,327 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:23:19,591 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:23:19,591 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:23:19,591 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-03-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:23:19,594 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2022-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:23:18.710112+00:00, run_end_date=2024-02-29 04:23:19.105751+00:00, run_duration=0.395639, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=6, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:23:10.940193+00:00, queued_by_job_id=1, pid=1209501
2024-02-29 04:23:19,595 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2022-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:23:18.606241+00:00, run_end_date=2024-02-29 04:23:19.015779+00:00, run_duration=0.409538, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=5, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:23:11.004840+00:00, queued_by_job_id=1, pid=1209491
2024-02-29 04:23:19,595 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2022-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:23:18.810894+00:00, run_end_date=2024-02-29 04:23:19.204143+00:00, run_duration=0.393249, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=7, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:23:11.141578+00:00, queued_by_job_id=1, pid=1209507
2024-02-29 04:26:36,333 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 04:27:58,601 INFO - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00
2024-02-29 04:27:58,627 INFO - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:27:57.979634+00:00: manual__2024-02-29T04:27:57.979634+00:00, state:running, queued_at: 2024-02-29 04:27:58.000441+00:00. externally triggered: True> successful
2024-02-29 04:27:58,627 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:27:57.979634+00:00, run_id=manual__2024-02-29T04:27:57.979634+00:00, run_start_date=2024-02-29 04:27:58.608535+00:00, run_end_date=2024-02-29 04:27:58.627731+00:00, run_duration=0.019196, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=0700539a944a05390069faa886aa1c81
2024-02-29 04:27:58,639 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:27:58,640 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 04:27:58,640 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:27:58,641 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:27:58,641 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:27:58,641 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:27:58,644 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:27:58,666 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 04:27:58,676 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:27:58,706 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:27:58,707 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 04:27:58,707 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:27:58,708 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:27:58,708 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:27:58,709 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:27:58,711 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:27:58,732 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 04:27:58,754 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:27:58,832 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:27:58,833 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 04:27:58,833 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:27:58,839 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:27:58,841 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:27:58,843 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:27:58,847 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:27:58,891 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:28:06,456 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:28:06,597 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:28:06,742 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:28:07,183 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:28:07,301 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:28:07,438 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:28:07,725 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:28:07,725 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:28:07,725 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:28:07,729 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:28:06.608244+00:00, run_end_date=2024-02-29 04:28:07.061299+00:00, run_duration=0.453055, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=8, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:27:58.640518+00:00, queued_by_job_id=1, pid=1214349
2024-02-29 04:28:07,730 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:28:06.739853+00:00, run_end_date=2024-02-29 04:28:07.184610+00:00, run_duration=0.444757, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=9, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:27:58.707723+00:00, queued_by_job_id=1, pid=1214358
2024-02-29 04:28:07,730 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:28:06.887426+00:00, run_end_date=2024-02-29 04:28:07.328477+00:00, run_duration=0.441051, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=10, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:27:58.834415+00:00, queued_by_job_id=1, pid=1214367
2024-02-29 04:31:36,390 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 04:33:07,127 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:33:07,127 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 04:33:07,127 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:33:07,129 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:33:07,129 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:33:07,131 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:33:07,162 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:33:08,249 INFO - 2 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:33:08,249 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 04:33:08,249 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 04:33:08,249 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:33:08,251 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:33:08,252 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:33:08,252 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:33:08,252 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:33:08,255 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:33:08,255 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:33:08,297 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:33:08,300 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:33:13,546 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:33:14,261 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:33:14,975 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:33:15,075 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:33:15,152 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=2, map_index=-1)
2024-02-29 04:33:15,159 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:33:13.708271+00:00, run_end_date=2024-02-29 04:33:14.152092+00:00, run_duration=0.443821, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=11, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:33:07.128304+00:00, queued_by_job_id=1, pid=1219439
2024-02-29 04:33:15,504 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:33:15,620 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:33:16,208 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1)
2024-02-29 04:33:16,208 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=2, map_index=-1)
2024-02-29 04:33:16,211 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:33:15.090836+00:00, run_end_date=2024-02-29 04:33:15.430831+00:00, run_duration=0.339995, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=12, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:33:08.250163+00:00, queued_by_job_id=1, pid=1219517
2024-02-29 04:33:16,212 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:33:15.204115+00:00, run_end_date=2024-02-29 04:33:15.533559+00:00, run_duration=0.329444, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=13, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:33:08.250163+00:00, queued_by_job_id=1, pid=1219533
2024-02-29 04:36:36,441 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 04:38:14,655 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:38:14,655 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 04:38:14,656 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:38:14,658 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:38:14,658 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:38:14,661 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:38:14,696 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:38:15,981 INFO - 2 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:38:15,981 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 04:38:15,981 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 04:38:15,981 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:38:15,983 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:38:15,984 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:38:15,984 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:38:15,985 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:38:15,987 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:38:15,987 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:38:16,023 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:38:16,024 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:38:26,716 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:38:26,753 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:38:26,784 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:38:27,458 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:38:27,502 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:38:27,510 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:38:28,057 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=3, map_index=-1)
2024-02-29 04:38:28,057 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=3, map_index=-1)
2024-02-29 04:38:28,057 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=3, map_index=-1)
2024-02-29 04:38:28,061 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:38:26.919917+00:00, run_end_date=2024-02-29 04:38:27.350712+00:00, run_duration=0.430795, state=failed, executor_state=failed, try_number=3, max_tries=2, job_id=14, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:38:15.982363+00:00, queued_by_job_id=1, pid=1225380
2024-02-29 04:38:28,061 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:38:26.947618+00:00, run_end_date=2024-02-29 04:38:27.385942+00:00, run_duration=0.438324, state=failed, executor_state=failed, try_number=3, max_tries=2, job_id=16, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:38:15.982363+00:00, queued_by_job_id=1, pid=1225382
2024-02-29 04:38:28,062 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:38:26.950422+00:00, run_end_date=2024-02-29 04:38:27.390567+00:00, run_duration=0.440145, state=failed, executor_state=failed, try_number=3, max_tries=2, job_id=15, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:38:14.656806+00:00, queued_by_job_id=1, pid=1225383
2024-02-29 04:38:30,134 ERROR - Marking run <DagRun run_pipeline_new @ 2021-02-01 00:00:00+00:00: scheduled__2021-02-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 04:27:58.661225+00:00. externally triggered: False> failed
2024-02-29 04:38:30,135 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-02-01 00:00:00+00:00, run_id=scheduled__2021-02-01T00:00:00+00:00, run_start_date=2024-02-29 04:27:58.674452+00:00, run_end_date=2024-02-29 04:38:30.135226+00:00, run_duration=631.460774, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2021-02-01 00:00:00+00:00, data_interval_end=2021-03-01 00:00:00+00:00, dag_hash=5273ba1f006f49bf7a7b0567010a411e
2024-02-29 04:38:30,138 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 04:38:30,141 ERROR - Marking run <DagRun run_pipeline_new @ 2021-01-01 00:00:00+00:00: scheduled__2021-01-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 04:27:58.595259+00:00. externally triggered: False> failed
2024-02-29 04:38:30,142 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-01-01 00:00:00+00:00, run_id=scheduled__2021-01-01T00:00:00+00:00, run_start_date=2024-02-29 04:27:58.608366+00:00, run_end_date=2024-02-29 04:38:30.142247+00:00, run_duration=631.533881, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2021-01-01 00:00:00+00:00, data_interval_end=2021-02-01 00:00:00+00:00, dag_hash=5273ba1f006f49bf7a7b0567010a411e
2024-02-29 04:38:30,145 INFO - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00
2024-02-29 04:38:30,147 ERROR - Marking run <DagRun run_pipeline_new @ 2021-03-01 00:00:00+00:00: scheduled__2021-03-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 04:27:58.728407+00:00. externally triggered: False> failed
2024-02-29 04:38:30,148 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-03-01 00:00:00+00:00, run_id=scheduled__2021-03-01T00:00:00+00:00, run_start_date=2024-02-29 04:27:58.745161+00:00, run_end_date=2024-02-29 04:38:30.148204+00:00, run_duration=631.403043, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2021-03-01 00:00:00+00:00, data_interval_end=2021-04-01 00:00:00+00:00, dag_hash=5273ba1f006f49bf7a7b0567010a411e
2024-02-29 04:38:30,151 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 04:39:26,612 INFO - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00
2024-02-29 04:39:26,632 INFO - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:39:25.741704+00:00: manual__2024-02-29T04:39:25.741704+00:00, state:running, queued_at: 2024-02-29 04:39:25.756239+00:00. externally triggered: True> successful
2024-02-29 04:39:26,632 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:39:25.741704+00:00, run_id=manual__2024-02-29T04:39:25.741704+00:00, run_start_date=2024-02-29 04:39:26.619363+00:00, run_end_date=2024-02-29 04:39:26.632640+00:00, run_duration=0.013277, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=949ca78e8da1ea9cf8e37558275a1c37
2024-02-29 04:39:26,642 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:39:26,642 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 04:39:26,642 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:39:26,643 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:39:26,644 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:39:26,644 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:39:26,646 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:39:26,667 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 04:39:26,676 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:39:26,705 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:39:26,705 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 04:39:26,706 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:39:26,707 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:39:26,708 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:39:26,708 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:39:26,710 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:39:26,733 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 04:39:26,750 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:39:26,793 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:39:26,795 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 04:39:26,796 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:39:26,800 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:39:26,802 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:39:26,803 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:39:26,808 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:39:26,926 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:39:34,363 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:39:34,518 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:39:34,772 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:39:35,045 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:39:35,175 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:39:35,347 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:39:35,723 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:39:35,723 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:39:35,723 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:39:35,726 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:39:34.524475+00:00, run_end_date=2024-02-29 04:39:34.915523+00:00, run_duration=0.391048, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=17, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:39:26.642953+00:00, queued_by_job_id=1, pid=1226841
2024-02-29 04:39:35,726 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:39:34.668247+00:00, run_end_date=2024-02-29 04:39:35.065218+00:00, run_duration=0.396971, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=18, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:39:26.706713+00:00, queued_by_job_id=1, pid=1226850
2024-02-29 04:39:35,727 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:39:34.889754+00:00, run_end_date=2024-02-29 04:39:35.257885+00:00, run_duration=0.368131, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=19, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:39:26.798331+00:00, queued_by_job_id=1, pid=1226864
2024-02-29 04:41:36,473 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 04:41:45,689 INFO - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00
2024-02-29 04:41:45,713 INFO - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:41:45.155619+00:00: manual__2024-02-29T04:41:45.155619+00:00, state:running, queued_at: 2024-02-29 04:41:45.174863+00:00. externally triggered: True> successful
2024-02-29 04:41:45,714 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:41:45.155619+00:00, run_id=manual__2024-02-29T04:41:45.155619+00:00, run_start_date=2024-02-29 04:41:45.697305+00:00, run_end_date=2024-02-29 04:41:45.713944+00:00, run_duration=0.016639, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=dfb27b41414c37c1881b257f4cbcb04c
2024-02-29 04:41:45,726 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:41:45,727 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 04:41:45,727 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:41:45,728 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:41:45,729 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:41:45,729 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:41:45,731 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:41:45,756 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 04:41:45,766 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:41:45,798 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:41:45,798 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 04:41:45,798 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:41:45,800 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:41:45,801 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:41:45,801 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:41:45,803 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:41:45,828 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 04:41:45,844 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:41:45,925 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:41:45,926 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 04:41:45,926 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:41:45,933 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:41:45,934 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:41:45,934 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:41:45,940 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:41:45,988 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:41:53,510 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:41:53,759 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:41:53,793 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:41:54,173 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:41:54,398 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:41:54,421 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:41:54,422 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:41:54,427 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:41:53.664503+00:00, run_end_date=2024-02-29 04:41:54.070130+00:00, run_duration=0.405627, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=20, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:41:45.727817+00:00, queued_by_job_id=1, pid=1229627
2024-02-29 04:41:54,427 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:41:53.904062+00:00, run_end_date=2024-02-29 04:41:54.294809+00:00, run_duration=0.390747, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=21, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:41:45.927669+00:00, queued_by_job_id=1, pid=1229643
2024-02-29 04:41:54,468 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:41:55,478 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:41:55,482 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:41:53.947508+00:00, run_end_date=2024-02-29 04:41:54.340786+00:00, run_duration=0.393278, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=22, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:41:45.799513+00:00, queued_by_job_id=1, pid=1229646
2024-02-29 04:45:56,888 INFO - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00
2024-02-29 04:45:56,924 INFO - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:45:56.721018+00:00: manual__2024-02-29T04:45:56.721018+00:00, state:running, queued_at: 2024-02-29 04:45:56.735353+00:00. externally triggered: True> successful
2024-02-29 04:45:56,924 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:45:56.721018+00:00, run_id=manual__2024-02-29T04:45:56.721018+00:00, run_start_date=2024-02-29 04:45:56.899286+00:00, run_end_date=2024-02-29 04:45:56.924791+00:00, run_duration=0.025505, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=fb6a14e7ad8c0f278d846e8b299cfe28
2024-02-29 04:45:56,943 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:45:56,943 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 04:45:56,943 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:45:56,945 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:45:56,946 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:45:56,947 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:45:56,951 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:45:56,986 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 04:45:57,001 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:45:57,033 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:45:57,034 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 04:45:57,034 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:45:57,037 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:45:57,038 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:45:57,038 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:45:57,041 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:45:57,089 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 04:45:57,091 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:45:57,191 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:45:57,192 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 04:45:57,196 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:45:57,204 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:45:57,206 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:45:57,208 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:45:57,212 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:45:57,274 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:46:04,796 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:46:04,839 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:46:05,044 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:46:05,497 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:46:05,509 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:46:05,639 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:46:06,079 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:46:06,079 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:46:06,079 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:46:06,083 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:46:04.958031+00:00, run_end_date=2024-02-29 04:46:05.383049+00:00, run_duration=0.425018, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=23, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:45:57.035370+00:00, queued_by_job_id=1, pid=1233725
2024-02-29 04:46:06,083 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:46:04.978724+00:00, run_end_date=2024-02-29 04:46:05.390347+00:00, run_duration=0.411623, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=24, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:45:56.944534+00:00, queued_by_job_id=1, pid=1233727
2024-02-29 04:46:06,083 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:46:05.181803+00:00, run_end_date=2024-02-29 04:46:05.554559+00:00, run_duration=0.372756, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=25, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:45:57.201165+00:00, queued_by_job_id=1, pid=1233741
2024-02-29 04:46:36,521 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 04:51:05,547 INFO - 2 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:51:05,547 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 04:51:05,547 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 04:51:05,547 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:51:05,549 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:51:05,549 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:51:05,549 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:51:05,549 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:51:05,552 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:51:05,552 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:51:05,582 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:51:05,585 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:51:05,612 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:51:05,613 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 04:51:05,613 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:51:05,615 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:51:05,615 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:51:05,618 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:51:05,656 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:51:12,176 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:51:12,196 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:51:12,252 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:51:12,778 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:51:12,789 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:51:12,846 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:51:12,905 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=2, map_index=-1)
2024-02-29 04:51:12,905 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=2, map_index=-1)
2024-02-29 04:51:12,905 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1)
2024-02-29 04:51:12,909 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:51:12.312814+00:00, run_end_date=2024-02-29 04:51:12.661598+00:00, run_duration=0.348784, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=26, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:51:05.548183+00:00, queued_by_job_id=1, pid=1239188
2024-02-29 04:51:12,909 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:51:12.333020+00:00, run_end_date=2024-02-29 04:51:12.702707+00:00, run_duration=0.369687, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=27, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:51:05.548183+00:00, queued_by_job_id=1, pid=1239191
2024-02-29 04:51:12,910 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:51:12.378625+00:00, run_end_date=2024-02-29 04:51:12.740694+00:00, run_duration=0.362069, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=28, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:51:05.613820+00:00, queued_by_job_id=1, pid=1239196
2024-02-29 04:51:36,576 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 04:53:12,997 INFO - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00
2024-02-29 04:53:13,022 INFO - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:53:12.457965+00:00: manual__2024-02-29T04:53:12.457965+00:00, state:running, queued_at: 2024-02-29 04:53:12.476586+00:00. externally triggered: True> successful
2024-02-29 04:53:13,023 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:53:12.457965+00:00, run_id=manual__2024-02-29T04:53:12.457965+00:00, run_start_date=2024-02-29 04:53:13.005339+00:00, run_end_date=2024-02-29 04:53:13.023109+00:00, run_duration=0.01777, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=219df785d51861aa772da8da671cfbf2
2024-02-29 04:53:13,032 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:53:13,033 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 04:53:13,033 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:53:13,034 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:53:13,034 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:53:13,034 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:53:13,038 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:53:13,060 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 04:53:13,072 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:53:13,103 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:53:13,104 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 04:53:13,104 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:53:13,106 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:53:13,107 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:53:13,107 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:53:13,110 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:53:13,133 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 04:53:13,150 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:53:13,215 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:53:13,216 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 04:53:13,217 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:53:13,222 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:53:13,225 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:53:13,227 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:53:13,232 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:53:13,307 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:53:20,991 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:53:21,028 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:53:21,224 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:53:21,639 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:53:21,639 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:53:21,807 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:53:22,193 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:53:22,193 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:53:22,194 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:53:22,198 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:53:21.138752+00:00, run_end_date=2024-02-29 04:53:21.523813+00:00, run_duration=0.385061, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=29, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:53:13.105495+00:00, queued_by_job_id=1, pid=1241838
2024-02-29 04:53:22,198 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:53:21.178747+00:00, run_end_date=2024-02-29 04:53:21.545601+00:00, run_duration=0.366854, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=30, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:53:13.033686+00:00, queued_by_job_id=1, pid=1241841
2024-02-29 04:53:22,199 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:53:21.357353+00:00, run_end_date=2024-02-29 04:53:21.739670+00:00, run_duration=0.382317, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=31, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:53:13.219901+00:00, queued_by_job_id=1, pid=1241854
2024-02-29 04:54:54,158 INFO - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00
2024-02-29 04:54:54,181 INFO - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:54:53.464879+00:00: manual__2024-02-29T04:54:53.464879+00:00, state:running, queued_at: 2024-02-29 04:54:53.482051+00:00. externally triggered: True> successful
2024-02-29 04:54:54,182 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:54:53.464879+00:00, run_id=manual__2024-02-29T04:54:53.464879+00:00, run_start_date=2024-02-29 04:54:54.165182+00:00, run_end_date=2024-02-29 04:54:54.182246+00:00, run_duration=0.017064, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=a9758587c093af9a140a345eaceaec7b
2024-02-29 04:54:54,193 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:54:54,193 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 04:54:54,193 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:54:54,194 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:54:54,194 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:54:54,195 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:54:54,197 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:54:54,218 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 04:54:54,230 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:54:54,262 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:54:54,263 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 04:54:54,263 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:54:54,265 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:54:54,266 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:54:54,266 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:54:54,268 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:54:54,291 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 04:54:54,307 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:54:54,350 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:54:54,351 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 04:54:54,351 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:54:54,357 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:54:54,357 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:54:54,357 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:54:54,360 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:54:54,446 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:55:02,228 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:55:02,289 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:55:02,290 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:55:02,893 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:55:02,908 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:55:02,963 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:55:03,409 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:55:03,410 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:55:03,410 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:55:03,413 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:55:02.371658+00:00, run_end_date=2024-02-29 04:55:02.779009+00:00, run_duration=0.407351, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=32, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:54:54.193810+00:00, queued_by_job_id=1, pid=1244250
2024-02-29 04:55:03,413 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:55:02.437325+00:00, run_end_date=2024-02-29 04:55:02.826409+00:00, run_duration=0.389084, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=33, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:54:54.354199+00:00, queued_by_job_id=1, pid=1244253
2024-02-29 04:55:03,414 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:55:02.442798+00:00, run_end_date=2024-02-29 04:55:02.842106+00:00, run_duration=0.399308, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=34, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:54:54.264228+00:00, queued_by_job_id=1, pid=1244254
2024-02-29 04:56:36,610 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 04:57:05,553 INFO - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00
2024-02-29 04:57:05,583 INFO - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:57:04.930203+00:00: manual__2024-02-29T04:57:04.930203+00:00, state:running, queued_at: 2024-02-29 04:57:04.949478+00:00. externally triggered: True> successful
2024-02-29 04:57:05,584 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:57:04.930203+00:00, run_id=manual__2024-02-29T04:57:04.930203+00:00, run_start_date=2024-02-29 04:57:05.562897+00:00, run_end_date=2024-02-29 04:57:05.584314+00:00, run_duration=0.021417, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=cac94c935a47a1195a453bdf831e8376
2024-02-29 04:57:05,599 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:57:05,599 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 04:57:05,599 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:57:05,601 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:57:05,602 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:57:05,602 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:57:05,605 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:57:05,635 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 04:57:05,654 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:57:05,700 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:57:05,701 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 04:57:05,701 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:57:05,703 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:57:05,703 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:57:05,703 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:57:05,706 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:57:05,734 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 04:57:05,758 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:57:05,813 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:57:05,814 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 04:57:05,814 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 04:57:05,816 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 04:57:05,817 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 04:57:05,817 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:57:05,820 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 04:57:05,927 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 04:57:14,797 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:57:14,808 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:57:14,981 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 04:57:15,434 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:57:15,471 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:57:15,618 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 04:57:15,765 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:57:15,765 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:57:15,765 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 04:57:15,769 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:57:14.971406+00:00, run_end_date=2024-02-29 04:57:15.342611+00:00, run_duration=0.371205, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=36, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:57:05.600389+00:00, queued_by_job_id=1, pid=1247341
2024-02-29 04:57:15,769 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:57:14.958132+00:00, run_end_date=2024-02-29 04:57:15.366373+00:00, run_duration=0.408241, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=35, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:57:05.702017+00:00, queued_by_job_id=1, pid=1247339
2024-02-29 04:57:15,769 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:57:15.118832+00:00, run_end_date=2024-02-29 04:57:15.502874+00:00, run_duration=0.384042, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=37, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:57:05.815440+00:00, queued_by_job_id=1, pid=1247354
2024-02-29 05:00:01,270 INFO - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00
2024-02-29 05:00:01,299 INFO - Marking run <DagRun run_pipeline_new @ 2024-02-29 05:00:01.200082+00:00: manual__2024-02-29T05:00:01.200082+00:00, state:running, queued_at: 2024-02-29 05:00:01.220164+00:00. externally triggered: True> successful
2024-02-29 05:00:01,300 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 05:00:01.200082+00:00, run_id=manual__2024-02-29T05:00:01.200082+00:00, run_start_date=2024-02-29 05:00:01.278171+00:00, run_end_date=2024-02-29 05:00:01.300352+00:00, run_duration=0.022181, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=5a5b3eddbb84d33a7c7b34b2eac70ec4
2024-02-29 05:00:01,313 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:00:01,313 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 05:00:01,313 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:00:01,315 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:00:01,316 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:00:01,316 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:00:01,318 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:00:01,350 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 05:00:01,371 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:00:01,428 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:00:01,429 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 05:00:01,430 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:00:01,437 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:00:01,438 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:00:01,439 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:00:01,443 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:00:01,478 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 05:00:01,515 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:00:01,606 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:00:01,606 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 05:00:01,607 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:00:01,611 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:00:01,611 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:00:01,611 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:00:01,614 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:00:01,669 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:00:10,913 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:00:10,983 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:00:11,098 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:00:11,605 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:00:11,675 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:00:11,792 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:00:12,343 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:00:12,343 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:00:12,343 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:00:12,347 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:00:11.111151+00:00, run_end_date=2024-02-29 05:00:11.541316+00:00, run_duration=0.430165, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=39, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:00:01.314412+00:00, queued_by_job_id=1, pid=1250394
2024-02-29 05:00:12,347 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:00:11.046752+00:00, run_end_date=2024-02-29 05:00:11.469216+00:00, run_duration=0.422464, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=38, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:00:01.434902+00:00, queued_by_job_id=1, pid=1250387
2024-02-29 05:00:12,347 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:00:11.250391+00:00, run_end_date=2024-02-29 05:00:11.676651+00:00, run_duration=0.42626, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=40, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:00:01.607971+00:00, queued_by_job_id=1, pid=1250403
2024-02-29 05:01:36,674 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 05:02:30,233 INFO - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00
2024-02-29 05:02:30,284 INFO - Marking run <DagRun run_pipeline_new @ 2024-02-29 05:02:30.054785+00:00: manual__2024-02-29T05:02:30.054785+00:00, state:running, queued_at: 2024-02-29 05:02:30.075138+00:00. externally triggered: True> successful
2024-02-29 05:02:30,285 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 05:02:30.054785+00:00, run_id=manual__2024-02-29T05:02:30.054785+00:00, run_start_date=2024-02-29 05:02:30.252732+00:00, run_end_date=2024-02-29 05:02:30.285491+00:00, run_duration=0.032759, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=6735f483cef949895fd4d7ee81492e55
2024-02-29 05:02:30,307 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:02:30,308 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 05:02:30,308 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:02:30,311 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:02:30,312 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:02:30,312 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:02:30,316 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:02:30,352 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 05:02:30,382 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:02:30,440 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:02:30,441 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 05:02:30,442 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:02:30,446 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:02:30,448 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:02:30,449 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:02:30,452 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:02:30,486 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 05:02:30,505 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:02:30,593 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:02:30,594 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 05:02:30,594 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:02:30,601 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:02:30,602 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:02:30,602 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:02:30,607 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:02:30,676 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:02:39,001 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:02:39,033 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:02:39,126 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:02:39,664 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:02:39,692 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:02:39,724 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:02:40,045 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:02:40,045 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:02:40,046 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:02:40,049 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:02:39.140823+00:00, run_end_date=2024-02-29 05:02:39.536895+00:00, run_duration=0.396072, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=41, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:02:30.309401+00:00, queued_by_job_id=1, pid=1253263
2024-02-29 05:02:40,049 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:02:39.172001+00:00, run_end_date=2024-02-29 05:02:39.561834+00:00, run_duration=0.389833, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=42, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:02:30.444206+00:00, queued_by_job_id=1, pid=1253266
2024-02-29 05:02:40,049 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:02:39.251833+00:00, run_end_date=2024-02-29 05:02:39.622877+00:00, run_duration=0.371044, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=43, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:02:30.598401+00:00, queued_by_job_id=1, pid=1253274
2024-02-29 05:05:26,227 INFO - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00
2024-02-29 05:05:26,248 INFO - Marking run <DagRun run_pipeline_new @ 2024-02-29 05:05:25.561301+00:00: manual__2024-02-29T05:05:25.561301+00:00, state:running, queued_at: 2024-02-29 05:05:25.577236+00:00. externally triggered: True> successful
2024-02-29 05:05:26,249 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 05:05:25.561301+00:00, run_id=manual__2024-02-29T05:05:25.561301+00:00, run_start_date=2024-02-29 05:05:26.234234+00:00, run_end_date=2024-02-29 05:05:26.249314+00:00, run_duration=0.01508, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=b272ad5d99ee7edad64d8cd05483073b
2024-02-29 05:05:26,262 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:05:26,262 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 05:05:26,263 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:05:26,264 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:05:26,265 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:05:26,265 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:05:26,267 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:05:26,294 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 05:05:26,316 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:05:26,345 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:05:26,346 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 05:05:26,346 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:05:26,347 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:05:26,348 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:05:26,348 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:05:26,350 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:05:26,371 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 05:05:26,386 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:05:26,467 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:05:26,468 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 05:05:26,476 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:05:26,483 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:05:26,485 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:05:26,493 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:05:26,498 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:05:26,605 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:05:34,499 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:05:34,889 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:05:34,918 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:05:44,374 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:05:44,795 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:05:44,802 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:05:34.675941+00:00, run_end_date=2024-02-29 05:05:44.233088+00:00, run_duration=9.557147, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=44, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:05:26.263554+00:00, queued_by_job_id=1, pid=1257050
2024-02-29 05:06:36,786 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 05:07:52,777 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:07:52,777 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 05:07:52,778 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:07:52,787 WARNING - cannot record scheduled_duration for task remove_file because previous state change time has not been saved
2024-02-29 05:07:52,787 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2024-02-29 05:07:52,788 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:07:52,793 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:07:52,847 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:07:52,877 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:07:52,953 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:07:52,964 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:05:35.024993+00:00, run_end_date=2024-02-29 05:07:52.593537+00:00, run_duration=137.568544, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=45, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:05:26.346763+00:00, queued_by_job_id=1, pid=1257074
2024-02-29 05:08:02,512 INFO - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:08:03,757 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:08:04,099 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:08:04,099 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 05:08:04,099 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:08:04,101 WARNING - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved
2024-02-29 05:08:04,101 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-02-29 05:08:04,101 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:08:04,104 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:08:04,104 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:08:04,104 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:08:04,114 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:08:02.645462+00:00, run_end_date=2024-02-29 05:08:03.127516+00:00, run_duration=0.482054, state=success, executor_state=success, try_number=1, max_tries=2, job_id=47, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:07:52.783067+00:00, queued_by_job_id=1, pid=1260213
2024-02-29 05:08:04,114 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:05:35.069284+00:00, run_end_date=2024-02-29 05:08:03.642126+00:00, run_duration=148.572842, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=46, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:05:26.479022+00:00, queued_by_job_id=1, pid=1257080
2024-02-29 05:08:04,136 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:08:08,898 INFO - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:08:14,956 INFO - Marking run <DagRun run_pipeline_new @ 2021-02-01 00:00:00+00:00: scheduled__2021-02-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:05:26.285240+00:00. externally triggered: False> successful
2024-02-29 05:08:14,956 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-02-01 00:00:00+00:00, run_id=scheduled__2021-02-01T00:00:00+00:00, run_start_date=2024-02-29 05:05:26.303253+00:00, run_end_date=2024-02-29 05:08:14.956802+00:00, run_duration=168.653549, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-02-01 00:00:00+00:00, data_interval_end=2021-03-01 00:00:00+00:00, dag_hash=7045eba0332add008305f32237c9dd7a
2024-02-29 05:08:14,960 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 05:08:14,984 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:08:14,987 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:08:09.011884+00:00, run_end_date=2024-02-29 05:08:14.610615+00:00, run_duration=5.598731, state=success, executor_state=success, try_number=1, max_tries=2, job_id=48, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:08:04.100215+00:00, queued_by_job_id=1, pid=1260507
2024-02-29 05:08:16,009 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 05:10:44,757 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:10:44,757 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 05:10:44,758 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:10:44,760 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:10:44,760 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:10:44,762 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:10:44,792 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:10:49,108 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:11:36,863 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 05:12:14,884 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:12:15,843 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:12:15,844 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 05:12:15,844 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:12:15,845 WARNING - cannot record scheduled_duration for task remove_file because previous state change time has not been saved
2024-02-29 05:12:15,846 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2024-02-29 05:12:15,846 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:12:15,851 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=2, map_index=-1)
2024-02-29 05:12:15,851 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:12:15,855 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:10:49.207086+00:00, run_end_date=2024-02-29 05:12:14.798061+00:00, run_duration=85.590975, state=success, executor_state=failed, try_number=2, max_tries=2, job_id=49, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:10:44.758858+00:00, queued_by_job_id=1, pid=1262806
2024-02-29 05:12:15,885 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:12:20,668 INFO - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:12:21,222 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:12:21,222 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 05:12:21,223 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:12:21,224 WARNING - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved
2024-02-29 05:12:21,225 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-02-29 05:12:21,225 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:12:21,227 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:12:21,268 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:12:21,285 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:12:21,291 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:12:20.779733+00:00, run_end_date=2024-02-29 05:12:21.148578+00:00, run_duration=0.368845, state=success, executor_state=success, try_number=1, max_tries=2, job_id=50, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:12:15.844623+00:00, queued_by_job_id=1, pid=1264876
2024-02-29 05:12:26,333 INFO - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:12:30,853 INFO - Marking run <DagRun run_pipeline_new @ 2021-01-01 00:00:00+00:00: scheduled__2021-01-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:05:26.222966+00:00. externally triggered: False> successful
2024-02-29 05:12:30,853 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-01-01 00:00:00+00:00, run_id=scheduled__2021-01-01T00:00:00+00:00, run_start_date=2024-02-29 05:05:26.234063+00:00, run_end_date=2024-02-29 05:12:30.853942+00:00, run_duration=424.619879, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-01-01 00:00:00+00:00, data_interval_end=2021-02-01 00:00:00+00:00, dag_hash=7045eba0332add008305f32237c9dd7a
2024-02-29 05:12:30,857 INFO - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00
2024-02-29 05:12:30,874 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:12:30,877 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:12:26.460913+00:00, run_end_date=2024-02-29 05:12:30.610446+00:00, run_duration=4.149533, state=success, executor_state=success, try_number=1, max_tries=2, job_id=51, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:12:21.223556+00:00, queued_by_job_id=1, pid=1265108
2024-02-29 05:12:31,691 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 05:12:32,743 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 05:13:04,095 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:13:04,096 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 05:13:04,096 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:13:04,097 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:13:04,097 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:13:04,099 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:13:04,128 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:13:09,134 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:14:07,000 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:14:07,152 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1)
2024-02-29 05:14:07,155 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:13:09.247710+00:00, run_end_date=2024-02-29 05:14:06.856940+00:00, run_duration=57.60923, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=52, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:13:04.096749+00:00, queued_by_job_id=1, pid=1266669
2024-02-29 05:16:36,901 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 05:21:36,936 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 05:21:50,930 INFO - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00
2024-02-29 05:21:50,956 INFO - Marking run <DagRun run_pipeline_new @ 2024-02-29 05:21:50.183886+00:00: manual__2024-02-29T05:21:50.183886+00:00, state:running, queued_at: 2024-02-29 05:21:50.196666+00:00. externally triggered: True> successful
2024-02-29 05:21:50,956 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 05:21:50.183886+00:00, run_id=manual__2024-02-29T05:21:50.183886+00:00, run_start_date=2024-02-29 05:21:50.939352+00:00, run_end_date=2024-02-29 05:21:50.956890+00:00, run_duration=0.017538, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=b24eaff9a4d995eab8b07fde9da3d752
2024-02-29 05:21:50,968 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:21:50,969 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 05:21:50,969 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:21:50,970 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:21:50,970 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:21:50,971 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:21:50,973 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:21:50,998 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 05:21:51,009 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:21:51,044 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:21:51,044 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 05:21:51,044 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:21:51,046 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:21:51,047 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:21:51,047 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:21:51,050 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:21:51,076 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 05:21:51,093 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:21:51,211 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:21:51,211 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 05:21:51,212 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:21:51,214 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:21:51,214 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:21:51,214 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:21:51,217 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:21:51,256 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:21:58,944 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:21:58,945 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:21:59,018 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:22:26,096 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:22:26,900 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:22:26,901 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 05:22:26,901 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:22:26,903 WARNING - cannot record scheduled_duration for task remove_file because previous state change time has not been saved
2024-02-29 05:22:26,904 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2024-02-29 05:22:26,904 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:22:26,906 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:22:26,906 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:22:26,912 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:21:59.185348+00:00, run_end_date=2024-02-29 05:22:25.969520+00:00, run_duration=26.784172, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=55, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:21:51.045151+00:00, queued_by_job_id=1, pid=1277499
2024-02-29 05:22:26,957 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:22:34,112 INFO - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:22:34,944 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:22:34,945 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 05:22:34,945 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:22:34,949 WARNING - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved
2024-02-29 05:22:34,950 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-02-29 05:22:34,950 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:22:34,953 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:22:34,955 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:22:34,960 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:22:34.323870+00:00, run_end_date=2024-02-29 05:22:34.786195+00:00, run_duration=0.462325, state=success, executor_state=success, try_number=1, max_tries=2, job_id=56, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:22:26.902039+00:00, queued_by_job_id=1, pid=1278339
2024-02-29 05:22:35,009 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:22:46,378 INFO - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:22:56,616 INFO - Marking run <DagRun run_pipeline_new @ 2021-02-01 00:00:00+00:00: scheduled__2021-02-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:21:50.991765+00:00. externally triggered: False> successful
2024-02-29 05:22:56,616 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-02-01 00:00:00+00:00, run_id=scheduled__2021-02-01T00:00:00+00:00, run_start_date=2024-02-29 05:21:51.007629+00:00, run_end_date=2024-02-29 05:22:56.616602+00:00, run_duration=65.608973, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-02-01 00:00:00+00:00, data_interval_end=2021-03-01 00:00:00+00:00, dag_hash=a801ea1cd406b22c7deb8998d8cbf99a
2024-02-29 05:22:56,620 INFO - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00
2024-02-29 05:22:56,647 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:22:56,651 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:22:46.589528+00:00, run_end_date=2024-02-29 05:22:56.210204+00:00, run_duration=9.620676, state=success, executor_state=success, try_number=1, max_tries=2, job_id=57, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:22:34.946982+00:00, queued_by_job_id=1, pid=1278756
2024-02-29 05:22:57,671 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 05:23:03,173 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:23:03,492 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:23:03,496 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:21:59.107064+00:00, run_end_date=2024-02-29 05:23:03.059986+00:00, run_duration=63.952922, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=53, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:21:50.969595+00:00, queued_by_job_id=1, pid=1277490
2024-02-29 05:23:16,127 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:23:16,914 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:23:16,917 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:21:59.116045+00:00, run_end_date=2024-02-29 05:23:15.965113+00:00, run_duration=76.849068, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=54, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:21:51.212795+00:00, queued_by_job_id=1, pid=1277492
2024-02-29 05:26:36,986 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 05:30:32,690 INFO - Setting next_dagrun for run_pipeline_new to 2021-04-01 00:00:00+00:00, run_after=2021-05-01 00:00:00+00:00
2024-02-29 05:30:32,722 INFO - Marking run <DagRun run_pipeline_new @ 2024-02-29 05:30:32.601434+00:00: manual__2024-02-29T05:30:32.601434+00:00, state:running, queued_at: 2024-02-29 05:30:32.616988+00:00. externally triggered: True> successful
2024-02-29 05:30:32,723 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 05:30:32.601434+00:00, run_id=manual__2024-02-29T05:30:32.601434+00:00, run_start_date=2024-02-29 05:30:32.697416+00:00, run_end_date=2024-02-29 05:30:32.722913+00:00, run_duration=0.025497, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=d0658b84929d53135d275ccf395a640f
2024-02-29 05:30:32,741 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:32,742 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 05:30:32,743 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:32,746 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:30:32,748 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:30:32,749 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:32,752 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:32,800 INFO - Setting next_dagrun for run_pipeline_new to 2021-05-01 00:00:00+00:00, run_after=2021-06-01 00:00:00+00:00
2024-02-29 05:30:32,805 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:30:32,881 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-04-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:32,882 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 05:30:32,882 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-04-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:32,888 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:30:32,890 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-04-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:30:32,891 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:32,896 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:32,926 INFO - Setting next_dagrun for run_pipeline_new to 2021-06-01 00:00:00+00:00, run_after=2021-07-01 00:00:00+00:00
2024-02-29 05:30:33,004 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:30:33,055 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-05-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:33,056 INFO - DAG run_pipeline_new has 2/16 running and queued tasks
2024-02-29 05:30:33,056 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-05-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:33,058 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:30:33,059 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-05-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:30:33,059 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:33,062 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:33,093 INFO - Setting next_dagrun for run_pipeline_new to 2021-07-01 00:00:00+00:00, run_after=2021-08-01 00:00:00+00:00
2024-02-29 05:30:33,114 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:30:33,282 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-06-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:33,283 INFO - DAG run_pipeline_new has 3/16 running and queued tasks
2024-02-29 05:30:33,284 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-06-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:33,288 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:30:33,289 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-06-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:30:33,289 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:33,293 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:33,340 INFO - Setting next_dagrun for run_pipeline_new to 2021-08-01 00:00:00+00:00, run_after=2021-09-01 00:00:00+00:00
2024-02-29 05:30:33,429 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:30:33,485 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-07-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:33,486 INFO - DAG run_pipeline_new has 4/16 running and queued tasks
2024-02-29 05:30:33,487 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-07-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:33,489 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:30:33,490 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-07-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:30:33,490 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:33,493 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:33,532 INFO - Setting next_dagrun for run_pipeline_new to 2021-09-01 00:00:00+00:00, run_after=2021-10-01 00:00:00+00:00
2024-02-29 05:30:33,598 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:30:33,698 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-08-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:33,699 INFO - DAG run_pipeline_new has 5/16 running and queued tasks
2024-02-29 05:30:33,700 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-08-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:33,702 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:30:33,703 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-08-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:30:33,704 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:33,706 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:33,741 INFO - Setting next_dagrun for run_pipeline_new to 2021-10-01 00:00:00+00:00, run_after=2021-11-01 00:00:00+00:00
2024-02-29 05:30:33,870 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:30:33,963 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-09-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:33,964 INFO - DAG run_pipeline_new has 6/16 running and queued tasks
2024-02-29 05:30:33,965 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-09-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:33,968 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:30:33,970 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-09-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:30:33,971 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:33,974 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:34,033 INFO - Setting next_dagrun for run_pipeline_new to 2021-11-01 00:00:00+00:00, run_after=2021-12-01 00:00:00+00:00
2024-02-29 05:30:34,097 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:30:34,264 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-10-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:34,265 INFO - DAG run_pipeline_new has 7/16 running and queued tasks
2024-02-29 05:30:34,266 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-10-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:34,270 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:30:34,271 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-10-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:30:34,272 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:34,276 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:34,322 INFO - Setting next_dagrun for run_pipeline_new to 2021-12-01 00:00:00+00:00, run_after=2022-01-01 00:00:00+00:00
2024-02-29 05:30:34,407 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:30:34,565 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-11-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:34,566 INFO - DAG run_pipeline_new has 8/16 running and queued tasks
2024-02-29 05:30:34,566 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-11-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:34,569 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:30:34,569 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-11-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:30:34,570 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-11-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:34,573 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-11-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:34,603 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 05:30:34,750 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:30:34,946 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-12-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:34,946 INFO - DAG run_pipeline_new has 9/16 running and queued tasks
2024-02-29 05:30:34,948 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-12-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:30:34,953 WARNING - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved
2024-02-29 05:30:34,954 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-12-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:30:34,954 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-12-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:34,957 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-12-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:30:35,124 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:30:55,010 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-04-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:30:55,456 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-07-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:30:56,145 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:30:56,264 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-05-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:30:56,401 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-06-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:30:56,623 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-09-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:30:56,815 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-10-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:30:58,057 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-08-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:30:58,327 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-12-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:30:59,003 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-11-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:31:37,276 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 05:35:33,894 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:35:34,894 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:35:34,903 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:56.549957+00:00, run_end_date=2024-02-29 05:35:33.508867+00:00, run_duration=276.95891, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=60, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:32.745567+00:00, queued_by_job_id=1, pid=1287144
2024-02-29 05:36:37,546 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 05:39:53,413 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:39:53,870 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-04-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:39:53,872 INFO - DAG run_pipeline_new has 8/16 running and queued tasks
2024-02-29 05:39:53,872 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-04-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:39:53,876 WARNING - cannot record scheduled_duration for task remove_file because previous state change time has not been saved
2024-02-29 05:39:53,876 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-04-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2024-02-29 05:39:53,877 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:39:53,976 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-04-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:39:53,975 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:39:53,981 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-04-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:55.664721+00:00, run_end_date=2024-02-29 05:39:53.237899+00:00, run_duration=537.573178, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=58, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:32.884880+00:00, queued_by_job_id=1, pid=1287087
2024-02-29 05:39:54,051 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:40:01,166 INFO - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-04-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:40:02,927 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-04-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:40:02,928 INFO - DAG run_pipeline_new has 8/16 running and queued tasks
2024-02-29 05:40:02,929 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-04-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:40:02,933 WARNING - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved
2024-02-29 05:40:02,933 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-04-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-02-29 05:40:02,934 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:40:02,945 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-04-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:40:02,947 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:40:02,952 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-04-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:40:01.361452+00:00, run_end_date=2024-02-29 05:40:02.306775+00:00, run_duration=0.945323, state=success, executor_state=success, try_number=1, max_tries=2, job_id=68, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:39:53.873977+00:00, queued_by_job_id=1, pid=1297641
2024-02-29 05:40:03,004 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:40:11,672 INFO - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-04-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:40:23,581 INFO - Marking run <DagRun run_pipeline_new @ 2021-04-01 00:00:00+00:00: scheduled__2021-04-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:32.790769+00:00. externally triggered: False> successful
2024-02-29 05:40:23,582 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-04-01 00:00:00+00:00, run_id=scheduled__2021-04-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:32.814613+00:00, run_end_date=2024-02-29 05:40:23.581944+00:00, run_duration=590.767331, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-04-01 00:00:00+00:00, data_interval_end=2021-05-01 00:00:00+00:00, dag_hash=52e54482f9af61e36762501cc3956179
2024-02-29 05:40:23,585 INFO - Setting next_dagrun for run_pipeline_new to 2021-05-01 00:00:00+00:00, run_after=2021-06-01 00:00:00+00:00
2024-02-29 05:40:23,764 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-04-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:40:23,770 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-04-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:40:11.914158+00:00, run_end_date=2024-02-29 05:40:23.310921+00:00, run_duration=11.396763, state=success, executor_state=success, try_number=1, max_tries=2, job_id=69, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:40:02.931437+00:00, queued_by_job_id=1, pid=1298001
2024-02-29 05:40:24,915 INFO - Setting next_dagrun for run_pipeline_new to 2021-06-01 00:00:00+00:00, run_after=2021-07-01 00:00:00+00:00
2024-02-29 05:40:26,201 INFO - Setting next_dagrun for run_pipeline_new to 2021-07-01 00:00:00+00:00, run_after=2021-08-01 00:00:00+00:00
2024-02-29 05:40:27,357 INFO - Setting next_dagrun for run_pipeline_new to 2021-08-01 00:00:00+00:00, run_after=2021-09-01 00:00:00+00:00
2024-02-29 05:40:27,568 INFO - Setting next_dagrun for run_pipeline_new to 2021-09-01 00:00:00+00:00, run_after=2021-10-01 00:00:00+00:00
2024-02-29 05:40:28,779 INFO - Setting next_dagrun for run_pipeline_new to 2021-10-01 00:00:00+00:00, run_after=2021-11-01 00:00:00+00:00
2024-02-29 05:40:29,453 INFO - Setting next_dagrun for run_pipeline_new to 2021-11-01 00:00:00+00:00, run_after=2021-12-01 00:00:00+00:00
2024-02-29 05:40:30,758 INFO - Setting next_dagrun for run_pipeline_new to 2021-12-01 00:00:00+00:00, run_after=2022-01-01 00:00:00+00:00
2024-02-29 05:40:31,915 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 05:40:34,048 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:40:34,048 INFO - DAG run_pipeline_new has 8/16 running and queued tasks
2024-02-29 05:40:34,048 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:40:34,050 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:40:34,051 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:40:34,079 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:40:34,126 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:40:41,697 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:41:37,838 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 05:42:04,718 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:42:05,483 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1)
2024-02-29 05:42:05,511 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:40:41.872994+00:00, run_end_date=2024-02-29 05:42:04.564561+00:00, run_duration=82.691567, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=70, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:40:34.049451+00:00, queued_by_job_id=1, pid=1298734
2024-02-29 05:46:38,142 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 05:47:04,874 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:47:04,875 INFO - DAG run_pipeline_new has 8/16 running and queued tasks
2024-02-29 05:47:04,875 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:47:04,876 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default
2024-02-29 05:47:04,876 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:47:04,886 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:47:04,936 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:47:12,636 INFO - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:48:33,202 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:48:33,917 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=3, map_index=-1)
2024-02-29 05:48:33,924 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:47:12.944133+00:00, run_end_date=2024-02-29 05:48:32.912542+00:00, run_duration=79.968409, state=failed, executor_state=failed, try_number=3, max_tries=2, job_id=71, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:47:04.875654+00:00, queued_by_job_id=1, pid=1306407
2024-02-29 05:48:36,281 ERROR - Marking run <DagRun run_pipeline_new @ 2021-03-01 00:00:00+00:00: scheduled__2021-03-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:32.684245+00:00. externally triggered: False> failed
2024-02-29 05:48:36,282 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-03-01 00:00:00+00:00, run_id=scheduled__2021-03-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:32.697201+00:00, run_end_date=2024-02-29 05:48:36.282093+00:00, run_duration=1083.584892, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2021-03-01 00:00:00+00:00, data_interval_end=2021-04-01 00:00:00+00:00, dag_hash=f8bdcd12abd45b309a61afad014086b7
2024-02-29 05:48:36,293 INFO - Setting next_dagrun for run_pipeline_new to 2021-04-01 00:00:00+00:00, run_after=2021-05-01 00:00:00+00:00
2024-02-29 05:48:37,444 INFO - Setting next_dagrun for run_pipeline_new to 2021-05-01 00:00:00+00:00, run_after=2021-06-01 00:00:00+00:00
2024-02-29 05:51:38,881 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 05:51:56,586 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:51:56,707 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-05-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:51:56,708 INFO - DAG run_pipeline_new has 7/16 running and queued tasks
2024-02-29 05:51:56,708 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-05-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:51:56,715 WARNING - cannot record scheduled_duration for task remove_file because previous state change time has not been saved
2024-02-29 05:51:56,716 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-05-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2024-02-29 05:51:56,716 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:51:56,721 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-05-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:51:56,721 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:51:56,764 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-05-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:56.741027+00:00, run_end_date=2024-02-29 05:51:56.044175+00:00, run_duration=1259.303148, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=61, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:33.057248+00:00, queued_by_job_id=1, pid=1287156
2024-02-29 05:51:57,151 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:52:17,124 INFO - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-05-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:52:20,363 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-05-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:52:20,363 INFO - DAG run_pipeline_new has 7/16 running and queued tasks
2024-02-29 05:52:20,364 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-05-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:52:20,367 WARNING - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved
2024-02-29 05:52:20,367 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-05-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-02-29 05:52:20,368 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:52:20,383 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-05-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:52:20,390 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:52:20,393 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-05-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:52:17.669410+00:00, run_end_date=2024-02-29 05:52:18.924166+00:00, run_duration=1.254756, state=success, executor_state=success, try_number=1, max_tries=2, job_id=72, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:51:56.709509+00:00, queued_by_job_id=1, pid=1313041
2024-02-29 05:52:20,487 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:52:47,370 INFO - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-05-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:53:03,672 INFO - Marking run <DagRun run_pipeline_new @ 2021-05-01 00:00:00+00:00: scheduled__2021-05-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:32.918929+00:00. externally triggered: False> successful
2024-02-29 05:53:03,676 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-05-01 00:00:00+00:00, run_id=scheduled__2021-05-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:32.947344+00:00, run_end_date=2024-02-29 05:53:03.676313+00:00, run_duration=1350.728969, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-05-01 00:00:00+00:00, data_interval_end=2021-06-01 00:00:00+00:00, dag_hash=61e6570e57d9d5c7d834633272ee188e
2024-02-29 05:53:03,683 INFO - Setting next_dagrun for run_pipeline_new to 2021-06-01 00:00:00+00:00, run_after=2021-07-01 00:00:00+00:00
2024-02-29 05:53:03,796 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-05-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:53:03,829 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-05-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:52:47.852987+00:00, run_end_date=2024-02-29 05:53:02.511328+00:00, run_duration=14.658341, state=success, executor_state=success, try_number=1, max_tries=2, job_id=73, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:52:20.364958+00:00, queued_by_job_id=1, pid=1313795
2024-02-29 05:53:04,887 INFO - Setting next_dagrun for run_pipeline_new to 2021-07-01 00:00:00+00:00, run_after=2021-08-01 00:00:00+00:00
2024-02-29 05:53:06,201 INFO - Setting next_dagrun for run_pipeline_new to 2021-08-01 00:00:00+00:00, run_after=2021-09-01 00:00:00+00:00
2024-02-29 05:53:07,441 INFO - Setting next_dagrun for run_pipeline_new to 2021-09-01 00:00:00+00:00, run_after=2021-10-01 00:00:00+00:00
2024-02-29 05:53:08,212 INFO - Setting next_dagrun for run_pipeline_new to 2021-10-01 00:00:00+00:00, run_after=2021-11-01 00:00:00+00:00
2024-02-29 05:53:08,901 INFO - Setting next_dagrun for run_pipeline_new to 2021-11-01 00:00:00+00:00, run_after=2021-12-01 00:00:00+00:00
2024-02-29 05:53:10,167 INFO - Setting next_dagrun for run_pipeline_new to 2021-12-01 00:00:00+00:00, run_after=2022-01-01 00:00:00+00:00
2024-02-29 05:53:11,353 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 05:53:39,835 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:53:40,799 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-07-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:53:40,800 INFO - DAG run_pipeline_new has 6/16 running and queued tasks
2024-02-29 05:53:40,800 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-07-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:53:40,803 WARNING - cannot record scheduled_duration for task remove_file because previous state change time has not been saved
2024-02-29 05:53:40,804 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-07-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2024-02-29 05:53:40,805 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:53:40,809 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-07-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:53:40,808 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:53:40,818 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-07-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:55.935263+00:00, run_end_date=2024-02-29 05:53:39.308342+00:00, run_duration=1363.373079, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=59, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:33.488077+00:00, queued_by_job_id=1, pid=1287097
2024-02-29 05:53:40,947 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:53:55,467 INFO - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-07-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:53:58,852 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-07-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:53:58,852 INFO - DAG run_pipeline_new has 6/16 running and queued tasks
2024-02-29 05:53:58,853 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-07-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:53:58,856 WARNING - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved
2024-02-29 05:53:58,856 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-07-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-02-29 05:53:58,857 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:53:58,867 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:53:58,879 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-07-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:53:58,897 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-07-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:53:56.164401+00:00, run_end_date=2024-02-29 05:53:57.703958+00:00, run_duration=1.539557, state=success, executor_state=success, try_number=1, max_tries=2, job_id=74, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:53:40.801295+00:00, queued_by_job_id=1, pid=1315436
2024-02-29 05:53:58,983 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:54:10,182 INFO - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-07-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:54:19,193 INFO - Marking run <DagRun run_pipeline_new @ 2021-07-01 00:00:00+00:00: scheduled__2021-07-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:33.328101+00:00. externally triggered: False> successful
2024-02-29 05:54:19,193 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-07-01 00:00:00+00:00, run_id=scheduled__2021-07-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:33.357537+00:00, run_end_date=2024-02-29 05:54:19.193679+00:00, run_duration=1425.836142, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-07-01 00:00:00+00:00, data_interval_end=2021-08-01 00:00:00+00:00, dag_hash=1166a2566c66f25a8594996b205f12fb
2024-02-29 05:54:19,199 INFO - Setting next_dagrun for run_pipeline_new to 2021-08-01 00:00:00+00:00, run_after=2021-09-01 00:00:00+00:00
2024-02-29 05:54:19,336 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-07-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:54:19,350 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-07-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:54:10.437814+00:00, run_end_date=2024-02-29 05:54:18.528618+00:00, run_duration=8.090804, state=success, executor_state=success, try_number=1, max_tries=2, job_id=75, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:53:58.854402+00:00, queued_by_job_id=1, pid=1315854
2024-02-29 05:54:20,380 INFO - Setting next_dagrun for run_pipeline_new to 2021-09-01 00:00:00+00:00, run_after=2021-10-01 00:00:00+00:00
2024-02-29 05:54:21,844 INFO - Setting next_dagrun for run_pipeline_new to 2021-10-01 00:00:00+00:00, run_after=2021-11-01 00:00:00+00:00
2024-02-29 05:54:23,046 INFO - Setting next_dagrun for run_pipeline_new to 2021-11-01 00:00:00+00:00, run_after=2021-12-01 00:00:00+00:00
2024-02-29 05:54:23,933 INFO - Setting next_dagrun for run_pipeline_new to 2021-12-01 00:00:00+00:00, run_after=2022-01-01 00:00:00+00:00
2024-02-29 05:54:25,178 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 05:55:19,191 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:55:19,312 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-08-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:55:19,313 INFO - DAG run_pipeline_new has 5/16 running and queued tasks
2024-02-29 05:55:19,314 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-08-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:55:19,320 WARNING - cannot record scheduled_duration for task remove_file because previous state change time has not been saved
2024-02-29 05:55:19,321 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-08-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2024-02-29 05:55:19,322 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:55:19,332 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-08-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:55:19,332 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:55:19,341 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-08-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:58.481280+00:00, run_end_date=2024-02-29 05:55:18.874584+00:00, run_duration=1460.393304, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=65, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:33.701085+00:00, queued_by_job_id=1, pid=1287246
2024-02-29 05:55:19,432 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:55:30,523 INFO - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-08-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:55:32,290 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-08-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:55:32,291 INFO - DAG run_pipeline_new has 5/16 running and queued tasks
2024-02-29 05:55:32,292 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-08-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:55:32,296 WARNING - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved
2024-02-29 05:55:32,297 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-08-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-02-29 05:55:32,297 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:55:32,305 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:55:32,306 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-08-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:55:32,318 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-08-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:55:30.868890+00:00, run_end_date=2024-02-29 05:55:31.962836+00:00, run_duration=1.093946, state=success, executor_state=success, try_number=1, max_tries=2, job_id=76, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:55:19.316409+00:00, queued_by_job_id=1, pid=1317809
2024-02-29 05:55:32,432 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:55:32,504 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:55:32,539 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-06-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:55:32,542 INFO - DAG run_pipeline_new has 5/16 running and queued tasks
2024-02-29 05:55:32,543 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-06-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:55:32,545 WARNING - cannot record scheduled_duration for task remove_file because previous state change time has not been saved
2024-02-29 05:55:32,546 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-06-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2024-02-29 05:55:32,546 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:55:32,551 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:55:32,562 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-06-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:55:32,569 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-06-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:56.846789+00:00, run_end_date=2024-02-29 05:55:32.276465+00:00, run_duration=1475.429676, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=62, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:33.285139+00:00, queued_by_job_id=1, pid=1287161
2024-02-29 05:55:32,737 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:55:41,991 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:55:42,384 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-09-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:55:42,385 INFO - DAG run_pipeline_new has 5/16 running and queued tasks
2024-02-29 05:55:42,386 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-09-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:55:42,395 WARNING - cannot record scheduled_duration for task remove_file because previous state change time has not been saved
2024-02-29 05:55:42,418 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-09-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2024-02-29 05:55:42,423 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:55:42,426 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:55:42,433 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-09-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:55:42,447 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-09-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:57.048215+00:00, run_end_date=2024-02-29 05:55:41.737733+00:00, run_duration=1484.689518, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=63, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:33.967179+00:00, queued_by_job_id=1, pid=1287182
2024-02-29 05:55:42,551 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:55:54,986 INFO - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-08-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:55:55,422 INFO - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-06-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:55:58,779 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-06-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:55:58,780 INFO - DAG run_pipeline_new has 5/16 running and queued tasks
2024-02-29 05:55:58,781 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-06-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:55:58,792 WARNING - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved
2024-02-29 05:55:58,793 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-06-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-02-29 05:55:58,798 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:55:58,814 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:55:58,825 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-06-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:55:58,855 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-06-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:55:55.958091+00:00, run_end_date=2024-02-29 05:55:57.244876+00:00, run_duration=1.286785, state=success, executor_state=success, try_number=1, max_tries=2, job_id=78, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:55:32.543925+00:00, queued_by_job_id=1, pid=1318700
2024-02-29 05:55:58,967 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:56:00,909 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:56:01,784 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-12-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:56:01,785 INFO - DAG run_pipeline_new has 5/16 running and queued tasks
2024-02-29 05:56:01,785 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-12-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:56:01,788 WARNING - cannot record scheduled_duration for task remove_file because previous state change time has not been saved
2024-02-29 05:56:01,791 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-12-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2024-02-29 05:56:01,791 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-12-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:56:01,817 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-12-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:56:01,820 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-12-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:56:01,832 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-12-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:58.610390+00:00, run_end_date=2024-02-29 05:56:00.670777+00:00, run_duration=1502.060387, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=66, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:34.949577+00:00, queued_by_job_id=1, pid=1287251
2024-02-29 05:56:02,122 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:56:02,444 INFO - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-09-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:56:05,455 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-09-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:56:05,457 INFO - DAG run_pipeline_new has 5/16 running and queued tasks
2024-02-29 05:56:05,458 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-09-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:56:05,478 WARNING - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved
2024-02-29 05:56:05,480 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-09-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-02-29 05:56:05,487 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:56:05,490 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:56:05,703 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:56:06,034 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-09-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:56:06,056 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-09-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:56:03.240873+00:00, run_end_date=2024-02-29 05:56:04.909496+00:00, run_duration=1.668623, state=success, executor_state=success, try_number=1, max_tries=2, job_id=79, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:55:42.388366+00:00, queued_by_job_id=1, pid=1319022
2024-02-29 05:56:10,512 INFO - Marking run <DagRun run_pipeline_new @ 2021-08-01 00:00:00+00:00: scheduled__2021-08-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:33.523550+00:00. externally triggered: False> successful
2024-02-29 05:56:10,513 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-08-01 00:00:00+00:00, run_id=scheduled__2021-08-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:33.541519+00:00, run_end_date=2024-02-29 05:56:10.513146+00:00, run_duration=1536.971627, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-08-01 00:00:00+00:00, data_interval_end=2021-09-01 00:00:00+00:00, dag_hash=6b8b17fe2fc42da85c4ea45b23a43bdf
2024-02-29 05:56:10,520 INFO - Setting next_dagrun for run_pipeline_new to 2021-09-01 00:00:00+00:00, run_after=2021-10-01 00:00:00+00:00
2024-02-29 05:56:11,665 INFO - Setting next_dagrun for run_pipeline_new to 2021-10-01 00:00:00+00:00, run_after=2021-11-01 00:00:00+00:00
2024-02-29 05:56:12,120 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-08-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:56:12,133 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-08-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:55:55.407195+00:00, run_end_date=2024-02-29 05:56:10.232890+00:00, run_duration=14.825695, state=success, executor_state=success, try_number=1, max_tries=2, job_id=77, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:55:32.293253+00:00, queued_by_job_id=1, pid=1318659
2024-02-29 05:56:13,016 INFO - Setting next_dagrun for run_pipeline_new to 2021-11-01 00:00:00+00:00, run_after=2021-12-01 00:00:00+00:00
2024-02-29 05:56:14,256 INFO - Setting next_dagrun for run_pipeline_new to 2021-12-01 00:00:00+00:00, run_after=2022-01-01 00:00:00+00:00
2024-02-29 05:56:15,510 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 05:56:17,370 INFO - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-06-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:56:19,380 INFO - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-12-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:56:20,693 INFO - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-09-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:56:21,828 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-12-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:56:21,829 INFO - DAG run_pipeline_new has 4/16 running and queued tasks
2024-02-29 05:56:21,829 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-12-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:56:21,832 WARNING - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved
2024-02-29 05:56:21,833 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-12-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-02-29 05:56:21,833 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-12-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:56:21,837 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-12-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:56:21,838 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-12-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:56:21,844 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-12-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:56:19.672865+00:00, run_end_date=2024-02-29 05:56:20.578257+00:00, run_duration=0.905392, state=success, executor_state=success, try_number=1, max_tries=2, job_id=81, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:56:01.787122+00:00, queued_by_job_id=1, pid=1319705
2024-02-29 05:56:21,957 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:56:25,924 INFO - Marking run <DagRun run_pipeline_new @ 2021-06-01 00:00:00+00:00: scheduled__2021-06-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:33.085533+00:00. externally triggered: False> successful
2024-02-29 05:56:25,929 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-06-01 00:00:00+00:00, run_id=scheduled__2021-06-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:33.112672+00:00, run_end_date=2024-02-29 05:56:25.929548+00:00, run_duration=1552.816876, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-06-01 00:00:00+00:00, data_interval_end=2021-07-01 00:00:00+00:00, dag_hash=6b8b17fe2fc42da85c4ea45b23a43bdf
2024-02-29 05:56:25,939 INFO - Setting next_dagrun for run_pipeline_new to 2021-07-01 00:00:00+00:00, run_after=2021-08-01 00:00:00+00:00
2024-02-29 05:56:26,069 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-06-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:56:26,080 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-06-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:56:17.627242+00:00, run_end_date=2024-02-29 05:56:25.178562+00:00, run_duration=7.55132, state=success, executor_state=success, try_number=1, max_tries=2, job_id=80, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:55:58.783620+00:00, queued_by_job_id=1, pid=1319648
2024-02-29 05:56:27,122 INFO - Setting next_dagrun for run_pipeline_new to 2021-08-01 00:00:00+00:00, run_after=2021-09-01 00:00:00+00:00
2024-02-29 05:56:28,267 INFO - Setting next_dagrun for run_pipeline_new to 2021-09-01 00:00:00+00:00, run_after=2021-10-01 00:00:00+00:00
2024-02-29 05:56:28,293 INFO - Marking run <DagRun run_pipeline_new @ 2021-09-01 00:00:00+00:00: scheduled__2021-09-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:33.728007+00:00. externally triggered: False> successful
2024-02-29 05:56:28,294 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-09-01 00:00:00+00:00, run_id=scheduled__2021-09-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:33.766867+00:00, run_end_date=2024-02-29 05:56:28.294204+00:00, run_duration=1554.527337, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-09-01 00:00:00+00:00, data_interval_end=2021-10-01 00:00:00+00:00, dag_hash=6b8b17fe2fc42da85c4ea45b23a43bdf
2024-02-29 05:56:28,299 INFO - Setting next_dagrun for run_pipeline_new to 2021-10-01 00:00:00+00:00, run_after=2021-11-01 00:00:00+00:00
2024-02-29 05:56:28,451 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-09-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:56:28,460 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-09-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:56:20.946149+00:00, run_end_date=2024-02-29 05:56:27.989163+00:00, run_duration=7.043014, state=success, executor_state=success, try_number=1, max_tries=2, job_id=82, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:56:05.467667+00:00, queued_by_job_id=1, pid=1319777
2024-02-29 05:56:29,507 INFO - Setting next_dagrun for run_pipeline_new to 2021-11-01 00:00:00+00:00, run_after=2021-12-01 00:00:00+00:00
2024-02-29 05:56:29,891 INFO - Setting next_dagrun for run_pipeline_new to 2021-12-01 00:00:00+00:00, run_after=2022-01-01 00:00:00+00:00
2024-02-29 05:56:31,050 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 05:56:31,163 INFO - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-12-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:56:38,045 INFO - Marking run <DagRun run_pipeline_new @ 2021-12-01 00:00:00+00:00: scheduled__2021-12-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:34.592505+00:00. externally triggered: False> successful
2024-02-29 05:56:38,046 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-12-01 00:00:00+00:00, run_id=scheduled__2021-12-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:34.631628+00:00, run_end_date=2024-02-29 05:56:38.046654+00:00, run_duration=1563.415026, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-12-01 00:00:00+00:00, data_interval_end=2022-01-01 00:00:00+00:00, dag_hash=6b8b17fe2fc42da85c4ea45b23a43bdf
2024-02-29 05:56:38,055 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 05:56:38,128 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-12-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:56:38,136 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-12-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:56:31.579672+00:00, run_end_date=2024-02-29 05:56:37.635215+00:00, run_duration=6.055543, state=success, executor_state=success, try_number=1, max_tries=2, job_id=83, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:56:21.830296+00:00, queued_by_job_id=1, pid=1320127
2024-02-29 05:56:39,026 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 05:57:01,924 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:57:02,222 ERROR - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.
2024-02-29 05:57:02,902 INFO - 2 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-10-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-11-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:57:02,902 INFO - DAG run_pipeline_new has 0/16 running and queued tasks
2024-02-29 05:57:02,902 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 05:57:02,903 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-10-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-11-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:57:02,904 WARNING - cannot record scheduled_duration for task remove_file because previous state change time has not been saved
2024-02-29 05:57:02,904 WARNING - cannot record scheduled_duration for task remove_file because previous state change time has not been saved
2024-02-29 05:57:02,905 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-10-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2024-02-29 05:57:02,905 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:57:02,905 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-11-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default
2024-02-29 05:57:02,905 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-11-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:57:02,942 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-11-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:57:02,942 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:57:02,942 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-11-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:57:02,942 INFO - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-10-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:57:02,945 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-10-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:57.206249+00:00, run_end_date=2024-02-29 05:57:02.123028+00:00, run_duration=1564.916779, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=64, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:34.269093+00:00, queued_by_job_id=1, pid=1287188
2024-02-29 05:57:02,946 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-11-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:59.372936+00:00, run_end_date=2024-02-29 05:57:01.825280+00:00, run_duration=1562.452344, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=67, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:34.567650+00:00, queued_by_job_id=1, pid=1287281
2024-02-29 05:57:02,990 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:57:03,014 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:57:14,302 INFO - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-10-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:57:14,498 INFO - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-11-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:57:15,034 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-10-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:57:15,035 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 05:57:15,036 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-10-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:57:15,038 WARNING - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved
2024-02-29 05:57:15,040 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-10-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-02-29 05:57:15,041 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:57:15,044 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:57:15,105 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:57:15,918 INFO - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-11-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:57:15,919 INFO - DAG run_pipeline_new has 1/16 running and queued tasks
2024-02-29 05:57:15,919 INFO - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-11-01T00:00:00+00:00 [scheduled]>
2024-02-29 05:57:15,921 WARNING - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved
2024-02-29 05:57:15,922 INFO - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-11-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default
2024-02-29 05:57:15,922 INFO - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-11-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:57:15,925 INFO - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-11-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py']
2024-02-29 05:57:15,926 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-10-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:57:15,927 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-11-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:57:15,934 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-10-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:57:14.436610+00:00, run_end_date=2024-02-29 05:57:14.951071+00:00, run_duration=0.514461, state=success, executor_state=success, try_number=1, max_tries=2, job_id=84, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:57:02.903471+00:00, queued_by_job_id=1, pid=1321258
2024-02-29 05:57:15,935 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-11-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:57:14.656274+00:00, run_end_date=2024-02-29 05:57:15.296959+00:00, run_duration=0.640685, state=success, executor_state=success, try_number=1, max_tries=2, job_id=85, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:57:02.903471+00:00, queued_by_job_id=1, pid=1321274
2024-02-29 05:57:15,976 INFO - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py
2024-02-29 05:57:21,726 INFO - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-10-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:57:22,619 INFO - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-11-01T00:00:00+00:00 [queued]> on host idowu-pc
2024-02-29 05:57:27,099 INFO - Marking run <DagRun run_pipeline_new @ 2021-10-01 00:00:00+00:00: scheduled__2021-10-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:34.006060+00:00. externally triggered: False> successful
2024-02-29 05:57:27,099 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-10-01 00:00:00+00:00, run_id=scheduled__2021-10-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:34.070658+00:00, run_end_date=2024-02-29 05:57:27.099501+00:00, run_duration=1613.028843, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-10-01 00:00:00+00:00, data_interval_end=2021-11-01 00:00:00+00:00, dag_hash=e3c0a33f700e4557aea886780c40b272
2024-02-29 05:57:27,102 INFO - Setting next_dagrun for run_pipeline_new to 2021-11-01 00:00:00+00:00, run_after=2021-12-01 00:00:00+00:00
2024-02-29 05:57:27,105 INFO - Marking run <DagRun run_pipeline_new @ 2021-11-01 00:00:00+00:00: scheduled__2021-11-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:34.308131+00:00. externally triggered: False> successful
2024-02-29 05:57:27,105 INFO - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-11-01 00:00:00+00:00, run_id=scheduled__2021-11-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:34.340199+00:00, run_end_date=2024-02-29 05:57:27.105902+00:00, run_duration=1612.765703, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-11-01 00:00:00+00:00, data_interval_end=2021-12-01 00:00:00+00:00, dag_hash=e3c0a33f700e4557aea886780c40b272
2024-02-29 05:57:27,109 INFO - Setting next_dagrun for run_pipeline_new to 2021-12-01 00:00:00+00:00, run_after=2022-01-01 00:00:00+00:00
2024-02-29 05:57:27,119 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-10-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:57:27,119 INFO - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-11-01T00:00:00+00:00', try_number=1, map_index=-1)
2024-02-29 05:57:27,123 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-10-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:57:21.892654+00:00, run_end_date=2024-02-29 05:57:26.259131+00:00, run_duration=4.366477, state=success, executor_state=success, try_number=1, max_tries=2, job_id=86, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:57:15.037425+00:00, queued_by_job_id=1, pid=1321772
2024-02-29 05:57:27,123 INFO - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-11-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:57:22.762278+00:00, run_end_date=2024-02-29 05:57:26.693046+00:00, run_duration=3.930768, state=success, executor_state=success, try_number=1, max_tries=2, job_id=87, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:57:15.920262+00:00, queued_by_job_id=1, pid=1321803
2024-02-29 05:57:28,143 INFO - Setting next_dagrun for run_pipeline_new to None, run_after=None
2024-02-29 06:01:39,063 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 06:06:39,104 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 06:11:39,138 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 06:16:39,176 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 06:21:39,212 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 06:26:39,243 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 06:31:39,276 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 06:36:39,320 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 06:41:39,356 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 06:46:39,388 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 06:51:39,419 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 06:56:39,453 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 07:01:39,487 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 07:06:39,519 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 07:11:39,550 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 07:16:39,584 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 07:21:39,618 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 07:26:39,652 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 07:31:39,692 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 07:36:39,725 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 07:41:39,758 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 07:46:39,798 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 07:51:39,832 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 07:56:39,866 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 08:01:39,905 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 08:06:39,941 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 08:11:39,975 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 08:16:40,005 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 08:21:40,038 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 08:26:40,074 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 08:31:40,110 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 08:36:40,143 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 08:41:40,176 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 08:46:40,211 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 08:51:40,246 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 08:56:40,279 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 09:01:40,310 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 09:06:40,343 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 09:11:40,376 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 09:16:40,410 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 09:21:40,443 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 09:26:40,459 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 09:31:40,488 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 09:36:40,520 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 09:41:40,554 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 09:46:40,589 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 09:51:40,620 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 09:56:40,654 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 10:01:40,685 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 10:06:40,716 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 10:11:40,752 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 10:16:40,782 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 10:21:40,815 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 10:26:40,849 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 10:31:40,883 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 10:36:40,920 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 10:41:40,955 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 10:46:40,986 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 10:51:41,017 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 10:56:41,048 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 11:01:41,080 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 11:06:41,113 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 11:11:41,147 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 11:16:41,175 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 11:21:41,205 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 11:26:41,237 INFO - Adopting or resetting orphaned tasks for active dag runs
2024-02-29 11:28:36,136 INFO - Exiting gracefully upon receiving signal 15
2024-02-29 11:28:36,542 INFO - Sending Signals.SIGTERM to group 1188951. PIDs of all processes in the group: []
2024-02-29 11:28:36,543 INFO - Sending the signal Signals.SIGTERM to group 1188951
2024-02-29 11:28:36,543 INFO - Sending the signal Signals.SIGTERM to process 1188951 as process group is missing.
2024-02-29 11:28:36,544 INFO - Shutting down LocalExecutor; waiting for running tasks to finish.  Signal again if you don't want to wait.
2024-02-29 11:28:36,544 ERROR - Exception when executing Executor.end
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 855, in _execute
    self._run_scheduler_loop()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 1010, in _run_scheduler_loop
    time.sleep(min(self._scheduler_idle_sleep_time, next_event or 0))
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 261, in _exit_gracefully
    sys.exit(os.EX_OK)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 876, in _execute
    self.job.executor.end()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 406, in end
    self.impl.end()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 350, in end
    self.queue.put((None, None))
  File "<string>", line 2, in put
  File "/usr/lib/python3.10/multiprocessing/managers.py", line 817, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2024-02-29 11:28:36,563 INFO - Sending Signals.SIGTERM to group 1188951. PIDs of all processes in the group: []
2024-02-29 11:28:36,564 INFO - Sending the signal Signals.SIGTERM to group 1188951
2024-02-29 11:28:36,564 INFO - Sending the signal Signals.SIGTERM to process 1188951 as process group is missing.
2024-02-29 11:28:36,565 INFO - Exited execute loop
2024-02-29 11:28:36,567 ERROR - Exception when running scheduler job
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 936, in _checkout
    raise exc.InvalidatePoolError()
sqlalchemy.exc.InvalidatePoolError: ()

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 970, in _checkout
    with util.safe_reraise():
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 967, in _checkout
    fairy._connection_record.get_connection()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 630, in get_connection
    self.__connect()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5434 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/scheduler_command.py", line 52, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 395, in run_job
    job.complete_execution(session=session)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 240, in complete_execution
    Job._update_in_db(job=self, session=session)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 332, in _update_in_db
    session.merge(job)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3056, in merge
    return self._merge(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3136, in _merge
    merged = self.get(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2853, in get
    return self._get_impl(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2975, in _get_impl
    return db_load_fn(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 970, in _checkout
    with util.safe_reraise():
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 967, in _checkout
    fairy._connection_record.get_connection()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 630, in get_connection
    self.__connect()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5434 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)
