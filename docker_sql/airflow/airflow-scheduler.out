[[34m2024-02-29T04:06:35.963+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-02-29T04:06:35.964+0000[0m] {[34mexecutor_loader.py:[0m115} INFO[0m - Loaded executor: LocalExecutor[0m
[[34m2024-02-29T04:06:35.998+0000[0m] {[34mscheduler_job_runner.py:[0m808} INFO[0m - Starting the scheduler[0m
[[34m2024-02-29T04:06:35.999+0000[0m] {[34mscheduler_job_runner.py:[0m815} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-02-29T04:06:36.118+0000[0m] {[34mmanager.py:[0m169} INFO[0m - Launched DagFileProcessorManager with pid: 1188951[0m
[[34m2024-02-29T04:06:36.119+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-29T04:06:36.123+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:11:36.190+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:16:36.225+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:17:32.826+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2022-02-01 00:00:00+00:00, run_after=2022-03-01 00:00:00+00:00[0m
[[34m2024-02-29T04:17:32.861+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:17:31.854773+00:00: manual__2024-02-29T04:17:31.854773+00:00, state:running, queued_at: 2024-02-29 04:17:31.876787+00:00. externally triggered: True> successful[0m
[[34m2024-02-29T04:17:32.862+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:17:31.854773+00:00, run_id=manual__2024-02-29T04:17:31.854773+00:00, run_start_date=2024-02-29 04:17:32.834738+00:00, run_end_date=2024-02-29 04:17:32.861974+00:00, run_duration=0.027236, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=b0bb47566357eee3e1d41de1b5d0b6c5[0m
[[34m2024-02-29T04:17:32.874+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:17:32.874+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T04:17:32.874+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:17:32.877+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:17:32.877+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:17:32.877+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:17:32.880+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:17:32.902+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2022-03-01 00:00:00+00:00, run_after=2022-04-01 00:00:00+00:00[0m
[[34m2024-02-29T04:17:32.911+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:17:32.940+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:17:32.940+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T04:17:32.941+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:17:32.943+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:17:32.943+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:17:32.944+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:17:32.946+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:17:32.969+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T04:17:32.987+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:17:33.077+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:17:33.078+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T04:17:33.078+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:17:33.081+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:17:33.083+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:17:33.084+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:17:33.087+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:17:33.176+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2022-01-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2022-01-01T00:00:00+00:00 permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new permission to 509
[[34m2024-02-29T04:17:40.788+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2022-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2022-03-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2022-03-01T00:00:00+00:00 permission to 509
[[34m2024-02-29T04:17:40.803+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2022-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2022-02-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2022-02-01T00:00:00+00:00 permission to 509
[[34m2024-02-29T04:17:40.885+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2022-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:17:41.453+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:17:41.462+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:17:41.527+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:17:42.025+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:17:42.026+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:17:42.026+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:17:42.034+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2022-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:17:40.934807+00:00, run_end_date=2024-02-29 04:17:41.334419+00:00, run_duration=0.399612, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=2, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:17:32.875237+00:00, queued_by_job_id=1, pid=1202769[0m
[[34m2024-02-29T04:17:42.034+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2022-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:17:41.036897+00:00, run_end_date=2024-02-29 04:17:41.420939+00:00, run_duration=0.384042, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=4, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:17:32.941868+00:00, queued_by_job_id=1, pid=1202779[0m
[[34m2024-02-29T04:17:42.035+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2022-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:17:40.945304+00:00, run_end_date=2024-02-29 04:17:41.359622+00:00, run_duration=0.414318, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=3, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:17:33.079339+00:00, queued_by_job_id=1, pid=1202771[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:21:36.280+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:23:10.905+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2022-02-01 00:00:00+00:00, run_after=2022-03-01 00:00:00+00:00[0m
[[34m2024-02-29T04:23:10.929+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:23:10.457423+00:00: manual__2024-02-29T04:23:10.457423+00:00, state:running, queued_at: 2024-02-29 04:23:10.475513+00:00. externally triggered: True> successful[0m
[[34m2024-02-29T04:23:10.929+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:23:10.457423+00:00, run_id=manual__2024-02-29T04:23:10.457423+00:00, run_start_date=2024-02-29 04:23:10.911576+00:00, run_end_date=2024-02-29 04:23:10.929477+00:00, run_duration=0.017901, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=63912eae4476e2279fc992c8d23e2126[0m
[[34m2024-02-29T04:23:10.939+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:23:10.939+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T04:23:10.939+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:23:10.941+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:23:10.941+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:23:10.941+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:23:10.943+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:23:10.964+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2022-03-01 00:00:00+00:00, run_after=2022-04-01 00:00:00+00:00[0m
[[34m2024-02-29T04:23:10.977+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:23:11.004+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:23:11.004+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T04:23:11.004+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:23:11.005+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:23:11.006+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:23:11.006+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:23:11.008+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:23:11.030+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T04:23:11.044+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:23:11.139+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:23:11.140+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T04:23:11.140+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2022-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:23:11.150+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:23:11.151+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:23:11.152+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:23:11.156+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2022-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:23:11.267+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:23:18.473+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2022-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:23:18.582+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2022-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:23:18.673+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2022-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:23:19.106+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:23:19.220+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:23:19.327+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:23:19.591+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:23:19.591+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:23:19.591+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2022-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:23:19.594+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2022-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:23:18.710112+00:00, run_end_date=2024-02-29 04:23:19.105751+00:00, run_duration=0.395639, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=6, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:23:10.940193+00:00, queued_by_job_id=1, pid=1209501[0m
[[34m2024-02-29T04:23:19.595+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2022-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:23:18.606241+00:00, run_end_date=2024-02-29 04:23:19.015779+00:00, run_duration=0.409538, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=5, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:23:11.004840+00:00, queued_by_job_id=1, pid=1209491[0m
[[34m2024-02-29T04:23:19.595+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2022-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:23:18.810894+00:00, run_end_date=2024-02-29 04:23:19.204143+00:00, run_duration=0.393249, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=7, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:23:11.141578+00:00, queued_by_job_id=1, pid=1209507[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:26:36.333+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:27:58.601+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00[0m
[[34m2024-02-29T04:27:58.627+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:27:57.979634+00:00: manual__2024-02-29T04:27:57.979634+00:00, state:running, queued_at: 2024-02-29 04:27:58.000441+00:00. externally triggered: True> successful[0m
[[34m2024-02-29T04:27:58.627+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:27:57.979634+00:00, run_id=manual__2024-02-29T04:27:57.979634+00:00, run_start_date=2024-02-29 04:27:58.608535+00:00, run_end_date=2024-02-29 04:27:58.627731+00:00, run_duration=0.019196, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=0700539a944a05390069faa886aa1c81[0m
[[34m2024-02-29T04:27:58.639+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:27:58.640+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T04:27:58.640+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:27:58.641+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:27:58.641+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:27:58.641+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:27:58.644+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:27:58.666+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T04:27:58.676+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:27:58.706+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:27:58.707+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T04:27:58.707+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:27:58.708+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:27:58.708+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:27:58.709+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:27:58.711+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:27:58.732+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T04:27:58.754+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:27:58.832+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:27:58.833+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T04:27:58.833+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:27:58.839+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:27:58.841+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:27:58.843+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:27:58.847+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:27:58.891+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-01-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-01-01T00:00:00+00:00 permission to 509
[[34m2024-02-29T04:28:06.456+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-02-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-02-01T00:00:00+00:00 permission to 509
[[34m2024-02-29T04:28:06.597+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-03-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-03-01T00:00:00+00:00 permission to 509
[[34m2024-02-29T04:28:06.742+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:28:07.183+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:28:07.301+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:28:07.438+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:28:07.725+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:28:07.725+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:28:07.725+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:28:07.729+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:28:06.608244+00:00, run_end_date=2024-02-29 04:28:07.061299+00:00, run_duration=0.453055, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=8, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:27:58.640518+00:00, queued_by_job_id=1, pid=1214349[0m
[[34m2024-02-29T04:28:07.730+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:28:06.739853+00:00, run_end_date=2024-02-29 04:28:07.184610+00:00, run_duration=0.444757, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=9, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:27:58.707723+00:00, queued_by_job_id=1, pid=1214358[0m
[[34m2024-02-29T04:28:07.730+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:28:06.887426+00:00, run_end_date=2024-02-29 04:28:07.328477+00:00, run_duration=0.441051, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=10, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:27:58.834415+00:00, queued_by_job_id=1, pid=1214367[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:31:36.390+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:33:07.127+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:33:07.127+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T04:33:07.127+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:33:07.129+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:33:07.129+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:33:07.131+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:33:07.162+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:33:08.249+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 2 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:33:08.249+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T04:33:08.249+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T04:33:08.249+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:33:08.251+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:33:08.252+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:33:08.252+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:33:08.252+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:33:08.255+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:33:08.255+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:33:08.297+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:33:08.300+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:33:13.546+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:33:14.261+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:33:14.975+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:33:15.075+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:33:15.152+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-02-29T04:33:15.159+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:33:13.708271+00:00, run_end_date=2024-02-29 04:33:14.152092+00:00, run_duration=0.443821, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=11, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:33:07.128304+00:00, queued_by_job_id=1, pid=1219439[0m
[[34m2024-02-29T04:33:15.504+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:33:15.620+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:33:16.208+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-02-29T04:33:16.208+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-02-29T04:33:16.211+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:33:15.090836+00:00, run_end_date=2024-02-29 04:33:15.430831+00:00, run_duration=0.339995, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=12, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:33:08.250163+00:00, queued_by_job_id=1, pid=1219517[0m
[[34m2024-02-29T04:33:16.212+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:33:15.204115+00:00, run_end_date=2024-02-29 04:33:15.533559+00:00, run_duration=0.329444, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=13, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:33:08.250163+00:00, queued_by_job_id=1, pid=1219533[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:36:36.441+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:38:14.655+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:38:14.655+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T04:38:14.656+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:38:14.658+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:38:14.658+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:38:14.661+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:38:14.696+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:38:15.981+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 2 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:38:15.981+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T04:38:15.981+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T04:38:15.981+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:38:15.983+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:38:15.984+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:38:15.984+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:38:15.985+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:38:15.987+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:38:15.987+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:38:16.023+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:38:16.024+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml

:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:38:26.716+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:38:26.753+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:38:26.784+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:38:27.458+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:38:27.502+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:38:27.510+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:38:28.057+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-02-29T04:38:28.057+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-02-29T04:38:28.057+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-02-29T04:38:28.061+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:38:26.919917+00:00, run_end_date=2024-02-29 04:38:27.350712+00:00, run_duration=0.430795, state=failed, executor_state=failed, try_number=3, max_tries=2, job_id=14, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:38:15.982363+00:00, queued_by_job_id=1, pid=1225380[0m
[[34m2024-02-29T04:38:28.061+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:38:26.947618+00:00, run_end_date=2024-02-29 04:38:27.385942+00:00, run_duration=0.438324, state=failed, executor_state=failed, try_number=3, max_tries=2, job_id=16, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:38:15.982363+00:00, queued_by_job_id=1, pid=1225382[0m
[[34m2024-02-29T04:38:28.062+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:38:26.950422+00:00, run_end_date=2024-02-29 04:38:27.390567+00:00, run_duration=0.440145, state=failed, executor_state=failed, try_number=3, max_tries=2, job_id=15, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:38:14.656806+00:00, queued_by_job_id=1, pid=1225383[0m
[[34m2024-02-29T04:38:30.134+0000[0m] {[34mdagrun.py:[0m774} ERROR[0m - Marking run <DagRun run_pipeline_new @ 2021-02-01 00:00:00+00:00: scheduled__2021-02-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 04:27:58.661225+00:00. externally triggered: False> failed[0m
[[34m2024-02-29T04:38:30.135+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-02-01 00:00:00+00:00, run_id=scheduled__2021-02-01T00:00:00+00:00, run_start_date=2024-02-29 04:27:58.674452+00:00, run_end_date=2024-02-29 04:38:30.135226+00:00, run_duration=631.460774, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2021-02-01 00:00:00+00:00, data_interval_end=2021-03-01 00:00:00+00:00, dag_hash=5273ba1f006f49bf7a7b0567010a411e[0m
[[34m2024-02-29T04:38:30.138+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T04:38:30.141+0000[0m] {[34mdagrun.py:[0m774} ERROR[0m - Marking run <DagRun run_pipeline_new @ 2021-01-01 00:00:00+00:00: scheduled__2021-01-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 04:27:58.595259+00:00. externally triggered: False> failed[0m
[[34m2024-02-29T04:38:30.142+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-01-01 00:00:00+00:00, run_id=scheduled__2021-01-01T00:00:00+00:00, run_start_date=2024-02-29 04:27:58.608366+00:00, run_end_date=2024-02-29 04:38:30.142247+00:00, run_duration=631.533881, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2021-01-01 00:00:00+00:00, data_interval_end=2021-02-01 00:00:00+00:00, dag_hash=5273ba1f006f49bf7a7b0567010a411e[0m
[[34m2024-02-29T04:38:30.145+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00[0m
[[34m2024-02-29T04:38:30.147+0000[0m] {[34mdagrun.py:[0m774} ERROR[0m - Marking run <DagRun run_pipeline_new @ 2021-03-01 00:00:00+00:00: scheduled__2021-03-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 04:27:58.728407+00:00. externally triggered: False> failed[0m
[[34m2024-02-29T04:38:30.148+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-03-01 00:00:00+00:00, run_id=scheduled__2021-03-01T00:00:00+00:00, run_start_date=2024-02-29 04:27:58.745161+00:00, run_end_date=2024-02-29 04:38:30.148204+00:00, run_duration=631.403043, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2021-03-01 00:00:00+00:00, data_interval_end=2021-04-01 00:00:00+00:00, dag_hash=5273ba1f006f49bf7a7b0567010a411e[0m
[[34m2024-02-29T04:38:30.151+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:39:26.612+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00[0m
[[34m2024-02-29T04:39:26.632+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:39:25.741704+00:00: manual__2024-02-29T04:39:25.741704+00:00, state:running, queued_at: 2024-02-29 04:39:25.756239+00:00. externally triggered: True> successful[0m
[[34m2024-02-29T04:39:26.632+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:39:25.741704+00:00, run_id=manual__2024-02-29T04:39:25.741704+00:00, run_start_date=2024-02-29 04:39:26.619363+00:00, run_end_date=2024-02-29 04:39:26.632640+00:00, run_duration=0.013277, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=949ca78e8da1ea9cf8e37558275a1c37[0m
[[34m2024-02-29T04:39:26.642+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:39:26.642+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T04:39:26.642+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:39:26.643+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:39:26.644+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:39:26.644+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:39:26.646+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:39:26.667+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T04:39:26.676+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:39:26.705+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:39:26.705+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T04:39:26.706+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:39:26.707+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:39:26.708+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:39:26.708+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:39:26.710+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:39:26.733+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T04:39:26.750+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:39:26.793+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:39:26.795+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T04:39:26.796+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:39:26.800+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:39:26.802+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:39:26.803+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:39:26.808+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:39:26.926+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:39:34.363+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:39:34.518+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:39:34.772+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:39:35.045+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:39:35.175+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:39:35.347+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:39:35.723+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:39:35.723+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:39:35.723+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:39:35.726+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:39:34.524475+00:00, run_end_date=2024-02-29 04:39:34.915523+00:00, run_duration=0.391048, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=17, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:39:26.642953+00:00, queued_by_job_id=1, pid=1226841[0m
[[34m2024-02-29T04:39:35.726+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:39:34.668247+00:00, run_end_date=2024-02-29 04:39:35.065218+00:00, run_duration=0.396971, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=18, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:39:26.706713+00:00, queued_by_job_id=1, pid=1226850[0m
[[34m2024-02-29T04:39:35.727+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:39:34.889754+00:00, run_end_date=2024-02-29 04:39:35.257885+00:00, run_duration=0.368131, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=19, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:39:26.798331+00:00, queued_by_job_id=1, pid=1226864[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:41:36.473+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:41:45.689+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00[0m
[[34m2024-02-29T04:41:45.713+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:41:45.155619+00:00: manual__2024-02-29T04:41:45.155619+00:00, state:running, queued_at: 2024-02-29 04:41:45.174863+00:00. externally triggered: True> successful[0m
[[34m2024-02-29T04:41:45.714+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:41:45.155619+00:00, run_id=manual__2024-02-29T04:41:45.155619+00:00, run_start_date=2024-02-29 04:41:45.697305+00:00, run_end_date=2024-02-29 04:41:45.713944+00:00, run_duration=0.016639, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=dfb27b41414c37c1881b257f4cbcb04c[0m
[[34m2024-02-29T04:41:45.726+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:41:45.727+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T04:41:45.727+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:41:45.728+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:41:45.729+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:41:45.729+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:41:45.731+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:41:45.756+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T04:41:45.766+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:41:45.798+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:41:45.798+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T04:41:45.798+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:41:45.800+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:41:45.801+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:41:45.801+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:41:45.803+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:41:45.828+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T04:41:45.844+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:41:45.925+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:41:45.926+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T04:41:45.926+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:41:45.933+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:41:45.934+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:41:45.934+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:41:45.940+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:41:45.988+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:41:53.510+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:41:53.759+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:41:53.793+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:41:54.173+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:41:54.398+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:41:54.421+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:41:54.422+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:41:54.427+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:41:53.664503+00:00, run_end_date=2024-02-29 04:41:54.070130+00:00, run_duration=0.405627, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=20, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:41:45.727817+00:00, queued_by_job_id=1, pid=1229627[0m
[[34m2024-02-29T04:41:54.427+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:41:53.904062+00:00, run_end_date=2024-02-29 04:41:54.294809+00:00, run_duration=0.390747, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=21, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:41:45.927669+00:00, queued_by_job_id=1, pid=1229643[0m
[[34m2024-02-29T04:41:54.468+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:41:55.478+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:41:55.482+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:41:53.947508+00:00, run_end_date=2024-02-29 04:41:54.340786+00:00, run_duration=0.393278, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=22, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:41:45.799513+00:00, queued_by_job_id=1, pid=1229646[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:45:56.888+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00[0m
[[34m2024-02-29T04:45:56.924+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:45:56.721018+00:00: manual__2024-02-29T04:45:56.721018+00:00, state:running, queued_at: 2024-02-29 04:45:56.735353+00:00. externally triggered: True> successful[0m
[[34m2024-02-29T04:45:56.924+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:45:56.721018+00:00, run_id=manual__2024-02-29T04:45:56.721018+00:00, run_start_date=2024-02-29 04:45:56.899286+00:00, run_end_date=2024-02-29 04:45:56.924791+00:00, run_duration=0.025505, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=fb6a14e7ad8c0f278d846e8b299cfe28[0m
[[34m2024-02-29T04:45:56.943+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:45:56.943+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T04:45:56.943+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:45:56.945+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:45:56.946+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:45:56.947+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:45:56.951+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:45:56.986+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T04:45:57.001+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:45:57.033+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:45:57.034+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T04:45:57.034+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:45:57.037+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:45:57.038+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:45:57.038+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:45:57.041+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:45:57.089+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T04:45:57.091+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:45:57.191+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:45:57.192+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T04:45:57.196+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:45:57.204+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:45:57.206+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:45:57.208+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:45:57.212+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:45:57.274+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:46:04.796+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:46:04.839+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:46:05.044+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:46:05.497+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:46:05.509+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:46:05.639+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:46:06.079+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:46:06.079+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:46:06.079+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:46:06.083+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:46:04.958031+00:00, run_end_date=2024-02-29 04:46:05.383049+00:00, run_duration=0.425018, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=23, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:45:57.035370+00:00, queued_by_job_id=1, pid=1233725[0m
[[34m2024-02-29T04:46:06.083+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:46:04.978724+00:00, run_end_date=2024-02-29 04:46:05.390347+00:00, run_duration=0.411623, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=24, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:45:56.944534+00:00, queued_by_job_id=1, pid=1233727[0m
[[34m2024-02-29T04:46:06.083+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:46:05.181803+00:00, run_end_date=2024-02-29 04:46:05.554559+00:00, run_duration=0.372756, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=25, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:45:57.201165+00:00, queued_by_job_id=1, pid=1233741[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:46:36.521+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:51:05.547+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 2 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:51:05.547+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T04:51:05.547+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T04:51:05.547+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:51:05.549+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:51:05.549+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:51:05.549+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:51:05.549+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:51:05.552+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:51:05.552+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:51:05.582+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:51:05.585+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:51:05.612+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:51:05.613+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T04:51:05.613+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:51:05.615+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:51:05.615+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:51:05.618+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:51:05.656+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:51:12.176+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:51:12.196+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:51:12.252+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:51:12.778+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:51:12.789+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:51:12.846+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:51:12.905+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-02-29T04:51:12.905+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-02-29T04:51:12.905+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-02-29T04:51:12.909+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:51:12.312814+00:00, run_end_date=2024-02-29 04:51:12.661598+00:00, run_duration=0.348784, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=26, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:51:05.548183+00:00, queued_by_job_id=1, pid=1239188[0m
[[34m2024-02-29T04:51:12.909+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:51:12.333020+00:00, run_end_date=2024-02-29 04:51:12.702707+00:00, run_duration=0.369687, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=27, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:51:05.548183+00:00, queued_by_job_id=1, pid=1239191[0m
[[34m2024-02-29T04:51:12.910+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:51:12.378625+00:00, run_end_date=2024-02-29 04:51:12.740694+00:00, run_duration=0.362069, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=28, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:51:05.613820+00:00, queued_by_job_id=1, pid=1239196[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:51:36.576+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:53:12.997+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00[0m
[[34m2024-02-29T04:53:13.022+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:53:12.457965+00:00: manual__2024-02-29T04:53:12.457965+00:00, state:running, queued_at: 2024-02-29 04:53:12.476586+00:00. externally triggered: True> successful[0m
[[34m2024-02-29T04:53:13.023+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:53:12.457965+00:00, run_id=manual__2024-02-29T04:53:12.457965+00:00, run_start_date=2024-02-29 04:53:13.005339+00:00, run_end_date=2024-02-29 04:53:13.023109+00:00, run_duration=0.01777, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=219df785d51861aa772da8da671cfbf2[0m
[[34m2024-02-29T04:53:13.032+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:53:13.033+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T04:53:13.033+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:53:13.034+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:53:13.034+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:53:13.034+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:53:13.038+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:53:13.060+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T04:53:13.072+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:53:13.103+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:53:13.104+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T04:53:13.104+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:53:13.106+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:53:13.107+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:53:13.107+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:53:13.110+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:53:13.133+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T04:53:13.150+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:53:13.215+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:53:13.216+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T04:53:13.217+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:53:13.222+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:53:13.225+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:53:13.227+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:53:13.232+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:53:13.307+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:53:20.991+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:53:21.028+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:53:21.224+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:53:21.639+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:53:21.639+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:53:21.807+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:53:22.193+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:53:22.193+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:53:22.194+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:53:22.198+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:53:21.138752+00:00, run_end_date=2024-02-29 04:53:21.523813+00:00, run_duration=0.385061, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=29, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:53:13.105495+00:00, queued_by_job_id=1, pid=1241838[0m
[[34m2024-02-29T04:53:22.198+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:53:21.178747+00:00, run_end_date=2024-02-29 04:53:21.545601+00:00, run_duration=0.366854, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=30, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:53:13.033686+00:00, queued_by_job_id=1, pid=1241841[0m
[[34m2024-02-29T04:53:22.199+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:53:21.357353+00:00, run_end_date=2024-02-29 04:53:21.739670+00:00, run_duration=0.382317, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=31, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:53:13.219901+00:00, queued_by_job_id=1, pid=1241854[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:54:54.158+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00[0m
[[34m2024-02-29T04:54:54.181+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:54:53.464879+00:00: manual__2024-02-29T04:54:53.464879+00:00, state:running, queued_at: 2024-02-29 04:54:53.482051+00:00. externally triggered: True> successful[0m
[[34m2024-02-29T04:54:54.182+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:54:53.464879+00:00, run_id=manual__2024-02-29T04:54:53.464879+00:00, run_start_date=2024-02-29 04:54:54.165182+00:00, run_end_date=2024-02-29 04:54:54.182246+00:00, run_duration=0.017064, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=a9758587c093af9a140a345eaceaec7b[0m
[[34m2024-02-29T04:54:54.193+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:54:54.193+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T04:54:54.193+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:54:54.194+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:54:54.194+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:54:54.195+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:54:54.197+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:54:54.218+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T04:54:54.230+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:54:54.262+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:54:54.263+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T04:54:54.263+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:54:54.265+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:54:54.266+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:54:54.266+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:54:54.268+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:54:54.291+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T04:54:54.307+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:54:54.350+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:54:54.351+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T04:54:54.351+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:54:54.357+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:54:54.357+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:54:54.357+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:54:54.360+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:54:54.446+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:55:02.228+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:55:02.289+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:55:02.290+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:55:02.893+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:55:02.908+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:55:02.963+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:55:03.409+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:55:03.410+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:55:03.410+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:55:03.413+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:55:02.371658+00:00, run_end_date=2024-02-29 04:55:02.779009+00:00, run_duration=0.407351, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=32, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:54:54.193810+00:00, queued_by_job_id=1, pid=1244250[0m
[[34m2024-02-29T04:55:03.413+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:55:02.437325+00:00, run_end_date=2024-02-29 04:55:02.826409+00:00, run_duration=0.389084, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=33, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:54:54.354199+00:00, queued_by_job_id=1, pid=1244253[0m
[[34m2024-02-29T04:55:03.414+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:55:02.442798+00:00, run_end_date=2024-02-29 04:55:02.842106+00:00, run_duration=0.399308, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=34, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:54:54.264228+00:00, queued_by_job_id=1, pid=1244254[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:56:36.610+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:57:05.553+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00[0m
[[34m2024-02-29T04:57:05.583+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2024-02-29 04:57:04.930203+00:00: manual__2024-02-29T04:57:04.930203+00:00, state:running, queued_at: 2024-02-29 04:57:04.949478+00:00. externally triggered: True> successful[0m
[[34m2024-02-29T04:57:05.584+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 04:57:04.930203+00:00, run_id=manual__2024-02-29T04:57:04.930203+00:00, run_start_date=2024-02-29 04:57:05.562897+00:00, run_end_date=2024-02-29 04:57:05.584314+00:00, run_duration=0.021417, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=cac94c935a47a1195a453bdf831e8376[0m
[[34m2024-02-29T04:57:05.599+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:57:05.599+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T04:57:05.599+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:57:05.601+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:57:05.602+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:57:05.602+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:57:05.605+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:57:05.635+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T04:57:05.654+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:57:05.700+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:57:05.701+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T04:57:05.701+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:57:05.703+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:57:05.703+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:57:05.703+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:57:05.706+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:57:05.734+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T04:57:05.758+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T04:57:05.813+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:57:05.814+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T04:57:05.814+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T04:57:05.816+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T04:57:05.817+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T04:57:05.817+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:57:05.820+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T04:57:05.927+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T04:57:14.797+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:57:14.808+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:57:14.981+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T04:57:15.434+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:57:15.471+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:57:15.618+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T04:57:15.765+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:57:15.765+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:57:15.765+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T04:57:15.769+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:57:14.971406+00:00, run_end_date=2024-02-29 04:57:15.342611+00:00, run_duration=0.371205, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=36, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:57:05.600389+00:00, queued_by_job_id=1, pid=1247341[0m
[[34m2024-02-29T04:57:15.769+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:57:14.958132+00:00, run_end_date=2024-02-29 04:57:15.366373+00:00, run_duration=0.408241, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=35, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:57:05.702017+00:00, queued_by_job_id=1, pid=1247339[0m
[[34m2024-02-29T04:57:15.769+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 04:57:15.118832+00:00, run_end_date=2024-02-29 04:57:15.502874+00:00, run_duration=0.384042, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=37, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 04:57:05.815440+00:00, queued_by_job_id=1, pid=1247354[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:00:01.270+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00[0m
[[34m2024-02-29T05:00:01.299+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2024-02-29 05:00:01.200082+00:00: manual__2024-02-29T05:00:01.200082+00:00, state:running, queued_at: 2024-02-29 05:00:01.220164+00:00. externally triggered: True> successful[0m
[[34m2024-02-29T05:00:01.300+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 05:00:01.200082+00:00, run_id=manual__2024-02-29T05:00:01.200082+00:00, run_start_date=2024-02-29 05:00:01.278171+00:00, run_end_date=2024-02-29 05:00:01.300352+00:00, run_duration=0.022181, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=5a5b3eddbb84d33a7c7b34b2eac70ec4[0m
[[34m2024-02-29T05:00:01.313+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:00:01.313+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T05:00:01.313+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:00:01.315+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:00:01.316+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:00:01.316+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:00:01.318+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:00:01.350+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T05:00:01.371+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:00:01.428+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:00:01.429+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T05:00:01.430+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:00:01.437+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:00:01.438+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:00:01.439+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:00:01.443+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:00:01.478+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T05:00:01.515+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:00:01.606+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:00:01.606+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T05:00:01.607+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:00:01.611+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:00:01.611+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:00:01.611+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:00:01.614+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:00:01.669+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:00:10.913+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:00:10.983+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:00:11.098+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:00:11.605+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:00:11.675+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:00:11.792+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:00:12.343+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:00:12.343+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:00:12.343+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:00:12.347+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:00:11.111151+00:00, run_end_date=2024-02-29 05:00:11.541316+00:00, run_duration=0.430165, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=39, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:00:01.314412+00:00, queued_by_job_id=1, pid=1250394[0m
[[34m2024-02-29T05:00:12.347+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:00:11.046752+00:00, run_end_date=2024-02-29 05:00:11.469216+00:00, run_duration=0.422464, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=38, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:00:01.434902+00:00, queued_by_job_id=1, pid=1250387[0m
[[34m2024-02-29T05:00:12.347+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:00:11.250391+00:00, run_end_date=2024-02-29 05:00:11.676651+00:00, run_duration=0.42626, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=40, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:00:01.607971+00:00, queued_by_job_id=1, pid=1250403[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:01:36.674+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:02:30.233+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00[0m
[[34m2024-02-29T05:02:30.284+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2024-02-29 05:02:30.054785+00:00: manual__2024-02-29T05:02:30.054785+00:00, state:running, queued_at: 2024-02-29 05:02:30.075138+00:00. externally triggered: True> successful[0m
[[34m2024-02-29T05:02:30.285+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 05:02:30.054785+00:00, run_id=manual__2024-02-29T05:02:30.054785+00:00, run_start_date=2024-02-29 05:02:30.252732+00:00, run_end_date=2024-02-29 05:02:30.285491+00:00, run_duration=0.032759, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=6735f483cef949895fd4d7ee81492e55[0m
[[34m2024-02-29T05:02:30.307+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:02:30.308+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T05:02:30.308+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:02:30.311+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:02:30.312+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:02:30.312+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:02:30.316+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:02:30.352+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T05:02:30.382+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:02:30.440+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:02:30.441+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T05:02:30.442+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:02:30.446+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:02:30.448+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:02:30.449+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:02:30.452+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:02:30.486+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T05:02:30.505+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:02:30.593+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:02:30.594+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T05:02:30.594+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:02:30.601+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:02:30.602+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:02:30.602+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:02:30.607+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:02:30.676+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:02:39.001+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:02:39.033+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:02:39.126+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:02:39.664+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:02:39.692+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:02:39.724+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:02:40.045+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:02:40.045+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:02:40.046+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:02:40.049+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:02:39.140823+00:00, run_end_date=2024-02-29 05:02:39.536895+00:00, run_duration=0.396072, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=41, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:02:30.309401+00:00, queued_by_job_id=1, pid=1253263[0m
[[34m2024-02-29T05:02:40.049+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:02:39.172001+00:00, run_end_date=2024-02-29 05:02:39.561834+00:00, run_duration=0.389833, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=42, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:02:30.444206+00:00, queued_by_job_id=1, pid=1253266[0m
[[34m2024-02-29T05:02:40.049+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:02:39.251833+00:00, run_end_date=2024-02-29 05:02:39.622877+00:00, run_duration=0.371044, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=43, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:02:30.598401+00:00, queued_by_job_id=1, pid=1253274[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:05:26.227+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00[0m
[[34m2024-02-29T05:05:26.248+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2024-02-29 05:05:25.561301+00:00: manual__2024-02-29T05:05:25.561301+00:00, state:running, queued_at: 2024-02-29 05:05:25.577236+00:00. externally triggered: True> successful[0m
[[34m2024-02-29T05:05:26.249+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 05:05:25.561301+00:00, run_id=manual__2024-02-29T05:05:25.561301+00:00, run_start_date=2024-02-29 05:05:26.234234+00:00, run_end_date=2024-02-29 05:05:26.249314+00:00, run_duration=0.01508, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=b272ad5d99ee7edad64d8cd05483073b[0m
[[34m2024-02-29T05:05:26.262+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:05:26.262+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T05:05:26.263+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:05:26.264+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:05:26.265+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:05:26.265+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:05:26.267+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:05:26.294+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T05:05:26.316+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:05:26.345+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:05:26.346+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T05:05:26.346+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:05:26.347+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:05:26.348+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:05:26.348+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:05:26.350+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:05:26.371+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T05:05:26.386+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:05:26.467+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:05:26.468+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T05:05:26.476+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:05:26.483+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:05:26.485+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:05:26.493+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:05:26.498+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:05:26.605+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:05:34.499+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:05:34.889+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:05:34.918+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:05:44.374+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:05:44.795+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:05:44.802+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:05:34.675941+00:00, run_end_date=2024-02-29 05:05:44.233088+00:00, run_duration=9.557147, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=44, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:05:26.263554+00:00, queued_by_job_id=1, pid=1257050[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:06:36.786+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:07:52.777+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:07:52.777+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T05:07:52.778+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:07:52.787+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task remove_file because previous state change time has not been saved[0m
[[34m2024-02-29T05:07:52.787+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-29T05:07:52.788+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:07:52.793+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:07:52.847+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:07:52.877+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:07:52.953+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:07:52.964+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:05:35.024993+00:00, run_end_date=2024-02-29 05:07:52.593537+00:00, run_duration=137.568544, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=45, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:05:26.346763+00:00, queued_by_job_id=1, pid=1257074[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-02-01T00:00:00+00:00/task_id=remove_file permission to 509
[[34m2024-02-29T05:08:02.512+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:08:03.757+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:08:04.099+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:08:04.099+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T05:08:04.099+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:08:04.101+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved[0m
[[34m2024-02-29T05:08:04.101+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-29T05:08:04.101+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:08:04.104+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:08:04.104+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:08:04.104+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:08:04.114+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:08:02.645462+00:00, run_end_date=2024-02-29 05:08:03.127516+00:00, run_duration=0.482054, state=success, executor_state=success, try_number=1, max_tries=2, job_id=47, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:07:52.783067+00:00, queued_by_job_id=1, pid=1260213[0m
[[34m2024-02-29T05:08:04.114+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:05:35.069284+00:00, run_end_date=2024-02-29 05:08:03.642126+00:00, run_duration=148.572842, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=46, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:05:26.479022+00:00, queued_by_job_id=1, pid=1257080[0m
[[34m2024-02-29T05:08:04.136+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-02-01T00:00:00+00:00/task_id=run_dbt_command permission to 509
[[34m2024-02-29T05:08:08.898+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[0m05:08:13  Running with dbt=1.7.8
[0m05:08:13  Error importing adapter: No module named 'dbt.adapters.postgres'
[0m05:08:13  Encountered an error:
Runtime Error
  Credentials in profile "ny_taxi_test", target "production" invalid: Runtime Error
    Could not find adapter type postgres!
[[34m2024-02-29T05:08:14.956+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2021-02-01 00:00:00+00:00: scheduled__2021-02-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:05:26.285240+00:00. externally triggered: False> successful[0m
[[34m2024-02-29T05:08:14.956+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-02-01 00:00:00+00:00, run_id=scheduled__2021-02-01T00:00:00+00:00, run_start_date=2024-02-29 05:05:26.303253+00:00, run_end_date=2024-02-29 05:08:14.956802+00:00, run_duration=168.653549, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-02-01 00:00:00+00:00, data_interval_end=2021-03-01 00:00:00+00:00, dag_hash=7045eba0332add008305f32237c9dd7a[0m
[[34m2024-02-29T05:08:14.960+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T05:08:14.984+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:08:14.987+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:08:09.011884+00:00, run_end_date=2024-02-29 05:08:14.610615+00:00, run_duration=5.598731, state=success, executor_state=success, try_number=1, max_tries=2, job_id=48, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:08:04.100215+00:00, queued_by_job_id=1, pid=1260507[0m
[[34m2024-02-29T05:08:16.009+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:10:44.757+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:10:44.757+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T05:10:44.758+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:10:44.760+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:10:44.760+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:10:44.762+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:10:44.792+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:10:49.108+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:11:36.863+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:12:14.884+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:12:15.843+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:12:15.844+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T05:12:15.844+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:12:15.845+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task remove_file because previous state change time has not been saved[0m
[[34m2024-02-29T05:12:15.846+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-29T05:12:15.846+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:12:15.851+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-02-29T05:12:15.851+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:12:15.855+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:10:49.207086+00:00, run_end_date=2024-02-29 05:12:14.798061+00:00, run_duration=85.590975, state=success, executor_state=failed, try_number=2, max_tries=2, job_id=49, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:10:44.758858+00:00, queued_by_job_id=1, pid=1262806[0m
[[34m2024-02-29T05:12:15.885+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-01-01T00:00:00+00:00/task_id=remove_file permission to 509
[[34m2024-02-29T05:12:20.668+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:12:21.222+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:12:21.222+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T05:12:21.223+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:12:21.224+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved[0m
[[34m2024-02-29T05:12:21.225+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-29T05:12:21.225+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:12:21.227+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:12:21.268+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:12:21.285+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:12:21.291+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:12:20.779733+00:00, run_end_date=2024-02-29 05:12:21.148578+00:00, run_duration=0.368845, state=success, executor_state=success, try_number=1, max_tries=2, job_id=50, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:12:15.844623+00:00, queued_by_job_id=1, pid=1264876[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-01-01T00:00:00+00:00/task_id=run_dbt_command permission to 509
[[34m2024-02-29T05:12:26.333+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[0m05:12:29  Running with dbt=1.7.8
[0m05:12:29  Error importing adapter: No module named 'dbt.adapters.postgres'
[0m05:12:29  Encountered an error:
Runtime Error
  Credentials in profile "ny_taxi_test", target "production" invalid: Runtime Error
    Could not find adapter type postgres!
[[34m2024-02-29T05:12:30.853+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2021-01-01 00:00:00+00:00: scheduled__2021-01-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:05:26.222966+00:00. externally triggered: False> successful[0m
[[34m2024-02-29T05:12:30.853+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-01-01 00:00:00+00:00, run_id=scheduled__2021-01-01T00:00:00+00:00, run_start_date=2024-02-29 05:05:26.234063+00:00, run_end_date=2024-02-29 05:12:30.853942+00:00, run_duration=424.619879, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-01-01 00:00:00+00:00, data_interval_end=2021-02-01 00:00:00+00:00, dag_hash=7045eba0332add008305f32237c9dd7a[0m
[[34m2024-02-29T05:12:30.857+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00[0m
[[34m2024-02-29T05:12:30.874+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:12:30.877+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:12:26.460913+00:00, run_end_date=2024-02-29 05:12:30.610446+00:00, run_duration=4.149533, state=success, executor_state=success, try_number=1, max_tries=2, job_id=51, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:12:21.223556+00:00, queued_by_job_id=1, pid=1265108[0m
[[34m2024-02-29T05:12:31.691+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T05:12:32.743+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:13:04.095+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:13:04.096+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T05:13:04.096+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:13:04.097+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:13:04.097+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:13:04.099+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:13:04.128+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:13:09.134+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:14:07.000+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:14:07.152+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-02-29T05:14:07.155+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:13:09.247710+00:00, run_end_date=2024-02-29 05:14:06.856940+00:00, run_duration=57.60923, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=52, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:13:04.096749+00:00, queued_by_job_id=1, pid=1266669[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:16:36.901+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:21:36.936+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-29T05:21:50.930+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-02-01 00:00:00+00:00, run_after=2021-03-01 00:00:00+00:00[0m
[[34m2024-02-29T05:21:50.956+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2024-02-29 05:21:50.183886+00:00: manual__2024-02-29T05:21:50.183886+00:00, state:running, queued_at: 2024-02-29 05:21:50.196666+00:00. externally triggered: True> successful[0m
[[34m2024-02-29T05:21:50.956+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 05:21:50.183886+00:00, run_id=manual__2024-02-29T05:21:50.183886+00:00, run_start_date=2024-02-29 05:21:50.939352+00:00, run_end_date=2024-02-29 05:21:50.956890+00:00, run_duration=0.017538, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=b24eaff9a4d995eab8b07fde9da3d752[0m
[[34m2024-02-29T05:21:50.968+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:21:50.969+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T05:21:50.969+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:21:50.970+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:21:50.970+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:21:50.971+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:21:50.973+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-01-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:21:50.998+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T05:21:51.009+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:21:51.044+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:21:51.044+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T05:21:51.044+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:21:51.046+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:21:51.047+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:21:51.047+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:21:51.050+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:21:51.076+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T05:21:51.093+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:21:51.211+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:21:51.211+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T05:21:51.212+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:21:51.214+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:21:51.214+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:21:51.214+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:21:51.217+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:21:51.256+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:21:58.944+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-01-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:21:58.945+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:21:59.018+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:22:26.096+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:22:26.900+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:22:26.901+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T05:22:26.901+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:22:26.903+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task remove_file because previous state change time has not been saved[0m
[[34m2024-02-29T05:22:26.904+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-29T05:22:26.904+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:22:26.906+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:22:26.906+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:22:26.912+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:21:59.185348+00:00, run_end_date=2024-02-29 05:22:25.969520+00:00, run_duration=26.784172, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=55, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:21:51.045151+00:00, queued_by_job_id=1, pid=1277499[0m
[[34m2024-02-29T05:22:26.957+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:22:34.112+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:22:34.944+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:22:34.945+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T05:22:34.945+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-02-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:22:34.949+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved[0m
[[34m2024-02-29T05:22:34.950+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-29T05:22:34.950+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:22:34.953+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-02-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:22:34.955+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:22:34.960+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:22:34.323870+00:00, run_end_date=2024-02-29 05:22:34.786195+00:00, run_duration=0.462325, state=success, executor_state=success, try_number=1, max_tries=2, job_id=56, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:22:26.902039+00:00, queued_by_job_id=1, pid=1278339[0m
[[34m2024-02-29T05:22:35.009+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:22:46.378+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-02-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[0m05:22:55  Running with dbt=1.7.8
[0m05:22:55  Error importing adapter: No module named 'dbt.adapters.postgres'
[0m05:22:55  Encountered an error:
Runtime Error
  Credentials in profile "ny_taxi_test", target "production" invalid: Runtime Error
    Could not find adapter type postgres!
[[34m2024-02-29T05:22:56.616+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2021-02-01 00:00:00+00:00: scheduled__2021-02-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:21:50.991765+00:00. externally triggered: False> successful[0m
[[34m2024-02-29T05:22:56.616+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-02-01 00:00:00+00:00, run_id=scheduled__2021-02-01T00:00:00+00:00, run_start_date=2024-02-29 05:21:51.007629+00:00, run_end_date=2024-02-29 05:22:56.616602+00:00, run_duration=65.608973, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-02-01 00:00:00+00:00, data_interval_end=2021-03-01 00:00:00+00:00, dag_hash=a801ea1cd406b22c7deb8998d8cbf99a[0m
[[34m2024-02-29T05:22:56.620+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-03-01 00:00:00+00:00, run_after=2021-04-01 00:00:00+00:00[0m
[[34m2024-02-29T05:22:56.647+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-02-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:22:56.651+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-02-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:22:46.589528+00:00, run_end_date=2024-02-29 05:22:56.210204+00:00, run_duration=9.620676, state=success, executor_state=success, try_number=1, max_tries=2, job_id=57, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:22:34.946982+00:00, queued_by_job_id=1, pid=1278756[0m
[[34m2024-02-29T05:22:57.671+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T05:23:03.173+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:23:03.492+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-01-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:23:03.496+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-01-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:21:59.107064+00:00, run_end_date=2024-02-29 05:23:03.059986+00:00, run_duration=63.952922, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=53, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:21:50.969595+00:00, queued_by_job_id=1, pid=1277490[0m
[[34m2024-02-29T05:23:16.127+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:23:16.914+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:23:16.917+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:21:59.116045+00:00, run_end_date=2024-02-29 05:23:15.965113+00:00, run_duration=76.849068, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=54, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:21:51.212795+00:00, queued_by_job_id=1, pid=1277492[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:26:36.986+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:30:32.690+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-04-01 00:00:00+00:00, run_after=2021-05-01 00:00:00+00:00[0m
[[34m2024-02-29T05:30:32.722+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2024-02-29 05:30:32.601434+00:00: manual__2024-02-29T05:30:32.601434+00:00, state:running, queued_at: 2024-02-29 05:30:32.616988+00:00. externally triggered: True> successful[0m
[[34m2024-02-29T05:30:32.723+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2024-02-29 05:30:32.601434+00:00, run_id=manual__2024-02-29T05:30:32.601434+00:00, run_start_date=2024-02-29 05:30:32.697416+00:00, run_end_date=2024-02-29 05:30:32.722913+00:00, run_duration=0.025497, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-01-01 00:00:00+00:00, data_interval_end=2024-02-01 00:00:00+00:00, dag_hash=d0658b84929d53135d275ccf395a640f[0m
[[34m2024-02-29T05:30:32.741+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:32.742+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T05:30:32.743+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:32.746+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:30:32.748+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:30:32.749+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:32.752+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:32.800+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-05-01 00:00:00+00:00, run_after=2021-06-01 00:00:00+00:00[0m
[[34m2024-02-29T05:30:32.805+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:30:32.881+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-04-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:32.882+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T05:30:32.882+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-04-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:32.888+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:30:32.890+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-04-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:30:32.891+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:32.896+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:32.926+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-06-01 00:00:00+00:00, run_after=2021-07-01 00:00:00+00:00[0m
[[34m2024-02-29T05:30:33.004+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:30:33.055+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-05-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:33.056+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 2/16 running and queued tasks[0m
[[34m2024-02-29T05:30:33.056+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-05-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:33.058+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:30:33.059+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-05-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:30:33.059+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:33.062+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:33.093+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-07-01 00:00:00+00:00, run_after=2021-08-01 00:00:00+00:00[0m
[[34m2024-02-29T05:30:33.114+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:30:33.282+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-06-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:33.283+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 3/16 running and queued tasks[0m
[[34m2024-02-29T05:30:33.284+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-06-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:33.288+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:30:33.289+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-06-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:30:33.289+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:33.293+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:33.340+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-08-01 00:00:00+00:00, run_after=2021-09-01 00:00:00+00:00[0m
[[34m2024-02-29T05:30:33.429+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:30:33.485+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-07-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:33.486+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 4/16 running and queued tasks[0m
[[34m2024-02-29T05:30:33.487+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-07-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:33.489+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:30:33.490+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-07-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:30:33.490+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:33.493+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:33.532+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-09-01 00:00:00+00:00, run_after=2021-10-01 00:00:00+00:00[0m
[[34m2024-02-29T05:30:33.598+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:30:33.698+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-08-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:33.699+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 5/16 running and queued tasks[0m
[[34m2024-02-29T05:30:33.700+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-08-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:33.702+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:30:33.703+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-08-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:30:33.704+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:33.706+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:33.741+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-10-01 00:00:00+00:00, run_after=2021-11-01 00:00:00+00:00[0m
[[34m2024-02-29T05:30:33.870+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:30:33.963+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-09-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:33.964+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 6/16 running and queued tasks[0m
[[34m2024-02-29T05:30:33.965+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-09-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:33.968+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:30:33.970+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-09-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:30:33.971+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:33.974+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:34.033+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-11-01 00:00:00+00:00, run_after=2021-12-01 00:00:00+00:00[0m
[[34m2024-02-29T05:30:34.097+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:30:34.264+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-10-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:34.265+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 7/16 running and queued tasks[0m
[[34m2024-02-29T05:30:34.266+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-10-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:34.270+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:30:34.271+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-10-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:30:34.272+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:34.276+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:34.322+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-12-01 00:00:00+00:00, run_after=2022-01-01 00:00:00+00:00[0m
[[34m2024-02-29T05:30:34.407+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:30:34.565+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-11-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:34.566+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 8/16 running and queued tasks[0m
[[34m2024-02-29T05:30:34.566+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-11-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:34.569+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:30:34.569+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-11-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:30:34.570+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-11-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:34.573+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-11-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:34.603+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T05:30:34.750+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:30:34.946+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-12-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:34.946+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 9/16 running and queued tasks[0m
[[34m2024-02-29T05:30:34.948+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-12-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:30:34.953+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task download_load_data because previous state change time has not been saved[0m
[[34m2024-02-29T05:30:34.954+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-12-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:30:34.954+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-12-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:34.957+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-12-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:30:35.124+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-04-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-04-01T00:00:00+00:00 permission to 509
[[34m2024-02-29T05:30:55.010+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-04-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-07-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-07-01T00:00:00+00:00 permission to 509
[[34m2024-02-29T05:30:55.456+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-07-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:30:56.145+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-05-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-05-01T00:00:00+00:00 permission to 509
[[34m2024-02-29T05:30:56.264+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-05-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-06-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-06-01T00:00:00+00:00 permission to 509
[[34m2024-02-29T05:30:56.401+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-06-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-09-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-09-01T00:00:00+00:00 permission to 509
[[34m2024-02-29T05:30:56.623+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-09-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-10-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-10-01T00:00:00+00:00 permission to 509
[[34m2024-02-29T05:30:56.815+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-10-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-08-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-08-01T00:00:00+00:00 permission to 509
[[34m2024-02-29T05:30:58.057+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-08-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-12-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-12-01T00:00:00+00:00 permission to 509
[[34m2024-02-29T05:30:58.327+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-12-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-11-01T00:00:00+00:00/task_id=download_load_data permission to 509
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-11-01T00:00:00+00:00 permission to 509
[[34m2024-02-29T05:30:59.003+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-11-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:31:37.276+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-02-29T05:32:03.044+0000] {manager.py:523} INFO - DAG run_pipeline_new is missing and will be deactivated.
[2024-02-29T05:32:03.048+0000] {manager.py:535} INFO - Deactivated 1 DAGs which are no longer present in file.
[2024-02-29T05:32:03.054+0000] {manager.py:539} INFO - Deleted DAG run_pipeline_new in serialized_dag table
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-02-29T05:34:03.963+0000] {manager.py:523} INFO - DAG run_pipeline_new is missing and will be deactivated.
[2024-02-29T05:34:03.971+0000] {manager.py:535} INFO - Deactivated 1 DAGs which are no longer present in file.
[2024-02-29T05:34:03.989+0000] {manager.py:539} INFO - Deleted DAG run_pipeline_new in serialized_dag table
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:35:33.894+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:35:34.894+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:35:34.903+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:56.549957+00:00, run_end_date=2024-02-29 05:35:33.508867+00:00, run_duration=276.95891, state=up_for_retry, executor_state=failed, try_number=1, max_tries=2, job_id=60, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:32.745567+00:00, queued_by_job_id=1, pid=1287144[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:36:37.546+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:39:53.413+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:39:53.870+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-04-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:39:53.872+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 8/16 running and queued tasks[0m
[[34m2024-02-29T05:39:53.872+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-04-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:39:53.876+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task remove_file because previous state change time has not been saved[0m
[[34m2024-02-29T05:39:53.876+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-04-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-29T05:39:53.877+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:39:53.976+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-04-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:39:53.975+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:39:53.981+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-04-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:55.664721+00:00, run_end_date=2024-02-29 05:39:53.237899+00:00, run_duration=537.573178, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=58, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:32.884880+00:00, queued_by_job_id=1, pid=1287087[0m
[[34m2024-02-29T05:39:54.051+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-04-01T00:00:00+00:00/task_id=remove_file permission to 509
[[34m2024-02-29T05:40:01.166+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-04-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:40:02.927+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-04-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:40:02.928+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 8/16 running and queued tasks[0m
[[34m2024-02-29T05:40:02.929+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-04-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:40:02.933+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved[0m
[[34m2024-02-29T05:40:02.933+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-04-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-29T05:40:02.934+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:40:02.945+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-04-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:40:02.947+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-04-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:40:02.952+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-04-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:40:01.361452+00:00, run_end_date=2024-02-29 05:40:02.306775+00:00, run_duration=0.945323, state=success, executor_state=success, try_number=1, max_tries=2, job_id=68, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:39:53.873977+00:00, queued_by_job_id=1, pid=1297641[0m
[[34m2024-02-29T05:40:03.004+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-04-01T00:00:00+00:00/task_id=run_dbt_command permission to 509
[[34m2024-02-29T05:40:11.672+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-04-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[0m05:40:22  Running with dbt=1.7.8
[0m05:40:22  Error importing adapter: No module named 'dbt.adapters.postgres'
[0m05:40:22  Encountered an error:
Runtime Error
  Credentials in profile "ny_taxi_test", target "production" invalid: Runtime Error
    Could not find adapter type postgres!
[[34m2024-02-29T05:40:23.581+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2021-04-01 00:00:00+00:00: scheduled__2021-04-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:32.790769+00:00. externally triggered: False> successful[0m
[[34m2024-02-29T05:40:23.582+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-04-01 00:00:00+00:00, run_id=scheduled__2021-04-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:32.814613+00:00, run_end_date=2024-02-29 05:40:23.581944+00:00, run_duration=590.767331, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-04-01 00:00:00+00:00, data_interval_end=2021-05-01 00:00:00+00:00, dag_hash=52e54482f9af61e36762501cc3956179[0m
[[34m2024-02-29T05:40:23.585+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-05-01 00:00:00+00:00, run_after=2021-06-01 00:00:00+00:00[0m
[[34m2024-02-29T05:40:23.764+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-04-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:40:23.770+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-04-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:40:11.914158+00:00, run_end_date=2024-02-29 05:40:23.310921+00:00, run_duration=11.396763, state=success, executor_state=success, try_number=1, max_tries=2, job_id=69, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:40:02.931437+00:00, queued_by_job_id=1, pid=1298001[0m
[[34m2024-02-29T05:40:24.915+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-06-01 00:00:00+00:00, run_after=2021-07-01 00:00:00+00:00[0m
[[34m2024-02-29T05:40:26.201+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-07-01 00:00:00+00:00, run_after=2021-08-01 00:00:00+00:00[0m
[[34m2024-02-29T05:40:27.357+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-08-01 00:00:00+00:00, run_after=2021-09-01 00:00:00+00:00[0m
[[34m2024-02-29T05:40:27.568+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-09-01 00:00:00+00:00, run_after=2021-10-01 00:00:00+00:00[0m
[[34m2024-02-29T05:40:28.779+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-10-01 00:00:00+00:00, run_after=2021-11-01 00:00:00+00:00[0m
[[34m2024-02-29T05:40:29.453+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-11-01 00:00:00+00:00, run_after=2021-12-01 00:00:00+00:00[0m
[[34m2024-02-29T05:40:30.758+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-12-01 00:00:00+00:00, run_after=2022-01-01 00:00:00+00:00[0m
[[34m2024-02-29T05:40:31.915+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T05:40:34.048+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:40:34.048+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 8/16 running and queued tasks[0m
[[34m2024-02-29T05:40:34.048+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:40:34.050+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:40:34.051+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:40:34.079+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:40:34.126+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:40:41.697+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:41:37.838+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-29T05:42:04.718+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:42:05.483+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=2, map_index=-1)[0m
[[34m2024-02-29T05:42:05.511+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:40:41.872994+00:00, run_end_date=2024-02-29 05:42:04.564561+00:00, run_duration=82.691567, state=up_for_retry, executor_state=failed, try_number=2, max_tries=2, job_id=70, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:40:34.049451+00:00, queued_by_job_id=1, pid=1298734[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:46:38.142+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-29T05:47:04.874+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:47:04.875+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 8/16 running and queued tasks[0m
[[34m2024-02-29T05:47:04.875+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:47:04.876+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=3, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-02-29T05:47:04.876+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:47:04.886+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'download_load_data', 'scheduled__2021-03-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:47:04.936+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:47:12.636+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.download_load_data scheduled__2021-03-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:48:33.202+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:48:33.917+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-03-01T00:00:00+00:00', try_number=3, map_index=-1)[0m
[[34m2024-02-29T05:48:33.924+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-03-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:47:12.944133+00:00, run_end_date=2024-02-29 05:48:32.912542+00:00, run_duration=79.968409, state=failed, executor_state=failed, try_number=3, max_tries=2, job_id=71, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:47:04.875654+00:00, queued_by_job_id=1, pid=1306407[0m
[[34m2024-02-29T05:48:36.281+0000[0m] {[34mdagrun.py:[0m774} ERROR[0m - Marking run <DagRun run_pipeline_new @ 2021-03-01 00:00:00+00:00: scheduled__2021-03-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:32.684245+00:00. externally triggered: False> failed[0m
[[34m2024-02-29T05:48:36.282+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-03-01 00:00:00+00:00, run_id=scheduled__2021-03-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:32.697201+00:00, run_end_date=2024-02-29 05:48:36.282093+00:00, run_duration=1083.584892, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2021-03-01 00:00:00+00:00, data_interval_end=2021-04-01 00:00:00+00:00, dag_hash=f8bdcd12abd45b309a61afad014086b7[0m
[[34m2024-02-29T05:48:36.293+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-04-01 00:00:00+00:00, run_after=2021-05-01 00:00:00+00:00[0m
[[34m2024-02-29T05:48:37.444+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-05-01 00:00:00+00:00, run_after=2021-06-01 00:00:00+00:00[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:51:38.881+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:51:56.586+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:51:56.707+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-05-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:51:56.708+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 7/16 running and queued tasks[0m
[[34m2024-02-29T05:51:56.708+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-05-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:51:56.715+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task remove_file because previous state change time has not been saved[0m
[[34m2024-02-29T05:51:56.716+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-05-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-29T05:51:56.716+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:51:56.721+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-05-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:51:56.721+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:51:56.764+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-05-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:56.741027+00:00, run_end_date=2024-02-29 05:51:56.044175+00:00, run_duration=1259.303148, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=61, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:33.057248+00:00, queued_by_job_id=1, pid=1287156[0m
[[34m2024-02-29T05:51:57.151+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-05-01T00:00:00+00:00/task_id=remove_file permission to 509
[[34m2024-02-29T05:52:17.124+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-05-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:52:20.363+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-05-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:52:20.363+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 7/16 running and queued tasks[0m
[[34m2024-02-29T05:52:20.364+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-05-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:52:20.367+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved[0m
[[34m2024-02-29T05:52:20.367+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-05-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-29T05:52:20.368+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:52:20.383+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-05-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:52:20.390+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-05-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:52:20.393+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-05-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:52:17.669410+00:00, run_end_date=2024-02-29 05:52:18.924166+00:00, run_duration=1.254756, state=success, executor_state=success, try_number=1, max_tries=2, job_id=72, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:51:56.709509+00:00, queued_by_job_id=1, pid=1313041[0m
[[34m2024-02-29T05:52:20.487+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-05-01T00:00:00+00:00/task_id=run_dbt_command permission to 509
[[34m2024-02-29T05:52:47.370+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-05-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[0m05:53:00  Running with dbt=1.7.8
[0m05:53:00  Error importing adapter: No module named 'dbt.adapters.postgres'
[0m05:53:00  Encountered an error:
Runtime Error
  Credentials in profile "ny_taxi_test", target "production" invalid: Runtime Error
    Could not find adapter type postgres!
[[34m2024-02-29T05:53:03.672+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2021-05-01 00:00:00+00:00: scheduled__2021-05-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:32.918929+00:00. externally triggered: False> successful[0m
[[34m2024-02-29T05:53:03.676+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-05-01 00:00:00+00:00, run_id=scheduled__2021-05-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:32.947344+00:00, run_end_date=2024-02-29 05:53:03.676313+00:00, run_duration=1350.728969, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-05-01 00:00:00+00:00, data_interval_end=2021-06-01 00:00:00+00:00, dag_hash=61e6570e57d9d5c7d834633272ee188e[0m
[[34m2024-02-29T05:53:03.683+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-06-01 00:00:00+00:00, run_after=2021-07-01 00:00:00+00:00[0m
[[34m2024-02-29T05:53:03.796+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-05-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:53:03.829+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-05-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:52:47.852987+00:00, run_end_date=2024-02-29 05:53:02.511328+00:00, run_duration=14.658341, state=success, executor_state=success, try_number=1, max_tries=2, job_id=73, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:52:20.364958+00:00, queued_by_job_id=1, pid=1313795[0m
[[34m2024-02-29T05:53:04.887+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-07-01 00:00:00+00:00, run_after=2021-08-01 00:00:00+00:00[0m
[[34m2024-02-29T05:53:06.201+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-08-01 00:00:00+00:00, run_after=2021-09-01 00:00:00+00:00[0m
[[34m2024-02-29T05:53:07.441+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-09-01 00:00:00+00:00, run_after=2021-10-01 00:00:00+00:00[0m
[[34m2024-02-29T05:53:08.212+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-10-01 00:00:00+00:00, run_after=2021-11-01 00:00:00+00:00[0m
[[34m2024-02-29T05:53:08.901+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-11-01 00:00:00+00:00, run_after=2021-12-01 00:00:00+00:00[0m
[[34m2024-02-29T05:53:10.167+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-12-01 00:00:00+00:00, run_after=2022-01-01 00:00:00+00:00[0m
[[34m2024-02-29T05:53:11.353+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:53:39.835+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:53:40.799+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-07-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:53:40.800+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 6/16 running and queued tasks[0m
[[34m2024-02-29T05:53:40.800+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-07-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:53:40.803+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task remove_file because previous state change time has not been saved[0m
[[34m2024-02-29T05:53:40.804+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-07-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-29T05:53:40.805+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:53:40.809+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-07-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:53:40.808+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:53:40.818+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-07-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:55.935263+00:00, run_end_date=2024-02-29 05:53:39.308342+00:00, run_duration=1363.373079, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=59, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:33.488077+00:00, queued_by_job_id=1, pid=1287097[0m
[[34m2024-02-29T05:53:40.947+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-07-01T00:00:00+00:00/task_id=remove_file permission to 509
[[34m2024-02-29T05:53:55.467+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-07-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:53:58.852+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-07-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:53:58.852+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 6/16 running and queued tasks[0m
[[34m2024-02-29T05:53:58.853+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-07-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:53:58.856+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved[0m
[[34m2024-02-29T05:53:58.856+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-07-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-29T05:53:58.857+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:53:58.867+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-07-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:53:58.879+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-07-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:53:58.897+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-07-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:53:56.164401+00:00, run_end_date=2024-02-29 05:53:57.703958+00:00, run_duration=1.539557, state=success, executor_state=success, try_number=1, max_tries=2, job_id=74, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:53:40.801295+00:00, queued_by_job_id=1, pid=1315436[0m
[[34m2024-02-29T05:53:58.983+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-07-01T00:00:00+00:00/task_id=run_dbt_command permission to 509
[[34m2024-02-29T05:54:10.182+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-07-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[0m05:54:16  Running with dbt=1.7.8
[0m05:54:16  Error importing adapter: No module named 'dbt.adapters.postgres'
[0m05:54:16  Encountered an error:
Runtime Error
  Credentials in profile "ny_taxi_test", target "production" invalid: Runtime Error
    Could not find adapter type postgres!
[[34m2024-02-29T05:54:19.193+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2021-07-01 00:00:00+00:00: scheduled__2021-07-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:33.328101+00:00. externally triggered: False> successful[0m
[[34m2024-02-29T05:54:19.193+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-07-01 00:00:00+00:00, run_id=scheduled__2021-07-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:33.357537+00:00, run_end_date=2024-02-29 05:54:19.193679+00:00, run_duration=1425.836142, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-07-01 00:00:00+00:00, data_interval_end=2021-08-01 00:00:00+00:00, dag_hash=1166a2566c66f25a8594996b205f12fb[0m
[[34m2024-02-29T05:54:19.199+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-08-01 00:00:00+00:00, run_after=2021-09-01 00:00:00+00:00[0m
[[34m2024-02-29T05:54:19.336+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-07-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:54:19.350+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-07-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:54:10.437814+00:00, run_end_date=2024-02-29 05:54:18.528618+00:00, run_duration=8.090804, state=success, executor_state=success, try_number=1, max_tries=2, job_id=75, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:53:58.854402+00:00, queued_by_job_id=1, pid=1315854[0m
[[34m2024-02-29T05:54:20.380+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-09-01 00:00:00+00:00, run_after=2021-10-01 00:00:00+00:00[0m
[[34m2024-02-29T05:54:21.844+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-10-01 00:00:00+00:00, run_after=2021-11-01 00:00:00+00:00[0m
[[34m2024-02-29T05:54:23.046+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-11-01 00:00:00+00:00, run_after=2021-12-01 00:00:00+00:00[0m
[[34m2024-02-29T05:54:23.933+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-12-01 00:00:00+00:00, run_after=2022-01-01 00:00:00+00:00[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:54:25.178+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:55:19.191+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:55:19.312+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-08-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:55:19.313+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 5/16 running and queued tasks[0m
[[34m2024-02-29T05:55:19.314+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-08-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:55:19.320+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task remove_file because previous state change time has not been saved[0m
[[34m2024-02-29T05:55:19.321+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-08-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-29T05:55:19.322+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:55:19.332+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-08-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:55:19.332+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:55:19.341+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-08-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:58.481280+00:00, run_end_date=2024-02-29 05:55:18.874584+00:00, run_duration=1460.393304, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=65, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:33.701085+00:00, queued_by_job_id=1, pid=1287246[0m
[[34m2024-02-29T05:55:19.432+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-08-01T00:00:00+00:00/task_id=remove_file permission to 509
[[34m2024-02-29T05:55:30.523+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-08-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:55:32.290+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-08-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:55:32.291+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 5/16 running and queued tasks[0m
[[34m2024-02-29T05:55:32.292+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-08-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:55:32.296+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved[0m
[[34m2024-02-29T05:55:32.297+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-08-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-29T05:55:32.297+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:55:32.305+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-08-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:55:32.306+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-08-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:55:32.318+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-08-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:55:30.868890+00:00, run_end_date=2024-02-29 05:55:31.962836+00:00, run_duration=1.093946, state=success, executor_state=success, try_number=1, max_tries=2, job_id=76, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:55:19.316409+00:00, queued_by_job_id=1, pid=1317809[0m
[[34m2024-02-29T05:55:32.432+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:55:32.504+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:55:32.539+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-06-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:55:32.542+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 5/16 running and queued tasks[0m
[[34m2024-02-29T05:55:32.543+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-06-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:55:32.545+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task remove_file because previous state change time has not been saved[0m
[[34m2024-02-29T05:55:32.546+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-06-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-29T05:55:32.546+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:55:32.551+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:55:32.562+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-06-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:55:32.569+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-06-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:56.846789+00:00, run_end_date=2024-02-29 05:55:32.276465+00:00, run_duration=1475.429676, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=62, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:33.285139+00:00, queued_by_job_id=1, pid=1287161[0m
[[34m2024-02-29T05:55:32.737+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:55:41.991+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:55:42.384+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-09-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:55:42.385+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 5/16 running and queued tasks[0m
[[34m2024-02-29T05:55:42.386+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-09-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:55:42.395+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task remove_file because previous state change time has not been saved[0m
[[34m2024-02-29T05:55:42.418+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-09-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-29T05:55:42.423+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:55:42.426+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:55:42.433+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-09-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:55:42.447+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-09-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:57.048215+00:00, run_end_date=2024-02-29 05:55:41.737733+00:00, run_duration=1484.689518, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=63, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:33.967179+00:00, queued_by_job_id=1, pid=1287182[0m
[[34m2024-02-29T05:55:42.551+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-08-01T00:00:00+00:00/task_id=run_dbt_command permission to 509
[[34m2024-02-29T05:55:54.986+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-08-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-06-01T00:00:00+00:00/task_id=remove_file permission to 509
[[34m2024-02-29T05:55:55.422+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-06-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:55:58.779+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-06-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:55:58.780+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 5/16 running and queued tasks[0m
[[34m2024-02-29T05:55:58.781+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-06-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:55:58.792+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved[0m
[[34m2024-02-29T05:55:58.793+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-06-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-29T05:55:58.798+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:55:58.814+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-06-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:55:58.825+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-06-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:55:58.855+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-06-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:55:55.958091+00:00, run_end_date=2024-02-29 05:55:57.244876+00:00, run_duration=1.286785, state=success, executor_state=success, try_number=1, max_tries=2, job_id=78, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:55:32.543925+00:00, queued_by_job_id=1, pid=1318700[0m
[[34m2024-02-29T05:55:58.967+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:56:00.909+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:56:01.784+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-12-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:56:01.785+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 5/16 running and queued tasks[0m
[[34m2024-02-29T05:56:01.785+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-12-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:56:01.788+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task remove_file because previous state change time has not been saved[0m
[[34m2024-02-29T05:56:01.791+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-12-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-29T05:56:01.791+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-12-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:56:01.817+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-12-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:56:01.820+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-12-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:56:01.832+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-12-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:58.610390+00:00, run_end_date=2024-02-29 05:56:00.670777+00:00, run_duration=1502.060387, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=66, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:34.949577+00:00, queued_by_job_id=1, pid=1287251[0m
[[34m2024-02-29T05:56:02.122+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-09-01T00:00:00+00:00/task_id=remove_file permission to 509
[[34m2024-02-29T05:56:02.444+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-09-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:56:05.455+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-09-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:56:05.457+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 5/16 running and queued tasks[0m
[[34m2024-02-29T05:56:05.458+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-09-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:56:05.478+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved[0m
[[34m2024-02-29T05:56:05.480+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-09-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-29T05:56:05.487+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:56:05.490+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-09-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:56:05.703+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:56:06.034+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-09-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:56:06.056+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-09-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:56:03.240873+00:00, run_end_date=2024-02-29 05:56:04.909496+00:00, run_duration=1.668623, state=success, executor_state=success, try_number=1, max_tries=2, job_id=79, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:55:42.388366+00:00, queued_by_job_id=1, pid=1319022[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[0m05:56:08  Running with dbt=1.7.8
[0m05:56:08  Error importing adapter: No module named 'dbt.adapters.postgres'
[0m05:56:08  Encountered an error:
Runtime Error
  Credentials in profile "ny_taxi_test", target "production" invalid: Runtime Error
    Could not find adapter type postgres!
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:56:10.512+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2021-08-01 00:00:00+00:00: scheduled__2021-08-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:33.523550+00:00. externally triggered: False> successful[0m
[[34m2024-02-29T05:56:10.513+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-08-01 00:00:00+00:00, run_id=scheduled__2021-08-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:33.541519+00:00, run_end_date=2024-02-29 05:56:10.513146+00:00, run_duration=1536.971627, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-08-01 00:00:00+00:00, data_interval_end=2021-09-01 00:00:00+00:00, dag_hash=6b8b17fe2fc42da85c4ea45b23a43bdf[0m
[[34m2024-02-29T05:56:10.520+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-09-01 00:00:00+00:00, run_after=2021-10-01 00:00:00+00:00[0m
[[34m2024-02-29T05:56:11.665+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-10-01 00:00:00+00:00, run_after=2021-11-01 00:00:00+00:00[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:56:12.120+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-08-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:56:12.133+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-08-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:55:55.407195+00:00, run_end_date=2024-02-29 05:56:10.232890+00:00, run_duration=14.825695, state=success, executor_state=success, try_number=1, max_tries=2, job_id=77, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:55:32.293253+00:00, queued_by_job_id=1, pid=1318659[0m
[[34m2024-02-29T05:56:13.016+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-11-01 00:00:00+00:00, run_after=2021-12-01 00:00:00+00:00[0m
[[34m2024-02-29T05:56:14.256+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-12-01 00:00:00+00:00, run_after=2022-01-01 00:00:00+00:00[0m
[[34m2024-02-29T05:56:15.510+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-06-01T00:00:00+00:00/task_id=run_dbt_command permission to 509
[[34m2024-02-29T05:56:17.370+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-06-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-12-01T00:00:00+00:00/task_id=remove_file permission to 509
[[34m2024-02-29T05:56:19.380+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-12-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-09-01T00:00:00+00:00/task_id=run_dbt_command permission to 509
[[34m2024-02-29T05:56:20.693+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-09-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:56:21.828+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-12-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:56:21.829+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 4/16 running and queued tasks[0m
[[34m2024-02-29T05:56:21.829+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-12-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:56:21.832+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved[0m
[[34m2024-02-29T05:56:21.833+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-12-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-29T05:56:21.833+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-12-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:56:21.837+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-12-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:56:21.838+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-12-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:56:21.844+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-12-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:56:19.672865+00:00, run_end_date=2024-02-29 05:56:20.578257+00:00, run_duration=0.905392, state=success, executor_state=success, try_number=1, max_tries=2, job_id=81, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:56:01.787122+00:00, queued_by_job_id=1, pid=1319705[0m
[[34m2024-02-29T05:56:21.957+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[0m05:56:23  Running with dbt=1.7.8
[0m05:56:23  Error importing adapter: No module named 'dbt.adapters.postgres'
[0m05:56:23  Encountered an error:
Runtime Error
  Credentials in profile "ny_taxi_test", target "production" invalid: Runtime Error
    Could not find adapter type postgres!
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:56:25.924+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2021-06-01 00:00:00+00:00: scheduled__2021-06-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:33.085533+00:00. externally triggered: False> successful[0m
[[34m2024-02-29T05:56:25.929+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-06-01 00:00:00+00:00, run_id=scheduled__2021-06-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:33.112672+00:00, run_end_date=2024-02-29 05:56:25.929548+00:00, run_duration=1552.816876, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-06-01 00:00:00+00:00, data_interval_end=2021-07-01 00:00:00+00:00, dag_hash=6b8b17fe2fc42da85c4ea45b23a43bdf[0m
[[34m2024-02-29T05:56:25.939+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-07-01 00:00:00+00:00, run_after=2021-08-01 00:00:00+00:00[0m
[[34m2024-02-29T05:56:26.069+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-06-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:56:26.080+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-06-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:56:17.627242+00:00, run_end_date=2024-02-29 05:56:25.178562+00:00, run_duration=7.55132, state=success, executor_state=success, try_number=1, max_tries=2, job_id=80, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:55:58.783620+00:00, queued_by_job_id=1, pid=1319648[0m
[0m05:56:26  Running with dbt=1.7.8
[0m05:56:26  Error importing adapter: No module named 'dbt.adapters.postgres'
[0m05:56:26  Encountered an error:
Runtime Error
  Credentials in profile "ny_taxi_test", target "production" invalid: Runtime Error
    Could not find adapter type postgres!
[[34m2024-02-29T05:56:27.122+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-08-01 00:00:00+00:00, run_after=2021-09-01 00:00:00+00:00[0m
[[34m2024-02-29T05:56:28.267+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-09-01 00:00:00+00:00, run_after=2021-10-01 00:00:00+00:00[0m
[[34m2024-02-29T05:56:28.293+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2021-09-01 00:00:00+00:00: scheduled__2021-09-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:33.728007+00:00. externally triggered: False> successful[0m
[[34m2024-02-29T05:56:28.294+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-09-01 00:00:00+00:00, run_id=scheduled__2021-09-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:33.766867+00:00, run_end_date=2024-02-29 05:56:28.294204+00:00, run_duration=1554.527337, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-09-01 00:00:00+00:00, data_interval_end=2021-10-01 00:00:00+00:00, dag_hash=6b8b17fe2fc42da85c4ea45b23a43bdf[0m
[[34m2024-02-29T05:56:28.299+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-10-01 00:00:00+00:00, run_after=2021-11-01 00:00:00+00:00[0m
[[34m2024-02-29T05:56:28.451+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-09-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:56:28.460+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-09-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:56:20.946149+00:00, run_end_date=2024-02-29 05:56:27.989163+00:00, run_duration=7.043014, state=success, executor_state=success, try_number=1, max_tries=2, job_id=82, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:56:05.467667+00:00, queued_by_job_id=1, pid=1319777[0m
[[34m2024-02-29T05:56:29.507+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-11-01 00:00:00+00:00, run_after=2021-12-01 00:00:00+00:00[0m
[[34m2024-02-29T05:56:29.891+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-12-01 00:00:00+00:00, run_after=2022-01-01 00:00:00+00:00[0m
[[34m2024-02-29T05:56:31.050+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-12-01T00:00:00+00:00/task_id=run_dbt_command permission to 509
[[34m2024-02-29T05:56:31.163+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-12-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[0m05:56:36  Running with dbt=1.7.8
[0m05:56:36  Error importing adapter: No module named 'dbt.adapters.postgres'
[0m05:56:36  Encountered an error:
Runtime Error
  Credentials in profile "ny_taxi_test", target "production" invalid: Runtime Error
    Could not find adapter type postgres!
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T05:56:38.045+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2021-12-01 00:00:00+00:00: scheduled__2021-12-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:34.592505+00:00. externally triggered: False> successful[0m
[[34m2024-02-29T05:56:38.046+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-12-01 00:00:00+00:00, run_id=scheduled__2021-12-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:34.631628+00:00, run_end_date=2024-02-29 05:56:38.046654+00:00, run_duration=1563.415026, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-12-01 00:00:00+00:00, data_interval_end=2022-01-01 00:00:00+00:00, dag_hash=6b8b17fe2fc42da85c4ea45b23a43bdf[0m
[[34m2024-02-29T05:56:38.055+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
[[34m2024-02-29T05:56:38.128+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-12-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:56:38.136+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-12-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:56:31.579672+00:00, run_end_date=2024-02-29 05:56:37.635215+00:00, run_duration=6.055543, state=success, executor_state=success, try_number=1, max_tries=2, job_id=83, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:56:21.830296+00:00, queued_by_job_id=1, pid=1320127[0m
[[34m2024-02-29T05:56:39.026+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-02-29T05:57:01.924+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:57:02.222+0000[0m] {[34mlocal_executor.py:[0m138} ERROR[0m - Failed to execute task It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063..[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 134, in _execute_work_in_fork
    args.func(args)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/cli.py", line 114, in wrapper
    return f(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 438, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 216, in _run_task_by_selected_method
    return _run_task_by_local_task_job(args, ti)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/task_command.py", line 278, in _run_task_by_local_task_job
    ret = run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 393, in run_job
    return execute_job(job, execute_callable=execute_callable)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 422, in execute_job
    ret = execute_callable()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 198, in _execute
    self.handle_task_exit(return_code)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 238, in handle_task_exit
    self.task_instance.schedule_downstream_tasks(max_tis_per_query=self.job.max_tis_per_query)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3329, in schedule_downstream_tasks
    return TaskInstance._schedule_downstream_tasks(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 3279, in _schedule_downstream_tasks
    partial_dag = task.dag.partial_subset(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2513, in partial_subset
    dag.task_dict = {
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2514, in <dictcomp>
    t.task_id: _deepcopy_task(t)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/dag.py", line 2511, in _deepcopy_task
    return copy.deepcopy(t, memo)
  File "/usr/lib/python3.10/copy.py", line 153, in deepcopy
    y = copier(memo)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 1211, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 172, in deepcopy
    y = _reconstruct(x, memo, *rv)
  File "/usr/lib/python3.10/copy.py", line 271, in _reconstruct
    state = deepcopy(state, memo)
  File "/usr/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/usr/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/usr/lib/python3.10/copy.py", line 161, in deepcopy
    rv = reductor(4)
  File "/opt/spark/python/pyspark/context.py", line 361, in __getnewargs__
    raise RuntimeError(
RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.[0m
[[34m2024-02-29T05:57:02.902+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 2 tasks up for execution:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-10-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-11-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:57:02.902+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 0/16 running and queued tasks[0m
[[34m2024-02-29T05:57:02.902+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T05:57:02.903+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-10-01T00:00:00+00:00 [scheduled]>
	<TaskInstance: run_pipeline_new.remove_file scheduled__2021-11-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:57:02.904+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task remove_file because previous state change time has not been saved[0m
[[34m2024-02-29T05:57:02.904+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task remove_file because previous state change time has not been saved[0m
[[34m2024-02-29T05:57:02.905+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-10-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-29T05:57:02.905+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:57:02.905+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-11-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-02-29T05:57:02.905+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-11-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:57:02.942+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-11-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:57:02.942+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:57:02.942+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'remove_file', 'scheduled__2021-11-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:57:02.942+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state failed for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='download_load_data', run_id='scheduled__2021-10-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:57:02.945+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-10-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:57.206249+00:00, run_end_date=2024-02-29 05:57:02.123028+00:00, run_duration=1564.916779, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=64, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:34.269093+00:00, queued_by_job_id=1, pid=1287188[0m
[[34m2024-02-29T05:57:02.946+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=download_load_data, run_id=scheduled__2021-11-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:30:59.372936+00:00, run_end_date=2024-02-29 05:57:01.825280+00:00, run_duration=1562.452344, state=success, executor_state=failed, try_number=1, max_tries=2, job_id=67, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-02-29 05:30:34.567650+00:00, queued_by_job_id=1, pid=1287281[0m
[[34m2024-02-29T05:57:02.990+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:57:03.014+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml

Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-10-01T00:00:00+00:00/task_id=remove_file permission to 509
[[34m2024-02-29T05:57:14.302+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-10-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-11-01T00:00:00+00:00/task_id=remove_file permission to 509
[[34m2024-02-29T05:57:14.498+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.remove_file scheduled__2021-11-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[[34m2024-02-29T05:57:15.034+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-10-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:57:15.035+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T05:57:15.036+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-10-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:57:15.038+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved[0m
[[34m2024-02-29T05:57:15.040+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-10-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-29T05:57:15.041+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:57:15.044+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-10-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:57:15.105+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
[[34m2024-02-29T05:57:15.918+0000[0m] {[34mscheduler_job_runner.py:[0m424} INFO[0m - 1 tasks up for execution:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-11-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:57:15.919+0000[0m] {[34mscheduler_job_runner.py:[0m487} INFO[0m - DAG run_pipeline_new has 1/16 running and queued tasks[0m
[[34m2024-02-29T05:57:15.919+0000[0m] {[34mscheduler_job_runner.py:[0m603} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-11-01T00:00:00+00:00 [scheduled]>[0m
[[34m2024-02-29T05:57:15.921+0000[0m] {[34mtaskinstance.py:[0m2283} WARNING[0m - cannot record scheduled_duration for task run_dbt_command because previous state change time has not been saved[0m
[[34m2024-02-29T05:57:15.922+0000[0m] {[34mscheduler_job_runner.py:[0m646} INFO[0m - Sending TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-11-01T00:00:00+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-02-29T05:57:15.922+0000[0m] {[34mbase_executor.py:[0m146} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-11-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:57:15.926+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-10-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:57:15.925+0000[0m] {[34mlocal_executor.py:[0m89} INFO[0m - QueuedLocalWorker running ['airflow', 'tasks', 'run', 'run_pipeline_new', 'run_dbt_command', 'scheduled__2021-11-01T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/run_pipeline.py'][0m
[[34m2024-02-29T05:57:15.927+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='remove_file', run_id='scheduled__2021-11-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:57:15.934+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-10-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:57:14.436610+00:00, run_end_date=2024-02-29 05:57:14.951071+00:00, run_duration=0.514461, state=success, executor_state=success, try_number=1, max_tries=2, job_id=84, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:57:02.903471+00:00, queued_by_job_id=1, pid=1321258[0m
[[34m2024-02-29T05:57:15.935+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=remove_file, run_id=scheduled__2021-11-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:57:14.656274+00:00, run_end_date=2024-02-29 05:57:15.296959+00:00, run_duration=0.640685, state=success, executor_state=success, try_number=1, max_tries=2, job_id=85, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-02-29 05:57:02.903471+00:00, queued_by_job_id=1, pid=1321274[0m
[[34m2024-02-29T05:57:15.976+0000[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/dags/run_pipeline.py[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-10-01T00:00:00+00:00/task_id=run_dbt_command permission to 509
[[34m2024-02-29T05:57:21.726+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-10-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
Changing /home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/airflow/logs/dag_id=run_pipeline_new/run_id=scheduled__2021-11-01T00:00:00+00:00/task_id=run_dbt_command permission to 509
[[34m2024-02-29T05:57:22.619+0000[0m] {[34mtask_command.py:[0m423} INFO[0m - Running <TaskInstance: run_pipeline_new.run_dbt_command scheduled__2021-11-01T00:00:00+00:00 [queued]> on host idowu-pc[0m
[0m05:57:24  Running with dbt=1.7.8
[0m05:57:24  Error importing adapter: No module named 'dbt.adapters.postgres'
[0m05:57:24  Encountered an error:
Runtime Error
  Credentials in profile "ny_taxi_test", target "production" invalid: Runtime Error
    Could not find adapter type postgres!
[0m05:57:25  Running with dbt=1.7.8
[0m05:57:25  Error importing adapter: No module named 'dbt.adapters.postgres'
[0m05:57:25  Encountered an error:
Runtime Error
  Credentials in profile "ny_taxi_test", target "production" invalid: Runtime Error
    Could not find adapter type postgres!
[[34m2024-02-29T05:57:27.099+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2021-10-01 00:00:00+00:00: scheduled__2021-10-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:34.006060+00:00. externally triggered: False> successful[0m
[[34m2024-02-29T05:57:27.099+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-10-01 00:00:00+00:00, run_id=scheduled__2021-10-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:34.070658+00:00, run_end_date=2024-02-29 05:57:27.099501+00:00, run_duration=1613.028843, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-10-01 00:00:00+00:00, data_interval_end=2021-11-01 00:00:00+00:00, dag_hash=e3c0a33f700e4557aea886780c40b272[0m
[[34m2024-02-29T05:57:27.102+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-11-01 00:00:00+00:00, run_after=2021-12-01 00:00:00+00:00[0m
[[34m2024-02-29T05:57:27.105+0000[0m] {[34mdagrun.py:[0m795} INFO[0m - Marking run <DagRun run_pipeline_new @ 2021-11-01 00:00:00+00:00: scheduled__2021-11-01T00:00:00+00:00, state:running, queued_at: 2024-02-29 05:30:34.308131+00:00. externally triggered: False> successful[0m
[[34m2024-02-29T05:57:27.105+0000[0m] {[34mdagrun.py:[0m846} INFO[0m - DagRun Finished: dag_id=run_pipeline_new, execution_date=2021-11-01 00:00:00+00:00, run_id=scheduled__2021-11-01T00:00:00+00:00, run_start_date=2024-02-29 05:30:34.340199+00:00, run_end_date=2024-02-29 05:57:27.105902+00:00, run_duration=1612.765703, state=success, external_trigger=False, run_type=scheduled, data_interval_start=2021-11-01 00:00:00+00:00, data_interval_end=2021-12-01 00:00:00+00:00, dag_hash=e3c0a33f700e4557aea886780c40b272[0m
[[34m2024-02-29T05:57:27.109+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to 2021-12-01 00:00:00+00:00, run_after=2022-01-01 00:00:00+00:00[0m
[[34m2024-02-29T05:57:27.119+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-10-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:57:27.119+0000[0m] {[34mscheduler_job_runner.py:[0m696} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='run_pipeline_new', task_id='run_dbt_command', run_id='scheduled__2021-11-01T00:00:00+00:00', try_number=1, map_index=-1)[0m
[[34m2024-02-29T05:57:27.123+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-10-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:57:21.892654+00:00, run_end_date=2024-02-29 05:57:26.259131+00:00, run_duration=4.366477, state=success, executor_state=success, try_number=1, max_tries=2, job_id=86, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:57:15.037425+00:00, queued_by_job_id=1, pid=1321772[0m
[[34m2024-02-29T05:57:27.123+0000[0m] {[34mscheduler_job_runner.py:[0m733} INFO[0m - TaskInstance Finished: dag_id=run_pipeline_new, task_id=run_dbt_command, run_id=scheduled__2021-11-01T00:00:00+00:00, map_index=-1, run_start_date=2024-02-29 05:57:22.762278+00:00, run_end_date=2024-02-29 05:57:26.693046+00:00, run_duration=3.930768, state=success, executor_state=success, try_number=1, max_tries=2, job_id=87, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-02-29 05:57:15.920262+00:00, queued_by_job_id=1, pid=1321803[0m
[[34m2024-02-29T05:57:28.143+0000[0m] {[34mdag.py:[0m3834} INFO[0m - Setting next_dagrun for run_pipeline_new to None, run_after=None[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T06:01:39.063+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T06:06:39.104+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T06:11:39.138+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T06:16:39.176+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T06:21:39.212+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T06:26:39.243+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T06:31:39.276+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T06:36:39.320+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T06:41:39.356+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T06:46:39.388+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T06:51:39.419+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T06:56:39.453+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T07:01:39.487+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T07:06:39.519+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T07:11:39.550+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T07:16:39.584+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T07:21:39.618+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T07:26:39.652+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T07:31:39.692+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T07:36:39.725+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T07:41:39.758+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T07:46:39.798+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T07:51:39.832+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T07:56:39.866+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T08:01:39.905+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T08:06:39.941+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T08:11:39.975+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T08:16:40.005+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T08:21:40.038+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T08:26:40.074+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T08:31:40.110+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T08:36:40.143+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T08:41:40.176+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T08:46:40.211+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T08:51:40.246+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T08:56:40.279+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T09:01:40.310+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T09:06:40.343+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T09:11:40.376+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T09:16:40.410+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T09:21:40.443+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T09:26:40.459+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T09:31:40.488+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T09:36:40.520+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T09:41:40.554+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T09:46:40.589+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T09:51:40.620+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T09:56:40.654+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T10:01:40.685+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T10:06:40.716+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T10:11:40.752+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T10:16:40.782+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T10:21:40.815+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T10:26:40.849+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T10:31:40.883+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T10:36:40.920+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T10:41:40.955+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T10:46:40.986+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T10:51:41.017+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T10:56:41.048+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T11:01:41.080+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T11:06:41.113+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T11:11:41.147+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T11:16:41.175+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T11:21:41.205+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T11:26:41.237+0000[0m] {[34mscheduler_job_runner.py:[0m1608} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
:: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
[[34m2024-02-29T11:28:36.136+0000[0m] {[34mscheduler_job_runner.py:[0m258} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2024-02-29T11:28:36.542+0000[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 1188951. PIDs of all processes in the group: [][0m
[[34m2024-02-29T11:28:36.543+0000[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 1188951[0m
[[34m2024-02-29T11:28:36.543+0000[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending the signal Signals.SIGTERM to process 1188951 as process group is missing.[0m
[[34m2024-02-29T11:28:36.544+0000[0m] {[34mlocal_executor.py:[0m402} INFO[0m - Shutting down LocalExecutor; waiting for running tasks to finish.  Signal again if you don't want to wait.[0m
[[34m2024-02-29T11:28:36.544+0000[0m] {[34mscheduler_job_runner.py:[0m878} ERROR[0m - Exception when executing Executor.end[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 855, in _execute
    self._run_scheduler_loop()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 1010, in _run_scheduler_loop
    time.sleep(min(self._scheduler_idle_sleep_time, next_event or 0))
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 261, in _exit_gracefully
    sys.exit(os.EX_OK)
SystemExit: 0

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/scheduler_job_runner.py", line 876, in _execute
    self.job.executor.end()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 406, in end
    self.impl.end()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/executors/local_executor.py", line 350, in end
    self.queue.put((None, None))
  File "<string>", line 2, in put
  File "/usr/lib/python3.10/multiprocessing/managers.py", line 817, in _callmethod
    conn.send((self._id, methodname, args, kwds))
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/usr/lib/python3.10/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe[0m
[[34m2024-02-29T11:28:36.563+0000[0m] {[34mprocess_utils.py:[0m131} INFO[0m - Sending Signals.SIGTERM to group 1188951. PIDs of all processes in the group: [][0m
[[34m2024-02-29T11:28:36.564+0000[0m] {[34mprocess_utils.py:[0m86} INFO[0m - Sending the signal Signals.SIGTERM to group 1188951[0m
[[34m2024-02-29T11:28:36.564+0000[0m] {[34mprocess_utils.py:[0m100} INFO[0m - Sending the signal Signals.SIGTERM to process 1188951 as process group is missing.[0m
[[34m2024-02-29T11:28:36.565+0000[0m] {[34mscheduler_job_runner.py:[0m884} INFO[0m - Exited execute loop[0m
[[34m2024-02-29T11:28:36.567+0000[0m] {[34mscheduler_command.py:[0m54} ERROR[0m - Exception when running scheduler job[0m
Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 936, in _checkout
    raise exc.InvalidatePoolError()
sqlalchemy.exc.InvalidatePoolError: ()

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 970, in _checkout
    with util.safe_reraise():
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 967, in _checkout
    fairy._connection_record.get_connection()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 630, in get_connection
    self.__connect()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "localhost" (127.0.0.1), port 5434 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/cli/commands/scheduler_command.py", line 52, in _run_scheduler_job
    run_job(job=job_runner.job, execute_callable=job_runner._execute)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 79, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 395, in run_job
    job.complete_execution(session=session)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 240, in complete_execution
    Job._update_in_db(job=self, session=session)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 114, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/airflow/jobs/job.py", line 332, in _update_in_db
    session.merge(job)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3056, in merge
    return self._merge(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3136, in _merge
    merged = self.get(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2853, in get
    return self._get_impl(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2975, in _get_impl
    return db_load_fn(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 970, in _checkout
    with util.safe_reraise():
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 967, in _checkout
    fairy._connection_record.get_connection()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 630, in get_connection
    self.__connect()
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/idowuileks/Desktop/tech_project/nyc_taxi_project/docker_sql/ny_taxi_test/dbtairflowenv/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "localhost" (127.0.0.1), port 5434 failed: Connection refused
	Is the server running on that host and accepting TCP/IP connections?

(Background on this error at: https://sqlalche.me/e/14/e3q8)[0m
